{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a90f4d9",
   "metadata": {
    "papermill": {
     "duration": 0.028457,
     "end_time": "2023-04-29T11:08:40.681387",
     "exception": false,
     "start_time": "2023-04-29T11:08:40.652930",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Optimizers \n",
    "\n",
    "So what are `Optimizers`\n",
    "\n",
    "**What** - Optimizer is a technique to set the `weights` and `biases` of a particular model such that they produce minimum `Loss` Possible.\n",
    "\n",
    "The bestest way (in my opinion) to get started is to fist understand what the hell is this `Regression`\n",
    "\n",
    "**What** - `Regression` is just like the lost brother of `classification`. In `classification` we have `discrete` or `particular values`, that we want to `classify`, In `regression` we have `continuous values`, that we want to `predict`\n",
    "\n",
    "| Classification |Regression |\n",
    "| --- | --- |\n",
    "|  We have discrete values| We have continuous values |\n",
    "|Usually we know these values in depth | We usually don't know these values in depth|\n",
    "|These are comparatively less in number| These are comparatively more in number |\n",
    "\n",
    "# 1 | What other things we will learn here\n",
    "* Slope of Function\n",
    "* Baisc Diffrentiation\n",
    "* Intercept "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caf80023",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:40.738964Z",
     "iopub.status.busy": "2023-04-29T11:08:40.737981Z",
     "iopub.status.idle": "2023-04-29T11:08:40.751495Z",
     "shell.execute_reply": "2023-04-29T11:08:40.750197Z"
    },
    "papermill": {
     "duration": 0.044489,
     "end_time": "2023-04-29T11:08:40.754238",
     "exception": false,
     "start_time": "2023-04-29T11:08:40.709749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ba61132",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:40.810393Z",
     "iopub.status.busy": "2023-04-29T11:08:40.809435Z",
     "iopub.status.idle": "2023-04-29T11:08:42.065493Z",
     "shell.execute_reply": "2023-04-29T11:08:42.064391Z"
    },
    "papermill": {
     "duration": 1.287179,
     "end_time": "2023-04-29T11:08:42.068509",
     "exception": false,
     "start_time": "2023-04-29T11:08:40.781330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db55f913",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:42.122898Z",
     "iopub.status.busy": "2023-04-29T11:08:42.122455Z",
     "iopub.status.idle": "2023-04-29T11:08:42.127334Z",
     "shell.execute_reply": "2023-04-29T11:08:42.126130Z"
    },
    "papermill": {
     "duration": 0.035151,
     "end_time": "2023-04-29T11:08:42.129878",
     "exception": false,
     "start_time": "2023-04-29T11:08:42.094727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9763ac0e",
   "metadata": {
    "papermill": {
     "duration": 0.026921,
     "end_time": "2023-04-29T11:08:42.182986",
     "exception": false,
     "start_time": "2023-04-29T11:08:42.156065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Lets assume we have data like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38bd7b33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:42.239319Z",
     "iopub.status.busy": "2023-04-29T11:08:42.238881Z",
     "iopub.status.idle": "2023-04-29T11:08:42.248927Z",
     "shell.execute_reply": "2023-04-29T11:08:42.248021Z"
    },
    "papermill": {
     "duration": 0.041282,
     "end_time": "2023-04-29T11:08:42.251596",
     "exception": false,
     "start_time": "2023-04-29T11:08:42.210314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.array([x for x in range(0 , 200 , 1)])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "927a2c34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:42.306898Z",
     "iopub.status.busy": "2023-04-29T11:08:42.306427Z",
     "iopub.status.idle": "2023-04-29T11:08:42.315437Z",
     "shell.execute_reply": "2023-04-29T11:08:42.313759Z"
    },
    "papermill": {
     "duration": 0.04011,
     "end_time": "2023-04-29T11:08:42.318542",
     "exception": false,
     "start_time": "2023-04-29T11:08:42.278432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   2,   4,   6,   8,  10,  12,  14,  16,  18,  20,  22,  24,\n",
       "        26,  28,  30,  32,  34,  36,  38,  40,  42,  44,  46,  48,  50,\n",
       "        52,  54,  56,  58,  60,  62,  64,  66,  68,  70,  72,  74,  76,\n",
       "        78,  80,  82,  84,  86,  88,  90,  92,  94,  96,  98, 100, 102,\n",
       "       104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128,\n",
       "       130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154,\n",
       "       156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180,\n",
       "       182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206,\n",
       "       208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232,\n",
       "       234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258,\n",
       "       260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284,\n",
       "       286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310,\n",
       "       312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336,\n",
       "       338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362,\n",
       "       364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388,\n",
       "       390, 392, 394, 396, 398])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = np.array([x for x in range(0 , 400 , 2)])\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75ed231",
   "metadata": {
    "papermill": {
     "duration": 0.026534,
     "end_time": "2023-04-29T11:08:42.373020",
     "exception": false,
     "start_time": "2023-04-29T11:08:42.346486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Lets assume there is a connection between the `target` , and `features`. By human instacne we know that every element in `target` is just a double of the corresponding element in `features`, or $target  = 2XFeatures$. \n",
    "\n",
    "Lets assume we change the target a little bit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8e5ce05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:42.429653Z",
     "iopub.status.busy": "2023-04-29T11:08:42.429121Z",
     "iopub.status.idle": "2023-04-29T11:08:42.439453Z",
     "shell.execute_reply": "2023-04-29T11:08:42.438093Z"
    },
    "papermill": {
     "duration": 0.042333,
     "end_time": "2023-04-29T11:08:42.442203",
     "exception": false,
     "start_time": "2023-04-29T11:08:42.399870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   3,   5,   7,   9,  11,  13,  15,  17,  19,  21,  23,  25,\n",
       "        27,  29,  31,  33,  35,  37,  39,  41,  43,  45,  47,  49,  51,\n",
       "        53,  55,  57,  59,  61,  63,  65,  67,  69,  71,  73,  75,  77,\n",
       "        79,  81,  83,  85,  87,  89,  91,  93,  95,  97,  99, 101, 103,\n",
       "       105, 107, 109, 111, 113, 115, 117, 119, 121, 123, 125, 127, 129,\n",
       "       131, 133, 135, 137, 139, 141, 143, 145, 147, 149, 151, 153, 155,\n",
       "       157, 159, 161, 163, 165, 167, 169, 171, 173, 175, 177, 179, 181,\n",
       "       183, 185, 187, 189, 191, 193, 195, 197, 199, 201, 203, 205, 207,\n",
       "       209, 211, 213, 215, 217, 219, 221, 223, 225, 227, 229, 231, 233,\n",
       "       235, 237, 239, 241, 243, 245, 247, 249, 251, 253, 255, 257, 259,\n",
       "       261, 263, 265, 267, 269, 271, 273, 275, 277, 279, 281, 283, 285,\n",
       "       287, 289, 291, 293, 295, 297, 299, 301, 303, 305, 307, 309, 311,\n",
       "       313, 315, 317, 319, 321, 323, 325, 327, 329, 331, 333, 335, 337,\n",
       "       339, 341, 343, 345, 347, 349, 351, 353, 355, 357, 359, 361, 363,\n",
       "       365, 367, 369, 371, 373, 375, 377, 379, 381, 383, 385, 387, 389,\n",
       "       391, 393, 395, 397, 399])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = np.array([x + 1 for x in range(0 , 400 , 2)])\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2c4af1",
   "metadata": {
    "papermill": {
     "duration": 0.026503,
     "end_time": "2023-04-29T11:08:42.495792",
     "exception": false,
     "start_time": "2023-04-29T11:08:42.469289",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now what could be the trend here..., We can see the code above and with the help of that we can say. That `target` value is just the `double + 1` of the corresponding element in `features`. or $target = 2Xfeatures + 1$\n",
    "\n",
    "Till now the problem was really easy to solve, and thats why we used the brain only, But these are just examples. As we move closer to the real world. The examples/problems get difficulat and we find it harder to find proper trends in the two `arrays`. Thats we try to teach machine, how to find trend in the data. The formula we had before $target = 2Xfeature + 1$ is subjective to only one problem or a similar problem. But this formula can be generlized by the equation of `straight line`, which is $y = mx + b$\n",
    "\n",
    "So what does this line means ???\n",
    "\n",
    "Lets first try to plot the data we had on a scatter plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "279a237f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:42.552703Z",
     "iopub.status.busy": "2023-04-29T11:08:42.552251Z",
     "iopub.status.idle": "2023-04-29T11:08:42.856096Z",
     "shell.execute_reply": "2023-04-29T11:08:42.854641Z"
    },
    "papermill": {
     "duration": 0.335608,
     "end_time": "2023-04-29T11:08:42.858896",
     "exception": false,
     "start_time": "2023-04-29T11:08:42.523288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x72e3b141ff10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(features , target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f955735",
   "metadata": {
    "papermill": {
     "duration": 0.02805,
     "end_time": "2023-04-29T11:08:42.916285",
     "exception": false,
     "start_time": "2023-04-29T11:08:42.888235",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You can see we got a sequence of dots that resembles kind of straight line. \n",
    "\n",
    "Lets assume we have a line that tries to capture most of the points on this, like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf45755e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:42.975002Z",
     "iopub.status.busy": "2023-04-29T11:08:42.973753Z",
     "iopub.status.idle": "2023-04-29T11:08:43.239359Z",
     "shell.execute_reply": "2023-04-29T11:08:43.238150Z"
    },
    "papermill": {
     "duration": 0.297483,
     "end_time": "2023-04-29T11:08:43.242010",
     "exception": false,
     "start_time": "2023-04-29T11:08:42.944527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x72e3b1178210>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(features , target)\n",
    "plt.plot([0 , 200] , [0 , 400] , \"yellow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d0cdfe",
   "metadata": {
    "papermill": {
     "duration": 0.02764,
     "end_time": "2023-04-29T11:08:43.298255",
     "exception": false,
     "start_time": "2023-04-29T11:08:43.270615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Again by human intution we found the `best fit line`. But what if we want to generalize the things and kind of do not find the best fit line...?\n",
    "\n",
    "First of all lets get a little bit more deep into equation $y = mx + b$\n",
    "\n",
    "So what does these terms resembles in this eqution. \n",
    "* `m` is the slope of the line\n",
    "\n",
    "# 1.1 | Slope of A function\n",
    "\n",
    "Slope of a function shows how steep a function is, or the direction of a function at a given point on the curve.\n",
    "\n",
    "Lets assume we have this curve $y = 4x^2$ the slope of this curve will be $y = 8x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28779534",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:43.357845Z",
     "iopub.status.busy": "2023-04-29T11:08:43.357346Z",
     "iopub.status.idle": "2023-04-29T11:08:43.365117Z",
     "shell.execute_reply": "2023-04-29T11:08:43.363800Z"
    },
    "papermill": {
     "duration": 0.041491,
     "end_time": "2023-04-29T11:08:43.367626",
     "exception": false,
     "start_time": "2023-04-29T11:08:43.326135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"400\"\n",
       "            src=\"https://www.desmos.com/calculator/zluqu5vyuh\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x72e3b118ddd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.desmos.com/calculator/zluqu5vyuh\" , 400 , 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a3569b",
   "metadata": {
    "papermill": {
     "duration": 0.027897,
     "end_time": "2023-04-29T11:08:43.423489",
     "exception": false,
     "start_time": "2023-04-29T11:08:43.395592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So how do we calculate the `slope` of a line???\n",
    "\n",
    "Lets assume we have a function $y = f(x)$,. To find the slope of a function, we simply diffrenctiate the function, thus, the slope of this line will be $y^` = f^`(x)$\n",
    "\n",
    "# 1.2 | Diffrentiation\n",
    "\n",
    "Diffrentaition can be explained as getting a small value of a function.\n",
    "\n",
    "lets assume we have a function `y = sin(x)`\n",
    "\n",
    "A small strip at that function will demonstrate taking a derivative of that function `sin(x)`\n",
    "\n",
    "Taking about the function we had taken before that is $y = 4x^2$\n",
    "\n",
    "Taking its derivative we will get $$y = 8x$$ ($x{n^`} = nx^{n-1}$)\n",
    "\n",
    "So the slope of $y = 4x^2$ can be represnted as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5768148b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:43.482683Z",
     "iopub.status.busy": "2023-04-29T11:08:43.482251Z",
     "iopub.status.idle": "2023-04-29T11:08:43.489719Z",
     "shell.execute_reply": "2023-04-29T11:08:43.488480Z"
    },
    "papermill": {
     "duration": 0.040118,
     "end_time": "2023-04-29T11:08:43.492586",
     "exception": false,
     "start_time": "2023-04-29T11:08:43.452468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"400\"\n",
       "            src=\"https://www.desmos.com/calculator/hrguwktg9q\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x72e3b1146d90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.desmos.com/calculator/hrguwktg9q\" , 400 , 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfffa66",
   "metadata": {
    "papermill": {
     "duration": 0.028391,
     "end_time": "2023-04-29T11:08:43.549038",
     "exception": false,
     "start_time": "2023-04-29T11:08:43.520647",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**If you want to know more aboud diffrentiation, here is [3Blue1Brown](https://www.youtube.com/@3blue1brown/featured) => [Essence Of Calculas](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)**\n",
    "\n",
    "So now you have a basic idea of `slope`\n",
    "\n",
    "# 1.3 | Intercept\n",
    "\n",
    "Now what `b` represents in the data. Usually it is called the `intercept`. Consider this graph of the equation $y = x$ or $y = 1x + b$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "727c3452",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:43.607101Z",
     "iopub.status.busy": "2023-04-29T11:08:43.606655Z",
     "iopub.status.idle": "2023-04-29T11:08:43.613630Z",
     "shell.execute_reply": "2023-04-29T11:08:43.612717Z"
    },
    "papermill": {
     "duration": 0.038466,
     "end_time": "2023-04-29T11:08:43.615792",
     "exception": false,
     "start_time": "2023-04-29T11:08:43.577326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"400\"\n",
       "            src=\"https://www.desmos.com/calculator/gai0veg5fh\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x72e3b113df90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.desmos.com/calculator/gai0veg5fh\" , 400 , 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e7f0f6",
   "metadata": {
    "papermill": {
     "duration": 0.028223,
     "end_time": "2023-04-29T11:08:43.672115",
     "exception": false,
     "start_time": "2023-04-29T11:08:43.643892",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This line passes the axis at $(0 , 0)$. These coordinates are called as the `intercepts` of this line. If we make `b` or `intercept` as $1$. The line will then pass from $(1 , -1)$. Basically the `intercept` moves a line in a plane. With that being said, Lets also undertand how the `slope` changes the line. If we make `m` as $2$. The line will rotate anti-clockwise. So as we increase the value of `m` or `slope`. The line moves anti-clockwise, And so the vice-versa, If we decrease the value of `m`, The slope will move in the clockwise direction. \n",
    "\n",
    "In short tweeking the values of `m` and `b` or `slope` and `intercept`. We can move the line in any direction and in any way we want `as long as it resembles a straight line`. We still cannot bend the line \n",
    "\n",
    "So now we have any data, we just need to difine the values of `slope` and `intercpet`. And we can get the best fit line. But still the question arises how do we generalize the values of these tuning parametes. \n",
    "\n",
    "In simple word we can say, How can we find a relation between the data we have and these tuning parameters. So that we only need to define that relationship and then we can easily predict the values.\n",
    "\n",
    "Lets think that the value assigned to the line is this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3add097",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:43.732541Z",
     "iopub.status.busy": "2023-04-29T11:08:43.732120Z",
     "iopub.status.idle": "2023-04-29T11:08:43.989731Z",
     "shell.execute_reply": "2023-04-29T11:08:43.988543Z"
    },
    "papermill": {
     "duration": 0.291613,
     "end_time": "2023-04-29T11:08:43.992553",
     "exception": false,
     "start_time": "2023-04-29T11:08:43.700940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x72e3b1109f50>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(features , target)\n",
    "plt.plot([0 , 300] , [0 , 400] , \"yellow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3416e163",
   "metadata": {
    "papermill": {
     "duration": 0.029205,
     "end_time": "2023-04-29T11:08:44.054186",
     "exception": false,
     "start_time": "2023-04-29T11:08:44.024981",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If we test this line on the training data only, we will find that this line is not correct. It is predicting points incorrect, We know that the best fit line we drew first, will predict points wiht lowest incorrect ones. For example the line we just defined if asked the corresponding value of $200$, it will say $250$. But rather it was $400$. There was some `error`, some `loss`, or some `cost` with the `actual` and `predicted` values.\n",
    "\n",
    "For measuring this loss, what we can do is find the difference between the `actual value` and the `predicted value`. A best fit line will give the lowest value of this difference.\n",
    "\n",
    "The word difference here is very difficult to say, so we can give this term a new fancy name, which is `The Loss`.\n",
    "\n",
    "One can deifne loss as $$Loss = actual - predicted$$.\n",
    "\n",
    "We only took the example of one value. but there are a large group of values. that can show the same trait, For that we can change the formula to \n",
    "\n",
    "Lets denote $actual$ as $a$ and $predicted$ as $p$\n",
    "\n",
    "$$Loss = (a_1 - p_1) + (a_2 - p_2) + (a_3 - p_3) + ... + (a_n - p_n)$$\n",
    "\n",
    "or $$Loss = \\sum\\limits_{i = 1}^{n}a_i - p_i$$ or $$Loss = \\sum\\limits_{i = 1}^{n}(y_i - \\hat y_i)$$\n",
    "\n",
    "Whenever you see $\\hat y$, think of it as the `predicted value`\n",
    "\n",
    "Now lets assume we have data like this and a random line is drawn like this \n",
    "\n",
    "<img src = \"https://cdn-media-1.freecodecamp.org/images/MNskFmGPKuQfMLdmpkT-X7-8w2cJXulP3683\">\n",
    "\n",
    "If you look closely, a lot of error terms will tend to cancel out each other. We can also get into a state where the line is `not the best fit`, but still gives $0$ error. With the `Loss` we defined before, we are not chossing a `best fit line`. Rather we are chossing a line that is in the `middle` of those points. One way to counter this is to add a `modulus` function like this $$Loss = \\sum\\limits_{i = 1}^{n}|y - \\hat y|$$\n",
    "\n",
    "But what is a modulus function. The function is nothing but converts, any negative numbers to postive. For example \n",
    "$|-1| = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90e967d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:44.115669Z",
     "iopub.status.busy": "2023-04-29T11:08:44.115236Z",
     "iopub.status.idle": "2023-04-29T11:08:44.122891Z",
     "shell.execute_reply": "2023-04-29T11:08:44.121704Z"
    },
    "papermill": {
     "duration": 0.041726,
     "end_time": "2023-04-29T11:08:44.125864",
     "exception": false,
     "start_time": "2023-04-29T11:08:44.084138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"400\"\n",
       "            src=\"https://www.desmos.com/calculator/kamxotjra2\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x72e3b10ac210>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.desmos.com/calculator/kamxotjra2\" , 400 , 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8b9146",
   "metadata": {
    "papermill": {
     "duration": 0.029074,
     "end_time": "2023-04-29T11:08:44.185973",
     "exception": false,
     "start_time": "2023-04-29T11:08:44.156899",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "But there is a problem with this function. A `modulus` is not diffrentiable. You might be thinking that why are we even seeing that part, like we we care for that. Why would you even diffrentiate a loss function. \n",
    "\n",
    "We actually diffrentiate loss function in further steps, thats why we will not use the modulus function. \n",
    "\n",
    "Another way of doing so is to, square the loss function like this $Loss = (y - \\hat y)^2$\n",
    "\n",
    "Its cool, its good and we can even diffrentiate this...\n",
    "\n",
    "Now we have a basic idea that we need to compute `m` and `b` for the lowest loss values. Now we should come to know how we can do this \n",
    "\n",
    "What if we somehow interelate the `losses` and `m and b`. \n",
    "\n",
    "Lets assume we intialize the parameters randomly, like this \n",
    "\n",
    "# 2 | SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d49876d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:44.271782Z",
     "iopub.status.busy": "2023-04-29T11:08:44.271301Z",
     "iopub.status.idle": "2023-04-29T11:08:44.279302Z",
     "shell.execute_reply": "2023-04-29T11:08:44.278241Z"
    },
    "papermill": {
     "duration": 0.060308,
     "end_time": "2023-04-29T11:08:44.282732",
     "exception": false,
     "start_time": "2023-04-29T11:08:44.222424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.51161234])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.random.randn(1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e98791b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:44.366355Z",
     "iopub.status.busy": "2023-04-29T11:08:44.364961Z",
     "iopub.status.idle": "2023-04-29T11:08:44.374423Z",
     "shell.execute_reply": "2023-04-29T11:08:44.373072Z"
    },
    "papermill": {
     "duration": 0.056582,
     "end_time": "2023-04-29T11:08:44.376998",
     "exception": false,
     "start_time": "2023-04-29T11:08:44.320416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.38966991])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases = np.random.randn(1)\n",
    "biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d29b4a2",
   "metadata": {
    "papermill": {
     "duration": 0.030657,
     "end_time": "2023-04-29T11:08:44.438060",
     "exception": false,
     "start_time": "2023-04-29T11:08:44.407403",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So the predicitions will be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39118fd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:44.506184Z",
     "iopub.status.busy": "2023-04-29T11:08:44.505167Z",
     "iopub.status.idle": "2023-04-29T11:08:44.514257Z",
     "shell.execute_reply": "2023-04-29T11:08:44.513063Z"
    },
    "papermill": {
     "duration": 0.043575,
     "end_time": "2023-04-29T11:08:44.516881",
     "exception": false,
     "start_time": "2023-04-29T11:08:44.473306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.9587002])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = weights * 30 + biases\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f45ae7",
   "metadata": {
    "papermill": {
     "duration": 0.030979,
     "end_time": "2023-04-29T11:08:44.579390",
     "exception": false,
     "start_time": "2023-04-29T11:08:44.548411",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "And its way far than what we had expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6512936",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:44.655197Z",
     "iopub.status.busy": "2023-04-29T11:08:44.654411Z",
     "iopub.status.idle": "2023-04-29T11:08:44.662410Z",
     "shell.execute_reply": "2023-04-29T11:08:44.660827Z"
    },
    "papermill": {
     "duration": 0.046803,
     "end_time": "2023-04-29T11:08:44.665776",
     "exception": false,
     "start_time": "2023-04-29T11:08:44.618973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-45.0412998])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = (pred - 60)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0a9850",
   "metadata": {
    "papermill": {
     "duration": 0.030623,
     "end_time": "2023-04-29T11:08:44.726532",
     "exception": false,
     "start_time": "2023-04-29T11:08:44.695909",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Our main motive is to reduce this loss as much as possible,. \n",
    "\n",
    "What if we subtract a small subset of the derivative of this loss from the parameters like this. The derivative of the loss will show us the steepness of the curve, and thus doing so might get us to the valeus of minimum loss. So how do we find the derivative of this function $Loss = (y - \\hat y)^2$. What we know is $\\hat y = mx + b$. COmputing this value in we get $$Loss = (y - mx - b)^2$$, Now we can diffrentiate the function\n",
    "\n",
    "## Diffrentiating wrt `b`\n",
    "$$\\frac {dLoss}{db}= \\frac {d}{db}(y - mx - b)^2$$\n",
    "$$= 2(y - mx - b)(-1)$$\n",
    "\n",
    "## Diffrentiating wrt `m`\n",
    "$$\\frac {dLoss}{dm} = \\frac {d}{dm}(y - mx - b)^2$$\n",
    "$$= 2(y - mx - b)(-x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c9987f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:44.789981Z",
     "iopub.status.busy": "2023-04-29T11:08:44.787396Z",
     "iopub.status.idle": "2023-04-29T11:08:44.795462Z",
     "shell.execute_reply": "2023-04-29T11:08:44.794516Z"
    },
    "papermill": {
     "duration": 0.042123,
     "end_time": "2023-04-29T11:08:44.798218",
     "exception": false,
     "start_time": "2023-04-29T11:08:44.756095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights -= (-2* (60 - weights*30 - biases)) * 0.001\n",
    "biases -= (2 * 30 * (60 - weights * 30 - biases)) * 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec3e171",
   "metadata": {
    "papermill": {
     "duration": 0.030912,
     "end_time": "2023-04-29T11:08:44.863291",
     "exception": false,
     "start_time": "2023-04-29T11:08:44.832379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "And if we then try to predict the values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccf47e24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:44.929060Z",
     "iopub.status.busy": "2023-04-29T11:08:44.927661Z",
     "iopub.status.idle": "2023-04-29T11:08:44.938835Z",
     "shell.execute_reply": "2023-04-29T11:08:44.937553Z"
    },
    "papermill": {
     "duration": 0.047153,
     "end_time": "2023-04-29T11:08:44.941216",
     "exception": false,
     "start_time": "2023-04-29T11:08:44.894063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.60169494]\n",
      "[-25.792963]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([67.7421149])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(weights)\n",
    "print(biases)\n",
    "loss = (60 - (weights * 30 + biases))\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e696849",
   "metadata": {
    "papermill": {
     "duration": 0.029564,
     "end_time": "2023-04-29T11:08:45.000845",
     "exception": false,
     "start_time": "2023-04-29T11:08:44.971281",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Our losses have been decreased, so lets do it again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b491694",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:45.065116Z",
     "iopub.status.busy": "2023-04-29T11:08:45.064180Z",
     "iopub.status.idle": "2023-04-29T11:08:45.076926Z",
     "shell.execute_reply": "2023-04-29T11:08:45.075890Z"
    },
    "papermill": {
     "duration": 0.047959,
     "end_time": "2023-04-29T11:08:45.079821",
     "exception": false,
     "start_time": "2023-04-29T11:08:45.031862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.95653723]\n",
      "[-24.4381207]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([25.74200366])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights -= -2 * loss * 0.01\n",
    "biases -= -2 * loss * 0.01\n",
    "print(weights)\n",
    "print(biases)\n",
    "loss = (60 - (weights * 30 + biases))\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845433cd",
   "metadata": {
    "papermill": {
     "duration": 0.030813,
     "end_time": "2023-04-29T11:08:45.140990",
     "exception": false,
     "start_time": "2023-04-29T11:08:45.110177",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So now we know if we do this iteratively, we will minimise the loss, and iteratively we will reach the optimal values of `weights` or `m` and `biases` or `m`\n",
    "\n",
    "Lets say we have runn this again and again for around 100 times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "554b8c09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:45.212162Z",
     "iopub.status.busy": "2023-04-29T11:08:45.210492Z",
     "iopub.status.idle": "2023-04-29T11:08:45.218790Z",
     "shell.execute_reply": "2023-04-29T11:08:45.217804Z"
    },
    "papermill": {
     "duration": 0.047111,
     "end_time": "2023-04-29T11:08:45.221657",
     "exception": false,
     "start_time": "2023-04-29T11:08:45.174546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    weights -= -2 * loss * 0.01\n",
    "    biases -= -2 * loss * 0.01\n",
    "    loss = (60 - (weights * 30 + biases))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df960fa",
   "metadata": {
    "papermill": {
     "duration": 0.030209,
     "end_time": "2023-04-29T11:08:45.283164",
     "exception": false,
     "start_time": "2023-04-29T11:08:45.252955",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Lets now see the weights and biases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fe10688",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:45.348325Z",
     "iopub.status.busy": "2023-04-29T11:08:45.347635Z",
     "iopub.status.idle": "2023-04-29T11:08:45.355604Z",
     "shell.execute_reply": "2023-04-29T11:08:45.354428Z"
    },
    "papermill": {
     "duration": 0.043518,
     "end_time": "2023-04-29T11:08:45.357976",
     "exception": false,
     "start_time": "2023-04-29T11:08:45.314458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.78692445]), array([-23.60773348]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights , biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419b40f4",
   "metadata": {
    "papermill": {
     "duration": 0.029378,
     "end_time": "2023-04-29T11:08:45.417138",
     "exception": false,
     "start_time": "2023-04-29T11:08:45.387760",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Though we have biases as high, but we have almost achived value of `weights`\n",
    "\n",
    "Lets do this all again, and now we will also try to plot a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bde6d17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:45.478565Z",
     "iopub.status.busy": "2023-04-29T11:08:45.477910Z",
     "iopub.status.idle": "2023-04-29T11:08:45.485642Z",
     "shell.execute_reply": "2023-04-29T11:08:45.484749Z"
    },
    "papermill": {
     "duration": 0.041375,
     "end_time": "2023-04-29T11:08:45.488137",
     "exception": false,
     "start_time": "2023-04-29T11:08:45.446762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = abs(np.random.randn(1))\n",
    "biases = abs(np.random.randn(1))\n",
    "losses = []\n",
    "for _ in range(100):\n",
    "    weights -= -2 * loss * 0.01\n",
    "    biases -= -2 * loss * 0.01\n",
    "    loss = (60 - (weights * 30 + biases))\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f9e39f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:45.550191Z",
     "iopub.status.busy": "2023-04-29T11:08:45.549802Z",
     "iopub.status.idle": "2023-04-29T11:08:45.837070Z",
     "shell.execute_reply": "2023-04-29T11:08:45.835643Z"
    },
    "papermill": {
     "duration": 0.32219,
     "end_time": "2023-04-29T11:08:45.839778",
     "exception": false,
     "start_time": "2023-04-29T11:08:45.517588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(np.array(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c498aebd",
   "metadata": {
    "papermill": {
     "duration": 0.034937,
     "end_time": "2023-04-29T11:08:45.905048",
     "exception": false,
     "start_time": "2023-04-29T11:08:45.870111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As we can see we have greatly decreased our losses "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a62986e",
   "metadata": {
    "papermill": {
     "duration": 0.02965,
     "end_time": "2023-04-29T11:08:45.965322",
     "exception": false,
     "start_time": "2023-04-29T11:08:45.935672",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.1 | Functionalities\n",
    "We have made out our **SGD**, now we need to add some functionalities to it. We can get funcitonalites form **[Tensorflow](https://www.tensorflow.org/)=>[Keras](https://keras.io/about/)=>[Optimizers](https://keras.io/api/optimizers/)=>[Experimental](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental)=>[SGD](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental/SGD)**. The type of optimization tensorflow uses is a little bit different. It initializes and keeps the computation in the optimizer function and do the `fit` and `predict` on another function that uses this function. This is done so as to make a generalized optimizer for both `Linear` and `Sequential` models. But we will try to use the vanilla gradient descent and combine the initialization and the `fit` methods.\n",
    "\n",
    "* List Of columns\n",
    "* `learning_rate` - A Tensor, floating point value, or a schedule that is a **[tf.keras.optimizers.schedules.LearningRateSchedule](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/LearningRateSchedule)**, or a callable that takes no arguments and returns the actual value to use. The learning rate. Defaults to $0.001$.\n",
    "* `momentum` - float hyperparameter >= 0 that accelerates gradient descent in the relevant direction and dampens oscillations. Defaults to 0, $i.e.$, vanilla gradient descent.\n",
    "* `weight_decay : Float, defaults to None` - If set, weight decay is applied.\n",
    "* `nesterov : boolean` - Whether to apply Nesterov momentum. Defaults to False.\n",
    "* `weight_decay : Float, defaults to None` - If set, weight decay is applied.\n",
    "* `clipnorm : Float` - If set, the gradient of each weight is individually clipped so that its norm is no higher than this value.\n",
    "* `clipvalue : Float` - If set, the gradient of each weight is clipped to be no higher than this value.\n",
    "* `use_ema : Boolean, defaults to False` - If True, exponential moving average (EMA) is applied. EMA consists of computing an exponential moving average of the weights of the model (as the weight values change after each training batch), and periodically overwriting the weights with their moving average.\n",
    "* `ema_momentum : Float, defaults to 0.99` - Only used if use_ema=True. This is the momentum to use when computing the EMA of the model's weights: `new_average = ema_momentum * old_average + (1 - ema_momentum) * current_variable_value`.\n",
    "\n",
    "# 2.1.1 | List Of Columns \n",
    "\n",
    "This function will only work if there are only $2$ columns, one `feature` and the other one `target`. What if the user gives out a list of columns. For this we nee dto take two different arguemnts form the user and work on them differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8031374",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:46.029340Z",
     "iopub.status.busy": "2023-04-29T11:08:46.028949Z",
     "iopub.status.idle": "2023-04-29T11:08:46.036915Z",
     "shell.execute_reply": "2023-04-29T11:08:46.035430Z"
    },
    "papermill": {
     "duration": 0.04388,
     "end_time": "2023-04-29T11:08:46.039360",
     "exception": false,
     "start_time": "2023-04-29T11:08:45.995480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SGD(X , y):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        weights -= -2 * loss * 0.01\n",
    "        biases -= -2 * loss * 0.01\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b6424e",
   "metadata": {
    "papermill": {
     "duration": 0.031295,
     "end_time": "2023-04-29T11:08:46.101442",
     "exception": false,
     "start_time": "2023-04-29T11:08:46.070147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.1.2 | Learning Rate\n",
    "\n",
    "You remeber we were taking a small part of the loss, not the whole loss. The parameter that defines how much loss we are taking is called the **Learning Rate** \n",
    "\n",
    "You might be thinking that it is not that important, but it is really important concept, a higher learning rate has a high chance that you will never converge to the model, a low very low learning rate means that you will take a very long time to converge for the model \n",
    "\n",
    "Here is a very good image that explains the importance of the learning rate \n",
    "\n",
    "<img src = \"https://www.researchgate.net/profile/Hajar-Feizi/publication/341609757/figure/fig2/AS:894745802977280@1590335431623/Changes-in-the-loss-function-vs-the-epoch-by-the-learning-rate-40.png\">\n",
    "\n",
    "It will be really easy for us to apply this functinality, we just need to change some varaibales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f1c3f36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:46.165962Z",
     "iopub.status.busy": "2023-04-29T11:08:46.164501Z",
     "iopub.status.idle": "2023-04-29T11:08:46.173302Z",
     "shell.execute_reply": "2023-04-29T11:08:46.172235Z"
    },
    "papermill": {
     "duration": 0.043634,
     "end_time": "2023-04-29T11:08:46.176075",
     "exception": false,
     "start_time": "2023-04-29T11:08:46.132441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SGD(X , y , learning_rate = 0.01):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        weights -= -2 * loss * learning_rate\n",
    "        biases -= -2 * loss * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d047413",
   "metadata": {
    "papermill": {
     "duration": 0.031052,
     "end_time": "2023-04-29T11:08:46.238728",
     "exception": false,
     "start_time": "2023-04-29T11:08:46.207676",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.1.3 | Momentum \n",
    "\n",
    "The probelem with gradient descent is this \n",
    "\n",
    "<img src = \"https://winder.ai/blog/2017/img/gradient_descent.svg\">\n",
    "\n",
    "Notice that before we were taking larger steps and as we reached the minimum value, are step size shortened. It is kind of a blessing as well as sometimes a curse for us. The problem is while getting very near to the minimum value, the step size gets so small. that it merely becomes $0$. not $0$ (that means we have reahed the minimum value). To cunter this we introduce momentum to the formula \n",
    "\n",
    "Lets assume you are going to a place that you dont know. Like you dont know where it exists. So what you do is you ask people in the way that where is the place, and they show you the directions. Lets assume the directions are only right $(->)$ and left $(<-)$. So you ask $1^{st}$ person and he says to go to the right $(->)$, then you ask the $2^{nd}$ person and he also asks you to go right $(->)$, so you gain a confidence that you are going right. so you increase your speed. you might skip the $3^{rd}$ person and directly ask the $4^{th}$. \n",
    "\n",
    "Lets assume the same situation from start. So you ask the $1^{st}$ person and he says to go to the right $(->)$ and then you ask the $2^{nd}$ person and he says to go left. But your inner instinct says that you are in the correct direction, but due to the influence of the $2^{nd}$ person, you will go slowly.\n",
    "\n",
    "So you increase and decrease you speed on the basis of the previously gained knowledge. This is the same concept `momentum`, tries to implement. \n",
    "\n",
    "One way of doing so is to add the commulative sum of all the gradients we achived previously like this \n",
    "\n",
    "$$w_{n+1} = w_n - \\frac {dLoss}{dw} \\alpha + (\\sum\\limits_{i = 1}^{n}w_i)$$\n",
    "\n",
    "$$b_{n+1} = b_n - \\frac {dLoss}{db} \\alpha + (\\sum\\limits_{i = 1}^{n}b_i)$$\n",
    "\n",
    "But there are majorly $2$ probelems with this formula $:-$\n",
    "* Rather than fastening the gradients a little bit, it will fasten them exponentially.\n",
    "* This formula values every gradient equal, \n",
    "\n",
    "To rectify this probelm we take the weighted average sum of all the gradients. or we actually multiply the sum with some constant. we change the formula a little bit \n",
    "\n",
    "$$w_{n+1} = w_n - (\\beta w_m + \\alpha(1 - \\beta) \\frac {dLoss}{dw})$$\n",
    "\n",
    "$$b_{n+1} = b_n - (\\beta b_m + \\alpha(1 - \\beta) \\frac {dLoss}{db})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "148a78b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:46.302425Z",
     "iopub.status.busy": "2023-04-29T11:08:46.301264Z",
     "iopub.status.idle": "2023-04-29T11:08:46.310645Z",
     "shell.execute_reply": "2023-04-29T11:08:46.309406Z"
    },
    "papermill": {
     "duration": 0.044293,
     "end_time": "2023-04-29T11:08:46.313188",
     "exception": false,
     "start_time": "2023-04-29T11:08:46.268895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SGD(X , y , learning_rate = 0.01 , momentum = 0):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        m_weights = momentum * m_weights + (1 - momentum) * (-2 * loss)\n",
    "        m_biases = momentum * m_biases + (1 - momentum) * (-2 * loss)\n",
    "        \n",
    "        weights -= m_weights[epochs + 1] * 0.01\n",
    "        biases -= m_biases[epochs + 1] * 0.01\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65fa849",
   "metadata": {
    "papermill": {
     "duration": 0.032048,
     "end_time": "2023-04-29T11:08:46.375495",
     "exception": false,
     "start_time": "2023-04-29T11:08:46.343447",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.1.4 | Nestrov\n",
    "While SGD with momentum can be effective at overcoming oscillations in the cost function, NAG has been shown to converge faster and more reliably in many cases, and thats why we will give the functionality of this function to \n",
    "$$w^n = w^{n-1} - \\beta w_v^{n-1} + (1-\\beta)(w^{n-1} - \\beta w_v^{n-1})$$\n",
    "$$b^n = b^{n-1} - \\beta b_v^{n-1} + (1-\\beta)(b^{n-1} - \\beta b_v^{n-1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31ec51db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:46.440986Z",
     "iopub.status.busy": "2023-04-29T11:08:46.440261Z",
     "iopub.status.idle": "2023-04-29T11:08:46.449413Z",
     "shell.execute_reply": "2023-04-29T11:08:46.448422Z"
    },
    "papermill": {
     "duration": 0.045393,
     "end_time": "2023-04-29T11:08:46.452047",
     "exception": false,
     "start_time": "2023-04-29T11:08:46.406654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SGD(X , y , learning_rate = 0.01 , momentum = 0 , nestrov = False):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "    \n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "    \n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if nestrov :\n",
    "\n",
    "            m_weights = (momentum * m_weights + ((1 - momentum) * (weights - momentum * m_weights))) * -2 * loss \n",
    "            m_biases = (momentum * m_biases + ((1 - momentum) * (weights - momentum * m_biases))) * -2 * loss\n",
    "\n",
    "        else :\n",
    "\n",
    "            m_weights = momentum * m_weights + (1 - momentum) * -2 * loss\n",
    "            m_biases = momentum * m_biases + (1 - momentum) * -2 * loss\n",
    "\n",
    "        weights -= m_weights * learning_rate\n",
    "        biases -= m_biases * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e790d657",
   "metadata": {
    "papermill": {
     "duration": 0.029755,
     "end_time": "2023-04-29T11:08:46.511977",
     "exception": false,
     "start_time": "2023-04-29T11:08:46.482222",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.1.5 | Weight Decay \n",
    "Weight decay is a powerful regularization technique that can help to prevent overfitting and improve the generalization performance of machine learning models trained with SGD optimization.\n",
    "$$u = u + \\gamma u$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4ddc96b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:46.575801Z",
     "iopub.status.busy": "2023-04-29T11:08:46.575348Z",
     "iopub.status.idle": "2023-04-29T11:08:46.585444Z",
     "shell.execute_reply": "2023-04-29T11:08:46.584332Z"
    },
    "papermill": {
     "duration": 0.045915,
     "end_time": "2023-04-29T11:08:46.587950",
     "exception": false,
     "start_time": "2023-04-29T11:08:46.542035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SGD(X , y , learning_rate = 0.01 , momentum = 0 , nestrov = False , weight_decay = None):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "    \n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "    \n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if nestrov :\n",
    "\n",
    "            m_weights = (momentum * m_weights + (1 - momentum) * (weights - momentum * m_weights)) * -2 * loss \n",
    "            m_biases = (momentum * m_biases + (1 - momentum) * (weights - momentum * m_biases)) * -2 * loss\n",
    "\n",
    "        else :\n",
    "\n",
    "            m_weights = momentum * m_weights + (1 - momentum) * -2 * loss\n",
    "            m_biases = momentum * m_biases + (1 - momentum) * -2 * loss\n",
    "\n",
    "        weights -= (m_weights + weight_decay * m_weights) * learning_rate\n",
    "        biases -= (m_biases + weight_decay * m_biases) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a94f772",
   "metadata": {
    "papermill": {
     "duration": 0.033594,
     "end_time": "2023-04-29T11:08:46.657462",
     "exception": false,
     "start_time": "2023-04-29T11:08:46.623868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.1.6 | Clip Norm\n",
    "\n",
    "Sometimes there is a chance that the weights go skyrocketting, which is generally considered a bad idea, parameters in the range of $(-1, 1)$ are considered to be good. Thats why sometimes we use clip-norm , that generates a upper baseline for the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dadd9f0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:46.722360Z",
     "iopub.status.busy": "2023-04-29T11:08:46.721184Z",
     "iopub.status.idle": "2023-04-29T11:08:46.731825Z",
     "shell.execute_reply": "2023-04-29T11:08:46.730881Z"
    },
    "papermill": {
     "duration": 0.046364,
     "end_time": "2023-04-29T11:08:46.734421",
     "exception": false,
     "start_time": "2023-04-29T11:08:46.688057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SGD(X , y , learning_rate = 0.01 , momentum = 0 , nestrov = False , weight_decay = None , clip_norm = None):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "    \n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "    \n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if nestrov :\n",
    "\n",
    "            m_weights = (momentum * m_weights + (1 - momentum) * (weights - momentum * m_weights)) * -2 * loss \n",
    "            m_biases = (momentum * m_biases + (1 - momentum) * (weights - momentum * m_biases)) * -2 * loss\n",
    "\n",
    "        else :\n",
    "\n",
    "            m_weights = momentum * m_weights + (1 - momentum) * -2 * loss\n",
    "            m_biases = momentum * m_biases + (1 - momentum) * -2 * loss\n",
    "\n",
    "        if clip_norm != None:\n",
    "            \n",
    "            weights = np.clip(weights , weights , clip_norm)\n",
    "            biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "        weights -= (m_weights + weight_decay * m_weights) * learning_rate\n",
    "        biases -= (m_biases + weight_decay * m_biases) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a31883",
   "metadata": {
    "papermill": {
     "duration": 0.029881,
     "end_time": "2023-04-29T11:08:46.794848",
     "exception": false,
     "start_time": "2023-04-29T11:08:46.764967",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.1.7 | Clip Values\n",
    "\n",
    "Sometimes the same can go with the `gradients` and thus we also clip them. Higher gradients means that learning rate is very high and need to be lowered. This gives us $2$ tuners to control the gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e53e714",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:46.858762Z",
     "iopub.status.busy": "2023-04-29T11:08:46.858039Z",
     "iopub.status.idle": "2023-04-29T11:08:46.869289Z",
     "shell.execute_reply": "2023-04-29T11:08:46.868100Z"
    },
    "papermill": {
     "duration": 0.045996,
     "end_time": "2023-04-29T11:08:46.872056",
     "exception": false,
     "start_time": "2023-04-29T11:08:46.826060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SGD(X , y , learning_rate = 0.01 , momentum = 0 , nestrov = False , weight_decay = None , clip_norm = None):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "    \n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "    \n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if nestrov :\n",
    "\n",
    "            m_weights = (momentum * m_weights + (1 - momentum) * (weights - momentum * m_weights)) * -2 * loss \n",
    "            m_biases = (momentum * m_biases + (1 - momentum) * (weights - momentum * m_biases)) * -2 * loss\n",
    "\n",
    "        else :\n",
    "\n",
    "            m_weights = momentum * m_weights + (1 - momentum) * -2 * loss\n",
    "            m_biases = momentum * m_biases + (1 - momentum) * -2 * loss\n",
    "\n",
    "        if clip_norm != None:\n",
    "            \n",
    "            weights = np.clip(weights , weights , clip_norm)\n",
    "            biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "        if clip_value != None:\n",
    "            \n",
    "            m_weights = np.clip(m_weights , m_weights , clip_value)\n",
    "            m_biases = np.clip(m_biases , m_biases , clip_value)\n",
    "\n",
    "        weights -= (m_weights + weight_decay * m_weights) * learning_rate\n",
    "        biases -= (m_biases + weight_decay * m_biases) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89803612",
   "metadata": {
    "papermill": {
     "duration": 0.029902,
     "end_time": "2023-04-29T11:08:46.932345",
     "exception": false,
     "start_time": "2023-04-29T11:08:46.902443",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.1.8 | Use EMA\n",
    "\n",
    "Wether to use the estimated momentum average or not. The major use of this function is when `ema_momentum` is given "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18251674",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:46.994724Z",
     "iopub.status.busy": "2023-04-29T11:08:46.993929Z",
     "iopub.status.idle": "2023-04-29T11:08:47.005164Z",
     "shell.execute_reply": "2023-04-29T11:08:47.004102Z"
    },
    "papermill": {
     "duration": 0.045763,
     "end_time": "2023-04-29T11:08:47.008265",
     "exception": false,
     "start_time": "2023-04-29T11:08:46.962502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SGD(X , y , learning_rate = 0.01 , momentum = 0 , nestrov = False , weight_decay = None , clip_norm = None , ema = False):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "    \n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "    \n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if nestrov :\n",
    "\n",
    "            m_weights = (momentum * m_weights + (1 - momentum) * (weights - momentum * m_weights)) * -2 * loss \n",
    "            m_biases = (momentum * m_biases + (1 - momentum) * (weights - momentum * m_biases)) * -2 * loss\n",
    "\n",
    "        else :\n",
    "\n",
    "            m_weights = momentum * m_weights + (1 - momentum) * -2 * loss\n",
    "            m_biases = momentum * m_biases + (1 - momentum) * -2 * loss\n",
    "\n",
    "        if ema:pass\n",
    "\n",
    "        if clip_norm != None:\n",
    "            \n",
    "            weights = np.clip(weights , weights , clip_norm)\n",
    "            biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "        if clip_value != None:\n",
    "            \n",
    "            m_weights = np.clip(m_weights , m_weights , clip_value)\n",
    "            m_biases = np.clip(m_biases , m_biases , clip_value)\n",
    "\n",
    "        weights -= (m_weights + weight_decay * m_weights) * learning_rate\n",
    "        biases -= (m_biases + weight_decay * m_biases) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b3567",
   "metadata": {
    "papermill": {
     "duration": 0.029706,
     "end_time": "2023-04-29T11:08:47.070592",
     "exception": false,
     "start_time": "2023-04-29T11:08:47.040886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.1.9 | EMA Momentum \n",
    "\n",
    "In RMSprop, the exponential moving average (EMA) of the gradient with momentum is used to improve convergence and prevent oscillations during training. The EMA of the gradient with momentum is updated at each iteration and incorporates information about the previous gradients and the current gradient to produce a more stable and consistent update direction. This helps to smooth out the noise in the gradient estimates and helps the optimizer to move more directly towards the minimum of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf051bfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:47.133934Z",
     "iopub.status.busy": "2023-04-29T11:08:47.133293Z",
     "iopub.status.idle": "2023-04-29T11:08:47.145359Z",
     "shell.execute_reply": "2023-04-29T11:08:47.144152Z"
    },
    "papermill": {
     "duration": 0.046778,
     "end_time": "2023-04-29T11:08:47.148265",
     "exception": false,
     "start_time": "2023-04-29T11:08:47.101487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SGD(X , y , learning_rate = 0.01 , momentum = 0 , nestrov = False , weight_decay = None , clip_norm = None , ema = False , ema_momentum = 0.99):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "    \n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "    \n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if nestrov :\n",
    "\n",
    "            m_weights = (momentum * m_weights + (1 - momentum) * (weights - momentum * m_weights)) * -2 * loss \n",
    "            m_biases = (momentum * m_biases + (1 - momentum) * (weights - momentum * m_biases)) * -2 * loss\n",
    "\n",
    "        else :\n",
    "\n",
    "            m_weights = momentum * m_weights + (1 - momentum) * -2 * loss\n",
    "            m_biases = momentum * m_biases + (1 - momentum) * -2 * loss\n",
    "\n",
    "        if ema:\n",
    "            m_weights = ema_momentum * m_weights + (1 - ema_momentum) * -2 * loss\n",
    "            m_biases = ema_momentum * m_biases + (1 - ema_momentum) * -2 * loss\n",
    "\n",
    "        if clip_norm != None:\n",
    "            \n",
    "            weights = np.clip(weights , weights , clip_norm)\n",
    "            biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "        if clip_value != None:\n",
    "            \n",
    "            m_weights = np.clip(m_weights , m_weights , clip_value)\n",
    "            m_biases = np.clip(m_biases , m_biases , clip_value)\n",
    "\n",
    "        weights -= (m_weights + weight_decay * m_weights) * learning_rate\n",
    "        biases -= (m_biases + weight_decay * m_biases) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1bfcaf",
   "metadata": {
    "papermill": {
     "duration": 0.03085,
     "end_time": "2023-04-29T11:08:47.209356",
     "exception": false,
     "start_time": "2023-04-29T11:08:47.178506",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.2 | Methods\n",
    "\n",
    "Now to add more functionalities, we will add different methods to our alogirthm, You can acces the list of methods we are going to use from the same link before we used for `Functionalities`. \n",
    "* `build` - Initialize optimizer variables. SGD optimizer has one variable momentums, only set if self.momentum is not 0.\n",
    "* * `weights` - Weights of the Optimizer\n",
    "* * `biases` - biases of the Optimizer\n",
    "* `compute_gradients` - Compute gradients of loss on trainable variables.\n",
    "* `minimize` - Minimize loss by updating `var_list`.\n",
    "* `update_step` - Update step given gradient and the associated model variable.\n",
    "\n",
    "**Note** - we will stop the function from being usable at some places\n",
    "\n",
    "# 2.2.1 | Build\n",
    "This method initializes all the values in the `SGD`, you can understand this as a constructor, but not perfectly as the one. \n",
    "\n",
    "We will give the user a functionality, that they can intialize there own variables. At the starting the `weights` and `baises` are `None`. If the user passes their own, the function first checks that they are or correct shape and if True assigns the values, else initialize random values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bbea50b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:47.272486Z",
     "iopub.status.busy": "2023-04-29T11:08:47.271741Z",
     "iopub.status.idle": "2023-04-29T11:08:47.294434Z",
     "shell.execute_reply": "2023-04-29T11:08:47.293454Z"
    },
    "papermill": {
     "duration": 0.057312,
     "end_time": "2023-04-29T11:08:47.297051",
     "exception": false,
     "start_time": "2023-04-29T11:08:47.239739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \n",
    "    def __init__(self , \n",
    "                 X , y , \n",
    "                 learning_rate = 0.01 , momentum = 0 , \n",
    "                 nestrov = False , weight_decay = 0 , \n",
    "                 clip_norm = None , clip_value = None , \n",
    "                 use_ema = False , ema_momentum = 0):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.nestrov = nestrov\n",
    "        self.weight_decay = weight_decay\n",
    "        self.clip_norm = clip_norm\n",
    "        self.clip_value = clip_value\n",
    "        self.use_ema = use_ema\n",
    "        self.ema_momentum = ema_momentum\n",
    "    \n",
    "    def build(self , weights = None , biases = None):\n",
    "        \n",
    "        if weights == None:\n",
    "            \n",
    "            self.weights = abs(np.random.randn(self.X.shape[0]))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if weights.shape[0] == self.X.shape[1]:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "                \n",
    "                raise UserWarning(\"Values do not match with the `X` , intializing random Values\")\n",
    "                \n",
    "        \n",
    "        if biases == None:\n",
    "            \n",
    "            self.biases = abs(np.random.randn(1))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if biases.shape[0] == 1:\n",
    "                \n",
    "                self.biases = biases\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                self.biases = abs(np.random.randn(1))\n",
    "                \n",
    "                raise UserWarning(\"Values do not match with the `X` , intializing random Values\")\n",
    "    \n",
    "        self.m_weights = 0\n",
    "        self.m_biases = 0\n",
    "    \n",
    "        self.losses = []\n",
    "    \n",
    "        for _ in range(100):\n",
    "        \n",
    "            pred = np.sum((weights * self.X).T) + biases\n",
    "            \n",
    "            loss = np.sum((pred - self.y) ** 2)\n",
    "            self.losses.append(loss)\n",
    "            \n",
    "            if self.nestrov :\n",
    "\n",
    "                self.m_weights = (self.momentum * self.m_weights + (1 - self.momentum) * (self.weights - self.momentum * self.m_weights)) * -2 * loss \n",
    "                self.m_biases = (self.momentum * self.m_biases + (1 - self.momentum) * (self.biases - self.momentum * self.m_biases)) * -2 * loss\n",
    "\n",
    "            else :\n",
    "\n",
    "                self.m_weights = self.momentum * self.m_weights + (1 - self.momentum) * -2 * loss\n",
    "                self.m_biases = self.momentum * self.m_biases + (1 - self.momentum) * -2 * loss\n",
    "            \n",
    "            if self.use_ema:\n",
    "                \n",
    "                self.m_weights = self.ema_momentum * self.m_weights + (1 - self.ema_momentum) * -2 * loss\n",
    "                self.m_biases = self.ema_momentum * self.m_biases + (1 - self.ema_momentum) * -2 * loss\n",
    "                \n",
    "            if self.clip_norm != None:\n",
    "                \n",
    "                weights = np.clip(weights , weights , self.clip_norm)\n",
    "                biases = np.clip(biases , biases , self.clip_norm)\n",
    "            \n",
    "            if self.clip_value != None:\n",
    "                \n",
    "                self.m_weights = np.clip(self.m_weights , self.m_weights , self.clip_value)\n",
    "                self.m_biases = np.clip(self.m_biases , self.m_biases , self.clip_value)\n",
    "            \n",
    "            weights -= (self.m_weights + self.weight_decay * self.m_weights) * self.learning_rate\n",
    "            biases -= (self.m_biases + self.weight_decay * self.m_biases) * self.learning_rate\n",
    "\n",
    "        return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea92b712",
   "metadata": {
    "papermill": {
     "duration": 0.035141,
     "end_time": "2023-04-29T11:08:47.363373",
     "exception": false,
     "start_time": "2023-04-29T11:08:47.328232",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.2.2 | Compute Gradients\n",
    "\n",
    "Compuute the loss function and gradients for the losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bae91edb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:47.426837Z",
     "iopub.status.busy": "2023-04-29T11:08:47.426051Z",
     "iopub.status.idle": "2023-04-29T11:08:47.439820Z",
     "shell.execute_reply": "2023-04-29T11:08:47.438789Z"
    },
    "papermill": {
     "duration": 0.048166,
     "end_time": "2023-04-29T11:08:47.442385",
     "exception": false,
     "start_time": "2023-04-29T11:08:47.394219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \n",
    "    def __init__(self , \n",
    "                 X , y , \n",
    "                 learning_rate = 0.01 , momentum = 0 , \n",
    "                 nestrov = False , weight_decay = 0 , \n",
    "                 clip_norm = None , clip_value = None , \n",
    "                 use_ema = False , ema_momentum = 0):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.nestrov = nestrov\n",
    "        self.weight_decay = weight_decay\n",
    "        self.clip_norm = clip_norm\n",
    "        self.clip_value = clip_value\n",
    "        self.use_ema = use_ema\n",
    "        self.ema_momentum = ema_momentum\n",
    "    \n",
    "    def build(self , weights = None , biases = None):\n",
    "        \n",
    "        if weights == None:\n",
    "            \n",
    "            self.weights = abs(np.random.randn(self.X.shape[0]))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if weights.shape[0] == self.X.shape[1]:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "                \n",
    "                raise UserWarning(\"Values do not match with the `X` , intializing random Values\")\n",
    "                \n",
    "        \n",
    "        if biases == None:\n",
    "            \n",
    "            self.biases = abs(np.random.randn(1))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if biases.shape[0] == 1:\n",
    "                \n",
    "                self.biases = biases\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                self.biases = abs(np.random.randn(1))\n",
    "                \n",
    "                raise UserWarning(\"Values do not match with the `X` , intializing random Values\")\n",
    "    \n",
    "        self.m_weights = 0\n",
    "        self.m_biases = 0\n",
    "    \n",
    "        self.losses = []\n",
    "\n",
    "    def comput_gradients(self , weights , biases):\n",
    "        \n",
    "            pred = np.sum((weights * self.X).T) + biases\n",
    "            \n",
    "            loss = np.sum((pred - self.y) ** 2)\n",
    "            \n",
    "            yield loss\n",
    "            \n",
    "            self.losses.append(loss)\n",
    "            \n",
    "        #     if self.nestrov :\n",
    "\n",
    "        #         self.m_weights = (self.momentum * self.m_weights + (1 - self.momentum) * (self.weights - self.momentum * self.m_weights)) * -2 * loss \n",
    "        #         self.m_biases = (self.momentum * self.m_biases + (1 - self.momentum) * (self.biases - self.momentum * self.m_biases)) * -2 * loss\n",
    "\n",
    "        #     else :\n",
    "\n",
    "        #         self.m_weights = self.momentum * self.m_weights + (1 - self.momentum) * -2 * loss\n",
    "        #         self.m_biases = self.momentum * self.m_biases + (1 - self.momentum) * -2 * loss\n",
    "            \n",
    "        #     if self.use_ema:\n",
    "                \n",
    "        #         self.m_weights = self.ema_momentum * self.momentum + (1 - self.ema_momentum) * -2 * loss\n",
    "        #         self.m_biases = self.ema_momentum * self.momentum + (1 - self.ema_momentum) * -2 * loss\n",
    "                \n",
    "        #     if self.clip_norm != None:\n",
    "                \n",
    "        #         weights = np.clip(weights , weights , self.clip_norm)\n",
    "        #         biases = np.clip(biases , biases , self.clip_norm)\n",
    "            \n",
    "        #     if self.clip_value != None:\n",
    "                \n",
    "        #         self.m_weights = np.clip(self.m_weights , self.m_weights , self.clip_value)\n",
    "        #         self.m_biases = np.clip(self.m_biases , self.m_biases , self.clip_value)\n",
    "            \n",
    "        #     weights -= (self.m_weights + self.weight_decay * self.m_weights) * self.learning_rate\n",
    "        #     biases -= (self.m_biases + self.weight_decay * self.m_biases) * self.learning_rate\n",
    "\n",
    "        # return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9276b8ec",
   "metadata": {
    "papermill": {
     "duration": 0.030414,
     "end_time": "2023-04-29T11:08:47.503924",
     "exception": false,
     "start_time": "2023-04-29T11:08:47.473510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.2.3 | Minimize \n",
    "Minimze the loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "914930d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:47.568853Z",
     "iopub.status.busy": "2023-04-29T11:08:47.568030Z",
     "iopub.status.idle": "2023-04-29T11:08:47.584855Z",
     "shell.execute_reply": "2023-04-29T11:08:47.583868Z"
    },
    "papermill": {
     "duration": 0.052911,
     "end_time": "2023-04-29T11:08:47.587569",
     "exception": false,
     "start_time": "2023-04-29T11:08:47.534658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \n",
    "    def __init__(self , \n",
    "                 X , y , \n",
    "                 learning_rate = 0.01 , momentum = 0 , \n",
    "                 nestrov = False , weight_decay = 0 , \n",
    "                 clip_norm = None , clip_value = None , \n",
    "                 use_ema = False , ema_momentum = 0):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.nestrov = nestrov\n",
    "        self.weight_decay = weight_decay\n",
    "        self.clip_norm = clip_norm\n",
    "        self.clip_value = clip_value\n",
    "        self.use_ema = use_ema\n",
    "        self.ema_momentum = ema_momentum\n",
    "    \n",
    "    def build(self , weights = None , biases = None):\n",
    "        \n",
    "        if weights == None:\n",
    "            \n",
    "            self.weights = abs(np.random.randn(self.X.shape[0]))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if weights.shape[0] == self.X.shape[1]:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "                \n",
    "                raise UserWarning(\"Values do not match with the `X` , intializing random Values\")\n",
    "                \n",
    "        \n",
    "        if biases == None:\n",
    "            \n",
    "            self.biases = abs(np.random.randn(1))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if biases.shape[0] == 1:\n",
    "                \n",
    "                self.biases = biases\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                self.biases = abs(np.random.randn(1))\n",
    "                \n",
    "                raise UserWarning(\"Values do not match with the `X` , intializing random Values\")\n",
    "    \n",
    "        self.m_weights = 0\n",
    "        self.m_biases = 0\n",
    "    \n",
    "        self.losses = []\n",
    "\n",
    "    def comput_gradients(self , weights , biases):\n",
    "        \n",
    "            pred = np.sum((weights * self.X).T) + biases\n",
    "            \n",
    "            loss = np.sum((pred - self.y) ** 2)\n",
    "            \n",
    "            yield loss\n",
    "            \n",
    "            self.losses.append(loss)\n",
    "            \n",
    "        #     if self.nestrov :\n",
    "\n",
    "        #         self.m_weights = (self.momentum * self.m_weights + (1 - self.momentum) * (self.weights - self.momentum * self.m_weights)) * -2 * loss \n",
    "        #         self.m_biases = (self.momentum * self.m_biases + (1 - self.momentum) * (self.biases - self.momentum * self.m_biases)) * -2 * loss\n",
    "\n",
    "        #     else :\n",
    "\n",
    "        #         self.m_weights = self.momentum * self.m_weights + (1 - self.momentum) * -2 * loss\n",
    "        #         self.m_biases = self.momentum * self.m_biases + (1 - self.momentum) * -2 * loss\n",
    "            \n",
    "        #     if self.use_ema:\n",
    "                \n",
    "        #         self.m_weights = self.ema_momentum * self.momentum + (1 - self.ema_momentum) * -2 * loss\n",
    "        #         self.m_biases = self.ema_momentum * self.momentum + (1 - self.ema_momentum) * -2 * loss\n",
    "    def minimize(self):                \n",
    "        if self.clip_norm != None:\n",
    "            \n",
    "            weights = np.clip(weights , weights , self.clip_norm)\n",
    "            biases = np.clip(biases , biases , self.clip_norm)\n",
    "        \n",
    "        if self.clip_value != None:\n",
    "            \n",
    "            self.m_weights = np.clip(self.m_weights , self.m_weights , self.clip_value)\n",
    "            self.m_biases = np.clip(self.m_biases , self.m_biases , self.clip_value)\n",
    "        \n",
    "        weights -= (self.m_weights + self.weight_decay * self.m_weights) * self.learning_rate\n",
    "        biases -= (self.m_biases + self.weight_decay * self.m_biases) * self.learning_rate\n",
    "\n",
    "        # return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf41281",
   "metadata": {
    "papermill": {
     "duration": 0.029714,
     "end_time": "2023-04-29T11:08:47.647273",
     "exception": false,
     "start_time": "2023-04-29T11:08:47.617559",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.2.4 | Update_Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6a261c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:47.712801Z",
     "iopub.status.busy": "2023-04-29T11:08:47.711963Z",
     "iopub.status.idle": "2023-04-29T11:08:47.732805Z",
     "shell.execute_reply": "2023-04-29T11:08:47.731581Z"
    },
    "papermill": {
     "duration": 0.057782,
     "end_time": "2023-04-29T11:08:47.735980",
     "exception": false,
     "start_time": "2023-04-29T11:08:47.678198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \n",
    "    def __init__(self , \n",
    "                 X , y , \n",
    "                 learning_rate = 0.01 , momentum = 0 , \n",
    "                 nestrov = False , weight_decay = 0 , \n",
    "                 clip_norm = None , clip_value = None , \n",
    "                 use_ema = False , ema_momentum = 0):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.nestrov = nestrov\n",
    "        self.weight_decay = weight_decay\n",
    "        self.clip_norm = clip_norm\n",
    "        self.clip_value = clip_value\n",
    "        self.use_ema = use_ema\n",
    "        self.ema_momentum = ema_momentum\n",
    "    \n",
    "    def build(self , weights = None , biases = None):\n",
    "        \n",
    "        if weights == None:\n",
    "            \n",
    "            self.weights = abs(np.random.randn(self.X.shape[0]))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if weights.shape[0] == self.X.shape[1]:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "                \n",
    "                raise UserWarning(\"Values do not match with the `X` , intializing random Values\")\n",
    "                \n",
    "        \n",
    "        if biases == None:\n",
    "            \n",
    "            self.biases = abs(np.random.randn(1))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if biases.shape[0] == 1:\n",
    "                \n",
    "                self.biases = biases\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                self.biases = abs(np.random.randn(1))\n",
    "                \n",
    "                raise UserWarning(\"Values do not match with the `X` , intializing random Values\")\n",
    "    \n",
    "        self.m_weights = 0\n",
    "        self.m_biases = 0\n",
    "    \n",
    "        self.losses = []\n",
    "\n",
    "    def comput_gradients(self , weights , biases):\n",
    "        \n",
    "            pred = np.sum((weights * self.X).T) + biases\n",
    "            \n",
    "            loss = np.sum((pred - self.y) ** 2)\n",
    "            \n",
    "            yield loss\n",
    "            \n",
    "            self.losses.append(loss)\n",
    "            \n",
    "    def update_step(self , loss):\n",
    "\n",
    "        if self.nestrov :\n",
    "\n",
    "            self.m_weights = (self.momentum * self.m_weights + (1 - self.momentum) * (self.weights - self.momentum * self.m_weights)) * -2 * loss \n",
    "            self.m_biases = (self.momentum * self.m_biases + (1 - self.momentum) * (self.biases - self.momentum * self.m_biases)) * -2 * loss\n",
    "\n",
    "        else :\n",
    "\n",
    "            self.m_weights = self.momentum * self.m_weights + (1 - self.momentum) * -2 * loss\n",
    "            self.m_biases = self.momentum * self.m_biases + (1 - self.momentum) * -2 * loss\n",
    "\n",
    "        if self.use_ema:\n",
    "\n",
    "            self.m_weights = self.ema_momentum * self.m_weights + (1 - self.ema_momentum) * -2 * loss\n",
    "            self.m_biases = self.ema_momentum * self.m_biases + (1 - self.ema_momentum) * -2 * loss\n",
    "    def minimize(self):                \n",
    "        if self.clip_norm != None:\n",
    "            \n",
    "            weights = np.clip(weights , weights , self.clip_norm)\n",
    "            biases = np.clip(biases , biases , self.clip_norm)\n",
    "        \n",
    "        if self.clip_value != None:\n",
    "            \n",
    "            self.m_weights = np.clip(self.m_weights , self.m_weights , self.clip_value)\n",
    "            self.m_biases = np.clip(self.m_biases , self.m_biases , self.clip_value)\n",
    "        \n",
    "        weights -= (self.m_weights + self.weight_decay * self.m_weights) * self.learning_rate\n",
    "        biases -= (self.m_biases + self.weight_decay * self.m_biases) * self.learning_rate\n",
    "\n",
    "        # return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f915ebd3",
   "metadata": {
    "papermill": {
     "duration": 0.032717,
     "end_time": "2023-04-29T11:08:47.801078",
     "exception": false,
     "start_time": "2023-04-29T11:08:47.768361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.2 | SGD Final Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0f7fd9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:47.865914Z",
     "iopub.status.busy": "2023-04-29T11:08:47.864883Z",
     "iopub.status.idle": "2023-04-29T11:08:47.884320Z",
     "shell.execute_reply": "2023-04-29T11:08:47.882996Z"
    },
    "papermill": {
     "duration": 0.054637,
     "end_time": "2023-04-29T11:08:47.887004",
     "exception": false,
     "start_time": "2023-04-29T11:08:47.832367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \n",
    "    def __init__(self , \n",
    "                 X , y , \n",
    "                 learning_rate = 0.01 , momentum = 0 , \n",
    "                 nestrov = False , weight_decay = 0 , \n",
    "                 clip_norm = None , clip_value = None , \n",
    "                 use_ema = False , ema_momentum = 0):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.nestrov = nestrov\n",
    "        self.weight_decay = weight_decay\n",
    "        self.clip_norm = clip_norm\n",
    "        self.clip_value = clip_value\n",
    "        self.use_ema = use_ema\n",
    "        self.ema_momentum = ema_momentum\n",
    "    \n",
    "    def build(self , weights = None , biases = None):\n",
    "        \n",
    "        if weights == None:\n",
    "            \n",
    "            self.weights = abs(np.random.randn(self.X.shape[0]))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if weights.shape[0] == self.X.shape[1]:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "                \n",
    "                raise UserWarning(\"Values do not match with the `X` , intializing random Values\")\n",
    "                \n",
    "        \n",
    "        if biases == None:\n",
    "            \n",
    "            self.biases = abs(np.random.randn(1))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if biases.shape[0] == 1:\n",
    "                \n",
    "                self.biases = biases\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                self.biases = abs(np.random.randn(1))\n",
    "                \n",
    "                raise UserWarning(\"Values do not match with the `X` , intializing random Values\")\n",
    "    \n",
    "        self.m_weights = 0\n",
    "        self.m_biases = 0\n",
    "    \n",
    "        self.losses = []\n",
    "\n",
    "    def comput_gradients(self , weights , biases):\n",
    "        \n",
    "            pred = np.sum((weights * self.X).T) + biases\n",
    "            \n",
    "            loss = np.sum((pred - self.y) ** 2)\n",
    "            \n",
    "            yield loss\n",
    "            \n",
    "            self.losses.append(loss)\n",
    "    def update_step(self , loss):    \n",
    "        \n",
    "        if self.nestrov :\n",
    "\n",
    "            self.m_weights = (self.momentum * self.m_weights + (1 - self.momentum) * (self.weights - self.momentum * self.m_weights)) * -2 * loss \n",
    "            self.m_biases = (self.momentum * self.m_biases + (1 - self.momentum) * (self.biases - self.momentum * self.m_biases)) * -2 * loss\n",
    "\n",
    "        else :\n",
    "\n",
    "            self.m_weights = self.momentum * self.m_weights + (1 - self.momentum) * -2 * loss\n",
    "            self.m_biases = self.momentum * self.m_biases + (1 - self.momentum) * -2 * loss\n",
    "        \n",
    "        if self.use_ema:\n",
    "            \n",
    "            self.m_weights = self.ema_momentum * self.m_weights + (1 - self.ema_momentum) * -2 * loss\n",
    "            self.m_biases = self.ema_momentum * self.m_weights + (1 - self.ema_momentum) * -2 * loss\n",
    "    \n",
    "    def minimize(self):                \n",
    "        if self.clip_norm != None:\n",
    "            \n",
    "            weights = np.clip(weights , weights , self.clip_norm)\n",
    "            biases = np.clip(biases , biases , self.clip_norm)\n",
    "        \n",
    "        if self.clip_value != None:\n",
    "            \n",
    "            self.m_weights = np.clip(self.m_weights , self.m_weights , self.clip_value)\n",
    "            self.m_biases = np.clip(self.m_biases , self.m_biases , self.clip_value)\n",
    "        \n",
    "        weights -= (self.m_weights + self.weight_decay * self.m_weights) * self.learning_rate\n",
    "        biases -= (self.m_biases + self.weight_decay * self.m_biases) * self.learning_rate\n",
    "\n",
    "        return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d26494a",
   "metadata": {
    "papermill": {
     "duration": 0.030236,
     "end_time": "2023-04-29T11:08:47.948157",
     "exception": false,
     "start_time": "2023-04-29T11:08:47.917921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3 | RMS Prop\n",
    "So what is this `RMSProp` and why the hell do we need this thing \n",
    "\n",
    "So actually there were some problems with `SGD`. I just searched on ChatGPT and found that \n",
    "* SGD was applying a fixed learning rate in every situation, and RMSProp adapts the learning rate for the situation\n",
    "* SGD is not really good with sparse inputs, and RMSProp works good with sparse inputs too\n",
    "We know a little bit about `SGD` before the basic vanilla formula for SGD is \n",
    "$$p_{n} = p_{n-1} - \\frac {dLoss}{dp}\\alpha$$\n",
    "The formula for `RMSProp` is simple as hell\n",
    "****\n",
    "$$v_t = \\beta v_{t-1} + (1 - \\beta)\\frac{dLoss}{dp}$$\n",
    "$$u = \\frac {n}{\\sqrt{v_t + e}}\\frac{dLoss}{dp}$$\n",
    "$$p_n = p_{n-1} - u$$\n",
    "****\n",
    "We know everything in this fomrula except of this one guy $e$, what is this $e$ doing here. Lets assume at some point the weight of one feature has became purely $0$. One way to think about this is as that feature doesnt contribute litrally anything to target values, then as we are dividing by `weight`  which is $0$, we will get error, as division by $0$ is not possible, thats why we add a terms $e$ in the weights. the $e$ is so small that when the weights have some value, it doesnt really make any sense, and if the values are $0$. it prevents from $0$ division\n",
    "Remeber this code...?\n",
    "\n",
    "We can modify this a little bit and we can find the code for `RMSProp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c044e757",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:48.011246Z",
     "iopub.status.busy": "2023-04-29T11:08:48.010453Z",
     "iopub.status.idle": "2023-04-29T11:08:48.020388Z",
     "shell.execute_reply": "2023-04-29T11:08:48.019495Z"
    },
    "papermill": {
     "duration": 0.044442,
     "end_time": "2023-04-29T11:08:48.022836",
     "exception": false,
     "start_time": "2023-04-29T11:08:47.978394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SGD(X , y , learning_rate = 0.01 , momentum = 0):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    m_weights = [0] * (100 + 1)\n",
    "    m_biases = [0] * (100 + 1)\n",
    "\n",
    "    predic = []\n",
    "    losses = []\n",
    "\n",
    "    for epochs in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        m_weights[epochs + 1] = momentum * m_weights[epochs] + (1 - momentum) * (-2 * loss)\n",
    "        m_biases[epochs + 1] = momentum * m_biases[epochs] + (1 - momentum) * (-2 * loss)\n",
    "        \n",
    "        weights -= m_weights[epochs + 1] * 0.01\n",
    "        biases -= m_biases[epochs + 1] * 0.01\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "925eb6c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:48.088219Z",
     "iopub.status.busy": "2023-04-29T11:08:48.087090Z",
     "iopub.status.idle": "2023-04-29T11:08:48.096055Z",
     "shell.execute_reply": "2023-04-29T11:08:48.095010Z"
    },
    "papermill": {
     "duration": 0.04553,
     "end_time": "2023-04-29T11:08:48.098563",
     "exception": false,
     "start_time": "2023-04-29T11:08:48.053033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rms_prop(X , y):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for epochs in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        l_weights = 0 * l_weights + (1 - 0) * (-2 * loss)\n",
    "        l_biases = 0 * l_biases + (1 - 0) * (-2 * loss)\n",
    "        \n",
    "        weights -= 1/np.sqrt(l_weights + 1e-6) * 0.01\n",
    "        biases -= 1/np.sqrt(l_biases + 1e-6) * 0.01\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b4f81c",
   "metadata": {
    "papermill": {
     "duration": 0.030973,
     "end_time": "2023-04-29T11:08:48.161407",
     "exception": false,
     "start_time": "2023-04-29T11:08:48.130434",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "And now we have our code for the `RMSProp` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d9db7",
   "metadata": {
    "papermill": {
     "duration": 0.030317,
     "end_time": "2023-04-29T11:08:48.224347",
     "exception": false,
     "start_time": "2023-04-29T11:08:48.194030",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.1 | Functionalities\n",
    "Now lets add some functionalieties to our code as we did before to make it more usable\n",
    "* `learning_rate` - Initial value for the learning rate: either a floating point value, or a **[tf](https://www.tensorflow.org/)=>[keras](https://keras.io/)=>[optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)=>[schedules](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules)=>[LearningRateSchedule](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/LearningRateSchedule)** instance. Defaults to $0.001$.\n",
    "* `rho` - float, defaults to $0.9$. Discounting factor for the old gradients.\n",
    "* `momentum` - float, defaults to $0.0$. If not $0.0$., the optimizer tracks the momentum value, with a decay rate equals to $1$ - momentum.\n",
    "* `weight_decay` - Float, defaults to None. If set, weight decay is applied.\n",
    "* `epsilon` - A small constant for numerical stability. This epsilon is \"epsilon hat\" in the Kingma and Ba paper (in the formula just before Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to $1e-7$.\n",
    "* `clipnorm` - Float. If set, the gradient of each weight is individually clipped so that its norm is no higher than this value.\n",
    "* `clipvalue` - Float. If set, the gradient of each weight is clipped to be no higher than this value.\n",
    "* `use_ema` - \tBoolean, defaults to False. If True, exponential moving average (EMA) is applied. EMA consists of computing an exponential moving average of the weights of the model (as the weight values change after each training batch), and periodically overwriting the weights with their moving average.\n",
    "* `ema_overwrite_frequency` - Int or None, defaults to None. Only used if use_ema=True. Every ema_overwrite_frequency steps of iterations, we overwrite the model variable by its moving average. If None, the optimizer does not overwrite model variables in the middle of training, and you need to explicitly overwrite the variables at the end of training by calling optimizer.finalize_variable_values() (which updates the model variables in-place). When using the built-in fit() training loop, this happens automatically after the last epoch, and you don't need to do anything.\n",
    "\n",
    "# 3.1.1 | Learning Rate\n",
    "\n",
    "It is the learning rate with how the model learns, we had a discussion about this when we were creating the `SGD`. Just scroll a little up and you will find that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9527ec38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:48.289345Z",
     "iopub.status.busy": "2023-04-29T11:08:48.288679Z",
     "iopub.status.idle": "2023-04-29T11:08:48.296323Z",
     "shell.execute_reply": "2023-04-29T11:08:48.295336Z"
    },
    "papermill": {
     "duration": 0.044395,
     "end_time": "2023-04-29T11:08:48.299054",
     "exception": false,
     "start_time": "2023-04-29T11:08:48.254659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rms_prop(X , y , learning_rate = 0.01):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "    predic = []\n",
    "\n",
    "    for epochs in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        l_weights = 0 * l_weights + (1 - 0) * (-2 * loss)\n",
    "        l_biases = 0 * l_biases + (1 - 0) * (-2 * loss)\n",
    "        \n",
    "        weights -= 1/np.sqrt(l_weights + 1e-6) * learning_rate\n",
    "        biases -= 1/np.sqrt(l_biases + 1e-6) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b902b8",
   "metadata": {
    "papermill": {
     "duration": 0.030153,
     "end_time": "2023-04-29T11:08:48.359579",
     "exception": false,
     "start_time": "2023-04-29T11:08:48.329426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.1.2 | Rho\n",
    "This is the $\\beta$ we saw in the formula, we had made our algortihtm such that the $\\beta$ was $0$, now we will replace that $0$ with a vraible, that the user can change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8dff17d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:48.422569Z",
     "iopub.status.busy": "2023-04-29T11:08:48.422081Z",
     "iopub.status.idle": "2023-04-29T11:08:48.431113Z",
     "shell.execute_reply": "2023-04-29T11:08:48.429866Z"
    },
    "papermill": {
     "duration": 0.04306,
     "end_time": "2023-04-29T11:08:48.433658",
     "exception": false,
     "start_time": "2023-04-29T11:08:48.390598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rms_prop(X , y , learning_rate = 0.01 , rho = 0.9):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "\n",
    "    predic = []\n",
    "\n",
    "    for epochs in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        l_weights = rho * l_weights + (1 - rho) * (-2 * loss)\n",
    "        l_biases = rho * l_biases + (1 - rho) * (-2 * loss)\n",
    "        \n",
    "        weights -= 1/np.sqrt(l_weights + 1e-6) * learning_rate\n",
    "        biases -= 1/np.sqrt(l_biases + 1e-6) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b3c3da",
   "metadata": {
    "papermill": {
     "duration": 0.029903,
     "end_time": "2023-04-29T11:08:48.494199",
     "exception": false,
     "start_time": "2023-04-29T11:08:48.464296",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.1.3 | Momentum\n",
    "\n",
    "Momentum allows the optimizer to build up velocity in directions with consistent gradients, leading to faster convergence. It can also help to reduce oscillations in the optimization process by smoothing out variations in the gradient updates. The combination of RMSprop and momentum can lead to more efficient and effective training of deep neural networks.\n",
    "\n",
    "$$m = \\gamma m + (1 - \\gamma)\\frac {dLoss}{dp}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f1712f0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:48.556234Z",
     "iopub.status.busy": "2023-04-29T11:08:48.555807Z",
     "iopub.status.idle": "2023-04-29T11:08:48.565706Z",
     "shell.execute_reply": "2023-04-29T11:08:48.564221Z"
    },
    "papermill": {
     "duration": 0.043932,
     "end_time": "2023-04-29T11:08:48.568224",
     "exception": false,
     "start_time": "2023-04-29T11:08:48.524292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rms_prop(X , y , learning_rate = 0.01 , rho = 0.9 , momentum = 0):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "\n",
    "    u_weights = 0\n",
    "    u_biases = 0\n",
    "\n",
    "    predic = []\n",
    "\n",
    "    for epochs in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        u_weights = rho * l_weights + (1 - rho) * (-2 * loss)\n",
    "        u_biases = rho * l_biases + (1 - rho) * (-2 * loss)\n",
    "\n",
    "        m_weights = momentum * m_weights + (1 - momentum) * loss\n",
    "        m_biases = momentum * m_biases + (1 - momentum) * loss\n",
    "        \n",
    "        weights -= (1/np.sqrt(u_weights + 1e-6) * 1 / np.sqrt(m_weights + 1e-6)) * learning_rate\n",
    "        biases -= (1/np.sqrt(u_biases + 1e-6) * 1 / np.sqrt(m_boases + 1e-6)) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ebf304",
   "metadata": {
    "papermill": {
     "duration": 0.029672,
     "end_time": "2023-04-29T11:08:48.628020",
     "exception": false,
     "start_time": "2023-04-29T11:08:48.598348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.1.4 | Epsilon\n",
    "\n",
    "In RMSprop, epsilon is a small constant used to prevent division by zero and improve numerical stability during the calculation of the adaptive learning rate. It is added to the denominator of the weight update equation to ensure that the divisor is always positive. Epsilon is typically set to a small value, such as 1e-8, and has a minimal impact on the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b090a9af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:48.689374Z",
     "iopub.status.busy": "2023-04-29T11:08:48.688947Z",
     "iopub.status.idle": "2023-04-29T11:08:48.699663Z",
     "shell.execute_reply": "2023-04-29T11:08:48.698421Z"
    },
    "papermill": {
     "duration": 0.044398,
     "end_time": "2023-04-29T11:08:48.702163",
     "exception": false,
     "start_time": "2023-04-29T11:08:48.657765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rms_prop(X , y , learning_rate = 0.01 , rho = 0.9 , momentum = 0 , epsilon = 1e-7):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "\n",
    "    u_weights = 0\n",
    "    u_biases = 0\n",
    "\n",
    "    predic = []\n",
    "\n",
    "    for epochs in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        u_weights = rho * l_weights + (1 - rho) * (-2 * loss)\n",
    "        u_biases = rho * l_biases + (1 - rho) * (-2 * loss)\n",
    "\n",
    "        m_weights = momentum * m_weights + (1 - momentum) * loss\n",
    "        m_biases = momentum * m_biases + (1 - momentum) * loss\n",
    "        \n",
    "        weights -= (1/np.sqrt(u_weights + epsilon) * 1 / np.sqrt(m_weights + epsilon)) * learning_rate\n",
    "        biases -= (1/np.sqrt(u_biases + epsilon) * 1 / np.sqrt(m_boases + epsilon)) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22fbf50",
   "metadata": {
    "papermill": {
     "duration": 0.030401,
     "end_time": "2023-04-29T11:08:48.762532",
     "exception": false,
     "start_time": "2023-04-29T11:08:48.732131",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.1.5 | Clipnorm\n",
    "The `clipnorm` parameter in `RMSprop` is used to prevent the gradient from becoming too large during training, which can cause instability or divergence. When clipnorm is set, the gradient is clipped to a maximum norm value, effectively scaling the gradient if its norm exceeds the specified value. This helps to ensure that the updates to the weights are not too large and that the optimization process remains stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b484091",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:48.825903Z",
     "iopub.status.busy": "2023-04-29T11:08:48.825454Z",
     "iopub.status.idle": "2023-04-29T11:08:48.836437Z",
     "shell.execute_reply": "2023-04-29T11:08:48.835237Z"
    },
    "papermill": {
     "duration": 0.046386,
     "end_time": "2023-04-29T11:08:48.838929",
     "exception": false,
     "start_time": "2023-04-29T11:08:48.792543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rms_prop(X , y , learning_rate = 0.01 , rho = 0.9 , momentum = 0 , epsilon = 1e-7 , clip_norm = None):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "\n",
    "    u_weights = 0\n",
    "    u_biases = 0\n",
    "\n",
    "    predic = []\n",
    "\n",
    "    for epochs in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        u_weights = rho * l_weights + (1 - rho) * (-2 * loss)\n",
    "        u_biases = rho * l_biases + (1 - rho) * (-2 * loss)\n",
    "\n",
    "        m_weights = momentum * m_weights + (1 - momentum) * loss\n",
    "        m_biases = momentum * m_biases + (1 - momentum) * loss\n",
    "\n",
    "        if clip_norm != None:\n",
    "            \n",
    "            weights = np.clip(weights , weights , clip_norm)\n",
    "            biases = np.clip(biases , biases , clip_norm)\n",
    "            \n",
    "        weights -= (1/np.sqrt(u_weights + epsilon) * 1 / np.sqrt(m_weights + epsilon)) * learning_rate\n",
    "        biases -= (1/np.sqrt(u_biases + epsilon) * 1 / np.sqrt(m_boases + epsilon)) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b28d90",
   "metadata": {
    "papermill": {
     "duration": 0.029814,
     "end_time": "2023-04-29T11:08:48.899400",
     "exception": false,
     "start_time": "2023-04-29T11:08:48.869586",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.1.6 | Clip Value\n",
    "\n",
    "If the gradient is larger than the clipvalue threshold, it will be clipped to that value. If the gradient is smaller than the negative clipvalue threshold, it will be clipped to that negative value. This ensures that the gradient remains within a certain range, which can prevent numerical instability and improve convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "95952300",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:48.963187Z",
     "iopub.status.busy": "2023-04-29T11:08:48.962778Z",
     "iopub.status.idle": "2023-04-29T11:08:48.974451Z",
     "shell.execute_reply": "2023-04-29T11:08:48.973189Z"
    },
    "papermill": {
     "duration": 0.046522,
     "end_time": "2023-04-29T11:08:48.977101",
     "exception": false,
     "start_time": "2023-04-29T11:08:48.930579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rms_prop(X , y , learning_rate = 0.01 , rho = 0.9 , momentum = 0 , epsilon = 1e-7 , clip_norm = None , clip_value = None):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "\n",
    "    u_weights = 0\n",
    "    u_biases = 0\n",
    "\n",
    "    predic = []\n",
    "\n",
    "    for epochs in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        u_weights = rho * l_weights + (1 - rho) * (-2 * loss)\n",
    "        u_biases = rho * l_biases + (1 - rho) * (-2 * loss)\n",
    "\n",
    "        m_weights = momentum * m_weights + (1 - momentum) * loss\n",
    "        m_biases = momentum * m_biases + (1 - momentum) * loss\n",
    "\n",
    "        if clip_norm != None:\n",
    "\n",
    "            weights = np.clip(weights , weights , clip_norm)\n",
    "            biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "        if clip_value != None:\n",
    "            \n",
    "            weights = np.clip(m_weigts , m_weights , clip_value)\n",
    "            biases = np.clip(m_biases , m_biases , clip_value)\n",
    "            \n",
    "        weights -= (1/np.sqrt(u_weights + epsilon) * 1 / np.sqrt(m_weights + epsilon)) * learning_rate\n",
    "        biases -= (1/np.sqrt(u_biases + epsilon) * 1 / np.sqrt(m_boases + epsilon)) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af89109b",
   "metadata": {
    "papermill": {
     "duration": 0.029992,
     "end_time": "2023-04-29T11:08:49.037818",
     "exception": false,
     "start_time": "2023-04-29T11:08:49.007826",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.1.7 EMA\n",
    "\n",
    "Wether to use the estimated momentum average or not. The major use of this function is when `ema_momentum` is given "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6dfe4bdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:49.100737Z",
     "iopub.status.busy": "2023-04-29T11:08:49.100288Z",
     "iopub.status.idle": "2023-04-29T11:08:49.111496Z",
     "shell.execute_reply": "2023-04-29T11:08:49.110547Z"
    },
    "papermill": {
     "duration": 0.045911,
     "end_time": "2023-04-29T11:08:49.113853",
     "exception": false,
     "start_time": "2023-04-29T11:08:49.067942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rms_prop(X , y , learning_rate = 0.01 , rho = 0.9 , momentum = 0 , epsilon = 1e-7 , clip_norm = None , clip_value = None , ema = False):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "\n",
    "    u_weights = 0\n",
    "    u_biases = 0\n",
    "\n",
    "    predic = []\n",
    "\n",
    "    for epochs in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        u_weights = rho * l_weights + (1 - rho) * (-2 * loss)\n",
    "        u_biases = rho * l_biases + (1 - rho) * (-2 * loss)\n",
    "\n",
    "        m_weights = momentum * m_weights + (1 - momentum) * loss\n",
    "        m_biases = momentum * m_biases + (1 - momentum) * loss\n",
    "        \n",
    "        if ema:pass\n",
    "\n",
    "        if clip_norm != None:\n",
    "\n",
    "            weights = np.clip(weights , weights , clip_norm)\n",
    "            biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "        if clip_value != None:\n",
    "            \n",
    "            weights = np.clip(m_weigts , m_weights , clip_value)\n",
    "            biases = np.clip(m_biases , m_biases , clip_value)\n",
    "            \n",
    "        weights -= (1/np.sqrt(u_weights + epsilon) * 1 / np.sqrt(m_weights + epsilon)) * learning_rate\n",
    "        biases -= (1/np.sqrt(u_biases + epsilon) * 1 / np.sqrt(m_boases + epsilon)) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6ae83f",
   "metadata": {
    "papermill": {
     "duration": 0.029676,
     "end_time": "2023-04-29T11:08:49.173441",
     "exception": false,
     "start_time": "2023-04-29T11:08:49.143765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.1.8 | EMA Momentum\n",
    "\n",
    "In RMSprop, the exponential moving average (EMA) of the gradient with momentum is used to improve convergence and prevent oscillations during training. The EMA of the gradient with momentum is updated at each iteration and incorporates information about the previous gradients and the current gradient to produce a more stable and consistent update direction. This helps to smooth out the noise in the gradient estimates and helps the optimizer to move more directly towards the minimum of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f6933b48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:49.236633Z",
     "iopub.status.busy": "2023-04-29T11:08:49.236195Z",
     "iopub.status.idle": "2023-04-29T11:08:49.248574Z",
     "shell.execute_reply": "2023-04-29T11:08:49.247668Z"
    },
    "papermill": {
     "duration": 0.047116,
     "end_time": "2023-04-29T11:08:49.250912",
     "exception": false,
     "start_time": "2023-04-29T11:08:49.203796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rms_prop(X , y , learning_rate = 0.01 , rho = 0.9 , momentum = 0 , epsilon = 1e-7 , clip_norm = None , clip_value = None , ema = False , ema_momentum = 0):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "\n",
    "    u_weights = 0\n",
    "    u_biases = 0\n",
    "\n",
    "    predic = []\n",
    "\n",
    "    for epochs in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        u_weights = rho * l_weights + (1 - rho) * (-2 * loss)\n",
    "        u_biases = rho * l_biases + (1 - rho) * (-2 * loss)\n",
    "\n",
    "        m_weights = momentum * m_weights + (1 - momentum) * loss\n",
    "        m_biases = momentum * m_biases + (1 - momentum) * loss\n",
    "        \n",
    "        if ema:\n",
    "            \n",
    "            m_weights = ema_momentum * m_weights + (1 - ema_momentum) * -2 * loss\n",
    "            m_biases = ema_momentum * m_biases + (1 - ema_momentum) * -2 * loss\n",
    "\n",
    "        if clip_norm != None:\n",
    "\n",
    "            weights = np.clip(weights , weights , clip_norm)\n",
    "            biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "        if clip_value != None:\n",
    "            \n",
    "            weights = np.clip(m_weigts , m_weights , clip_value)\n",
    "            biases = np.clip(m_biases , m_biases , clip_value)\n",
    "            \n",
    "        weights -= (1/np.sqrt(u_weights + epsilon) * 1 / np.sqrt(m_weights + epsilon)) * learning_rate\n",
    "        biases -= (1/np.sqrt(u_biases + epsilon) * 1 / np.sqrt(m_boases + epsilon)) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55378584",
   "metadata": {
    "papermill": {
     "duration": 0.030248,
     "end_time": "2023-04-29T11:08:49.312036",
     "exception": false,
     "start_time": "2023-04-29T11:08:49.281788",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.2 | Methods\n",
    "\n",
    "Now to add more functionalities, we will add different methods to our alogirthm, You can acces the list of methods we are going to use from the same link before we used for `Functionalities`. \n",
    "* `build` - Initialize optimizer variables. SGD optimizer has one variable momentums, only set if self.momentum is not 0.\n",
    "* * `weights` - Weights of the Optimizer\n",
    "* * `biases` - biases of the Optimizer\n",
    "* `compute_gradients` - Compute gradients of loss on trainable variables.\n",
    "* `minimize` - Minimize loss by updating `var_list`.\n",
    "* `update_step` - Update step given gradient and the associated model variable.\n",
    "\n",
    "**Note** - we will stop the function from being usable at some places\n",
    "\n",
    "# 3.2.1 | Build\n",
    "This method initializes all the values in the `rms_prop`, you can understand this as a constructor, but not perfectly as the one. \n",
    "\n",
    "We will give the user a functionality, that they can intialize there own variables. At the starting the `weights` and `baises` are `None`. If the user passes their own, the function first checks that they are or correct shape and if True assigns the values, else initialize random values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "59af8c50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:49.377768Z",
     "iopub.status.busy": "2023-04-29T11:08:49.376537Z",
     "iopub.status.idle": "2023-04-29T11:08:49.389357Z",
     "shell.execute_reply": "2023-04-29T11:08:49.388440Z"
    },
    "papermill": {
     "duration": 0.048873,
     "end_time": "2023-04-29T11:08:49.391822",
     "exception": false,
     "start_time": "2023-04-29T11:08:49.342949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class rms_prop:\n",
    "    def __init__(X , y , \n",
    "                 learning_rate = 0.01 , rho = 0.9 , \n",
    "                 momentum = 0 , epsilon = 1e-7 , \n",
    "                 clip_norm = None , clip_value = None , \n",
    "                 ema = False , ema_momentum = 0):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.learning_rate = learning_rate\n",
    "        self.rho = rho\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.clip_norm = clip-norm\n",
    "        self.ema = ema\n",
    "        self.ema_momentum = ema_momentum\n",
    "\n",
    "    def build(self , weights = None , biases = None):\n",
    "        \n",
    "        if weights == None:\n",
    "            \n",
    "            self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if weights.shape[1] == self.X.shape[1]:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match. Using random variables\")\n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "\n",
    "        if biases == None:\n",
    "            \n",
    "            self.biases = abs(np.random.randn(1))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if baises.shape[1] == 1:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match. Using random variables\")\n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "\n",
    "        self.m_weights = 0\n",
    "        self.m_biases = 0\n",
    "\n",
    "        self.u_weights = 0\n",
    "        self.u_biases = 0\n",
    "\n",
    "        self.predic = []\n",
    "\n",
    "    # for epochs in range(100):\n",
    "\n",
    "    #     pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "    #     loss = np.sum(pred - y)\n",
    "    #     losses.append(loss)\n",
    "\n",
    "    #     u_weights = rho * l_weights + (1 - rho) * (-2 * loss)\n",
    "    #     u_biases = rho * l_biases + (1 - rho) * (-2 * loss)\n",
    "\n",
    "    #     m_weights = momentum * m_weights + (1 - momentum) * loss\n",
    "    #     m_biases = momentum * m_biases + (1 - momentum) * loss\n",
    "        \n",
    "    #     if ema:\n",
    "            \n",
    "    #         m_weights = ema_momentum * m_weights + (1 - ema_momentum) * -2 * loss\n",
    "    #         m_biases = ema_momentum * m_biases + (1 - ema_momentum) * -2 * loss\n",
    "\n",
    "    #     if clip_norm != None:\n",
    "\n",
    "    #         weights = np.clip(weights , weights , clip_norm)\n",
    "    #         biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "    #     if clip_value != None:\n",
    "            \n",
    "    #         weights = np.clip(m_weigts , m_weights , clip_value)\n",
    "    #         biases = np.clip(m_biases , m_biases , clip_value)\n",
    "            \n",
    "    #     weights -= (1/np.sqrt(u_weights + epsilon) * 1 / np.sqrt(m_weights + epsilon)) * learning_rate\n",
    "    #     biases -= (1/np.sqrt(u_biases + epsilon) * 1 / np.sqrt(m_boases + epsilon)) * learning_rate\n",
    "\n",
    "    # return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4985ce7",
   "metadata": {
    "papermill": {
     "duration": 0.029789,
     "end_time": "2023-04-29T11:08:49.451467",
     "exception": false,
     "start_time": "2023-04-29T11:08:49.421678",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.2.2 | Compute Gradients\n",
    "\n",
    "Compuute the loss function and gradients for the losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3320da81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:49.513550Z",
     "iopub.status.busy": "2023-04-29T11:08:49.512734Z",
     "iopub.status.idle": "2023-04-29T11:08:49.526610Z",
     "shell.execute_reply": "2023-04-29T11:08:49.525240Z"
    },
    "papermill": {
     "duration": 0.048201,
     "end_time": "2023-04-29T11:08:49.529450",
     "exception": false,
     "start_time": "2023-04-29T11:08:49.481249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class rms_prop:\n",
    "    def __init__(X , y , \n",
    "                 learning_rate = 0.01 , rho = 0.9 , \n",
    "                 momentum = 0 , epsilon = 1e-7 , \n",
    "                 clip_norm = None , clip_value = None , \n",
    "                 ema = False , ema_momentum = 0):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.learning_rate = learning_rate\n",
    "        self.rho = rho\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.clip_norm = clip-norm\n",
    "        self.ema = ema\n",
    "        self.ema_momentum = ema_momentum\n",
    "\n",
    "    def build(self , weights = None , biases = None):\n",
    "        \n",
    "        if weights == None:\n",
    "            \n",
    "            self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if weights.shape[1] == self.X.shape[1]:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match. Using random variables\")\n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "\n",
    "        if biases == None:\n",
    "            \n",
    "            self.biases = abs(np.random.randn(1))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if baises.shape[1] == 1:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match. Using random variables\")\n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "\n",
    "        self.m_weights = 0\n",
    "        self.m_biases = 0\n",
    "\n",
    "        self.u_weights = 0\n",
    "        self.u_biases = 0\n",
    "\n",
    "        self.predic = []\n",
    "\n",
    "    def compute_gradients(self , weights , biases):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        \n",
    "        yield loss\n",
    "        \n",
    "        losses.append(loss)\n",
    "\n",
    "    #     u_weights = rho * l_weights + (1 - rho) * (-2 * loss)\n",
    "    #     u_biases = rho * l_biases + (1 - rho) * (-2 * loss)\n",
    "\n",
    "    #     m_weights = momentum * m_weights + (1 - momentum) * loss\n",
    "    #     m_biases = momentum * m_biases + (1 - momentum) * loss\n",
    "        \n",
    "    #     if ema:\n",
    "            \n",
    "    #         m_weights = ema_momentum * m_weights + (1 - ema_momentum) * -2 * loss\n",
    "    #         m_biases = ema_momentum * m_biases + (1 - ema_momentum) * -2 * loss\n",
    "\n",
    "    #     if clip_norm != None:\n",
    "\n",
    "    #         weights = np.clip(weights , weights , clip_norm)\n",
    "    #         biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "    #     if clip_value != None:\n",
    "            \n",
    "    #         weights = np.clip(m_weigts , m_weights , clip_value)\n",
    "    #         biases = np.clip(m_biases , m_biases , clip_value)\n",
    "            \n",
    "    #     weights -= (1/np.sqrt(u_weights + epsilon) * 1 / np.sqrt(m_weights + epsilon)) * learning_rate\n",
    "    #     biases -= (1/np.sqrt(u_biases + epsilon) * 1 / np.sqrt(m_boases + epsilon)) * learning_rate\n",
    "\n",
    "    # return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c4f19c",
   "metadata": {
    "papermill": {
     "duration": 0.03025,
     "end_time": "2023-04-29T11:08:49.590179",
     "exception": false,
     "start_time": "2023-04-29T11:08:49.559929",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.2.3 | Minimize\n",
    "Minimze the loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "26d3f09e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:49.653944Z",
     "iopub.status.busy": "2023-04-29T11:08:49.653210Z",
     "iopub.status.idle": "2023-04-29T11:08:49.670121Z",
     "shell.execute_reply": "2023-04-29T11:08:49.669064Z"
    },
    "papermill": {
     "duration": 0.052156,
     "end_time": "2023-04-29T11:08:49.672632",
     "exception": false,
     "start_time": "2023-04-29T11:08:49.620476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class rms_prop:\n",
    "    def __init__(X , y , \n",
    "                 learning_rate = 0.01 , rho = 0.9 , \n",
    "                 momentum = 0 , epsilon = 1e-7 , \n",
    "                 clip_norm = None , clip_value = None , \n",
    "                 ema = False , ema_momentum = 0):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.learning_rate = learning_rate\n",
    "        self.rho = rho\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.clip_norm = clip-norm\n",
    "        self.ema = ema\n",
    "        self.ema_momentum = ema_momentum\n",
    "\n",
    "    def build(self , weights = None , biases = None):\n",
    "        \n",
    "        if weights == None:\n",
    "            \n",
    "            self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if weights.shape[1] == self.X.shape[1]:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match. Using random variables\")\n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "\n",
    "        if biases == None:\n",
    "            \n",
    "            self.biases = abs(np.random.randn(1))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if baises.shape[1] == 1:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match. Using random variables\")\n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "\n",
    "        self.m_weights = 0\n",
    "        self.m_biases = 0\n",
    "\n",
    "        self.u_weights = 0\n",
    "        self.u_biases = 0\n",
    "\n",
    "        self.predic = []\n",
    "\n",
    "    def compute_gradients(self , weights , biases):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        \n",
    "        yield loss\n",
    "        \n",
    "        losses.append(loss)\n",
    "\n",
    "    #     u_weights = rho * l_weights + (1 - rho) * (-2 * loss)\n",
    "    #     u_biases = rho * l_biases + (1 - rho) * (-2 * loss)\n",
    "\n",
    "    #     m_weights = momentum * m_weights + (1 - momentum) * loss\n",
    "    #     m_biases = momentum * m_biases + (1 - momentum) * loss\n",
    "        \n",
    "    #     if ema:\n",
    "            \n",
    "    #         m_weights = ema_momentum * m_weights + (1 - ema_momentum) * -2 * loss\n",
    "    #         m_biases = ema_momentum * m_biases + (1 - ema_momentum) * -2 * loss\n",
    "    def minimize(self):\n",
    "        \n",
    "        if self.clip_norm != None:\n",
    "\n",
    "            self.weights = np.clip(self.weights , self.weights , self.clip_norm)\n",
    "            self.biases = np.clip(self.biases , self.biases , self.clip_norm)\n",
    "\n",
    "        if self.clip_value != None:\n",
    "            \n",
    "            self.weights = np.clip(self.m_weights , self.m_weights , self.clip_value)\n",
    "            self.biases = np.clip(self.m_biases , self.m_biases , self.clip_value)\n",
    "            \n",
    "        self.weights -= (1/np.sqrt(self.u_weights + self.epsilon) * 1 / np.sqrt(self.m_weights + self.epsilon)) * learning_rate\n",
    "        self.biases -= (1/np.sqrt(self.u_biases + self.epsilon) * 1 / np.sqrt(self.m_biases + self.epsilon)) * learning_rate\n",
    "\n",
    "        return self.weights , self.biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df315dad",
   "metadata": {
    "papermill": {
     "duration": 0.029595,
     "end_time": "2023-04-29T11:08:49.732161",
     "exception": false,
     "start_time": "2023-04-29T11:08:49.702566",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.2.4 | Update_Step\n",
    "Update the values accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f95500fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:49.794391Z",
     "iopub.status.busy": "2023-04-29T11:08:49.793606Z",
     "iopub.status.idle": "2023-04-29T11:08:49.813452Z",
     "shell.execute_reply": "2023-04-29T11:08:49.812201Z"
    },
    "papermill": {
     "duration": 0.05414,
     "end_time": "2023-04-29T11:08:49.816437",
     "exception": false,
     "start_time": "2023-04-29T11:08:49.762297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class rms_prop:\n",
    "    def __init__(X , y , \n",
    "                 learning_rate = 0.01 , rho = 0.9 , \n",
    "                 momentum = 0 , epsilon = 1e-7 , \n",
    "                 clip_norm = None , clip_value = None , \n",
    "                 ema = False , ema_momentum = 0):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.learning_rate = learning_rate\n",
    "        self.rho = rho\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.clip_norm = clip-norm\n",
    "        self.ema = ema\n",
    "        self.ema_momentum = ema_momentum\n",
    "\n",
    "    def build(self , weights = None , biases = None):\n",
    "        \n",
    "        if weights == None:\n",
    "            \n",
    "            self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if weights.shape[1] == self.X.shape[1]:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match. Using random variables\")\n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "\n",
    "        if biases == None:\n",
    "            \n",
    "            self.biases = abs(np.random.randn(1))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if baises.shape[1] == 1:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match. Using random variables\")\n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "\n",
    "        self.m_weights = 0\n",
    "        self.m_biases = 0\n",
    "\n",
    "        self.u_weights = 0\n",
    "        self.u_biases = 0\n",
    "\n",
    "        self.predic = []\n",
    "\n",
    "    def compute_gradients(self , weights , biases):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        \n",
    "        yield loss\n",
    "        \n",
    "        losses.append(loss)\n",
    "    def update_step(self , loss):\n",
    "        \n",
    "        self.u_weights = self.rho * self.u_weights + (1 - self.rho) * (-2 * loss)\n",
    "        self.u_biases = self.rho * self.u_biases + (1 - self.rho) * (-2 * loss)\n",
    "\n",
    "        self.m_weights = self.momentum * self.m_weights + (1 - self.momentum) * loss\n",
    "        self.m_biases = self.momentum * self.m_biases + (1 - self.momentum) * loss\n",
    "        \n",
    "        if self.ema:\n",
    "            \n",
    "            self.m_weights = self.ema_momentum * self.m_weights + (1 - self.ema_momentum) * -2 * loss\n",
    "            self.m_biases = self.ema_momentum * self.m_biases + (1 - self.ema_momentum) * -2 * loss\n",
    "    \n",
    "    def minimize(self):\n",
    "        \n",
    "        if self.clip_norm != None:\n",
    "\n",
    "            self.weights = np.clip(self.weights , self.weights , self.clip_norm)\n",
    "            self.biases = np.clip(self.biases , self.biases , self.clip_norm)\n",
    "\n",
    "        if self.clip_value != None:\n",
    "            \n",
    "            self.weights = np.clip(m_weigts , self.m_weights , self.clip_value)\n",
    "            self.biases = np.clip(self.m_biases , self.m_biases , self.clip_value)\n",
    "            \n",
    "        self.weights -= (1/np.sqrt(self.u_weights + self.epsilon) * 1 / np.sqrt(self.m_weights + self.epsilon)) * self.learning_rate\n",
    "        self.biases -= (1/np.sqrt(self.u_biases + self.epsilon) * 1 / np.sqrt(self.m_biases + self.epsilon)) * self.learning_rate\n",
    "\n",
    "        return self.weights , self.biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7bf33d",
   "metadata": {
    "papermill": {
     "duration": 0.029649,
     "end_time": "2023-04-29T11:08:49.876281",
     "exception": false,
     "start_time": "2023-04-29T11:08:49.846632",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.2 | RMSProp Final Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ff09d6d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:49.938499Z",
     "iopub.status.busy": "2023-04-29T11:08:49.938079Z",
     "iopub.status.idle": "2023-04-29T11:08:49.958214Z",
     "shell.execute_reply": "2023-04-29T11:08:49.957006Z"
    },
    "papermill": {
     "duration": 0.054493,
     "end_time": "2023-04-29T11:08:49.961155",
     "exception": false,
     "start_time": "2023-04-29T11:08:49.906662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class rms_prop:\n",
    "    def __init__(X , y , \n",
    "                 learning_rate = 0.01 , rho = 0.9 , \n",
    "                 momentum = 0 , epsilon = 1e-7 , \n",
    "                 clip_norm = None , clip_value = None , \n",
    "                 ema = False , ema_momentum = 0):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.learning_rate = learning_rate\n",
    "        self.rho = rho\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.clip_norm = clip-norm\n",
    "        self.ema = ema\n",
    "        self.ema_momentum = ema_momentum\n",
    "\n",
    "    def build(self , weights = None , biases = None):\n",
    "        \n",
    "        if weights == None:\n",
    "            \n",
    "            self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if weights.shape[1] == self.X.shape[1]:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match. Using random variables\")\n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "\n",
    "        if biases == None:\n",
    "            \n",
    "            self.biases = abs(np.random.randn(1))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if baises.shape[1] == 1:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match. Using random variables\")\n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "\n",
    "        self.m_weights = 0\n",
    "        self.m_biases = 0\n",
    "\n",
    "        self.u_weights = 0\n",
    "        self.u_biases = 0\n",
    "\n",
    "        self.predic = []\n",
    "\n",
    "    def compute_gradients(self , weights , biases):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        \n",
    "        yield loss\n",
    "        \n",
    "        losses.append(loss)\n",
    "    def update_step(self , loss):\n",
    "        \n",
    "        self.u_weights = self.rho * self.u_weights + (1 - self.rho) * (-2 * loss)\n",
    "        self.u_biases = self.rho * self.u_biases + (1 - self.rho) * (-2 * loss)\n",
    "\n",
    "        self.m_weights = self.momentum * self.m_weights + (1 - self.momentum) * loss\n",
    "        self.m_biases = self.momentum * self.m_biases + (1 - self.momentum) * loss\n",
    "        \n",
    "        if self.ema:\n",
    "            \n",
    "            self.m_weights = self.ema_momentum * self.m_weights + (1 - self.ema_momentum) * -2 * loss\n",
    "            self.m_biases = self.ema_momentum * self.m_biases + (1 - self.ema_momentum) * -2 * loss\n",
    "    \n",
    "    def minimize(self):\n",
    "        \n",
    "        if self.clip_norm != None:\n",
    "\n",
    "            self.weights = np.clip(self.weights , self.weights , self.clip_norm)\n",
    "            self.biases = np.clip(self.biases , self.biases , self.clip_norm)\n",
    "\n",
    "        if self.clip_value != None:\n",
    "            \n",
    "            self.weights = np.clip(m_weigts , self.m_weights , self.clip_value)\n",
    "            self.biases = np.clip(self.m_biases , self.m_biases , self.clip_value)\n",
    "            \n",
    "        self.weights -= (1/np.sqrt(self.u_weights + self.epsilon) * 1 / np.sqrt(self.m_weights + self.epsilon)) * self.learning_rate\n",
    "        self.biases -= (1/np.sqrt(self.u_biases + self.epsilon) * 1 / np.sqrt(self.m_biases + self.epsilon)) * self.learning_rate\n",
    "\n",
    "        return self.weights , self.biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad95c75",
   "metadata": {
    "papermill": {
     "duration": 0.032197,
     "end_time": "2023-04-29T11:08:50.023330",
     "exception": false,
     "start_time": "2023-04-29T11:08:49.991133",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4 | AdaGrad\n",
    "\n",
    "So what the hell is this `adagrad` and why do we even use it. Frankly speaking I dont know a good explanation but what is have learnt is `RMS_Prop` uses the average of shorter list of gradients that were previously used, whereas `adagrad` uses all the list of previous gradients. \n",
    "\n",
    "Though it has a similar formula....\n",
    "****\n",
    "$$w = \\frac {\\alpha g_{n-1}}{\\sqrt{g_n + e}}$$\n",
    "****\n",
    "\n",
    "Remember the code we had for the first time we created `SGD`...?\n",
    "\n",
    "We can somewhat tweek that a little bit to make it working for `AdaGrad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5187f28e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:50.085273Z",
     "iopub.status.busy": "2023-04-29T11:08:50.084813Z",
     "iopub.status.idle": "2023-04-29T11:08:50.092747Z",
     "shell.execute_reply": "2023-04-29T11:08:50.091577Z"
    },
    "papermill": {
     "duration": 0.04144,
     "end_time": "2023-04-29T11:08:50.095035",
     "exception": false,
     "start_time": "2023-04-29T11:08:50.053595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adagrad(X , y):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        weights -= -2 * loss * 0.01\n",
    "        biases -= -2 * loss * 0.01\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47810e27",
   "metadata": {
    "papermill": {
     "duration": 0.029894,
     "end_time": "2023-04-29T11:08:50.155546",
     "exception": false,
     "start_time": "2023-04-29T11:08:50.125652",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Till now we were using the previously used `m_weights` or any other variable to update itself. We can use this approach here too, but there we only needed the updated part not the previous part, but according to the formula, we need both the parts. One was of doing this is to create a list that stores the value of all the `gradients`. But we can make this soluiton more effective. Even in the list we need to look up at the two consecutive values only. not the whole list. Lets assume we need to update the value parameters of the index `n`. for that we need only the gradients at index `n` and `n-1`. we do not need othet indexes like `n+1` or `n-2`. We can assume that we are iterating over a list where the window size is only of $2$ indexes. To make this more simple, what we can do is to make a list of $2$ indexes only. At first both are $0$. When we update the grdient of index $1$, and use the gradients of index $0$ and $1$ in the parameter update process, then we can replace the gradient at $0$ with $1$ and then compute for, say $2$ and then repeat the same process. This looks more simpler when you look at the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b6ee0b98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:50.217147Z",
     "iopub.status.busy": "2023-04-29T11:08:50.216746Z",
     "iopub.status.idle": "2023-04-29T11:08:50.225414Z",
     "shell.execute_reply": "2023-04-29T11:08:50.224167Z"
    },
    "papermill": {
     "duration": 0.04249,
     "end_time": "2023-04-29T11:08:50.227920",
     "exception": false,
     "start_time": "2023-04-29T11:08:50.185430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adagrad(X , y):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    losses = []\n",
    "    grad = [0] * 2 # Intialize of only 2 elements\n",
    "\n",
    "    for _ in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        grad[1] = -2 * loss # calculate the gradient for 1 epoch\n",
    "        \n",
    "        weights -= np.sqrt(-1 / (grad[1] + 1e-7)) * grad[0] * 0.01#|Update the weights using gradient \n",
    "        biases -= np.sqrt(-1 / (grad[1] + 1e-7)) * grad[0] * 0.01#-|\n",
    "\n",
    "        grad[1] = grad[0] # Replace the 1 gradient with 0 and move on \n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b5038c",
   "metadata": {
    "papermill": {
     "duration": 0.029585,
     "end_time": "2023-04-29T11:08:50.288593",
     "exception": false,
     "start_time": "2023-04-29T11:08:50.259008",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4.1 | Functionalities\n",
    "Now lets add some functionalieties to our code as we did before to make it more usable\n",
    "* `learning_rate` - Initial value for the learning rate: either a floating point value, or a **[tf](https://www.tensorflow.org/)=>[keras](https://keras.io/)=>[optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)=>[schedules](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules)=>[LearningRateSchedule](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/LearningRateSchedule)** instance. Defaults to $0.001$.\n",
    "* `epsilon` - A small constant for numerical stability. This epsilon is \"epsilon hat\" in the Kingma and Ba paper (in the formula just before Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to $1e-7$.\n",
    "* `weight_decay` - Float, defaults to None. If set, weight decay is applied.\n",
    "* `clipnorm` - Float. If set, the gradient of each weight is individually clipped so that its norm is no higher than this value.\n",
    "* `clipvalue` - Float. If set, the gradient of each weight is clipped to be no higher than this value.\n",
    "* `use_ema` - \tBoolean, defaults to False. If True, exponential moving average (EMA) is applied. EMA consists of computing an exponential moving average of the weights of the model (as the weight values change after each training batch), and periodically overwriting the weights with their moving average.\n",
    "* `ema_overwrite_frequency` - Int or None, defaults to None. Only used if use_ema=True. Every ema_overwrite_frequency steps of iterations, we overwrite the model variable by its moving average. If None, the optimizer does not overwrite model variables in the middle of training, and you need to explicitly overwrite the variables at the end of training by calling optimizer.finalize_variable_values() (which updates the model variables in-place). When using the built-in fit() training loop, this happens automatically after the last epoch, and you don't need to do anything.\n",
    "\n",
    "# 4.1.1 | Learning Rate\n",
    "\n",
    "It is the learning rate with how the model learns, we had a discussion about this when we were creating the `SGD`. Just scroll a little up and you will find that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8edcba09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:50.352779Z",
     "iopub.status.busy": "2023-04-29T11:08:50.351241Z",
     "iopub.status.idle": "2023-04-29T11:08:50.365924Z",
     "shell.execute_reply": "2023-04-29T11:08:50.361913Z"
    },
    "papermill": {
     "duration": 0.052155,
     "end_time": "2023-04-29T11:08:50.371304",
     "exception": false,
     "start_time": "2023-04-29T11:08:50.319149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adagrad(X , y , learning_rate = 0.01):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    losses = []\n",
    "    grad = [0] * 2\n",
    "\n",
    "    for _ in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        grad[1] = -2 * loss\n",
    "        \n",
    "        weights -= np.sqrt(-1 / (grad[1] + 1e-7)) * grad[0] * learning_rate \n",
    "        biases -= np.sqrt(-1 / (grad[1] + 1e-7)) * grad[0] * learning_rate\n",
    "\n",
    "        grad[1] = grad[0]\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39472334",
   "metadata": {
    "papermill": {
     "duration": 0.044639,
     "end_time": "2023-04-29T11:08:50.452492",
     "exception": false,
     "start_time": "2023-04-29T11:08:50.407853",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4.1.4 | Epsilon\n",
    "\n",
    "In RMSprop, epsilon is a small constant used to prevent division by zero and improve numerical stability during the calculation of the adaptive learning rate. It is added to the denominator of the weight update equation to ensure that the divisor is always positive. Epsilon is typically set to a small value, such as 1e-8, and has a minimal impact on the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4ac44360",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:50.517060Z",
     "iopub.status.busy": "2023-04-29T11:08:50.516553Z",
     "iopub.status.idle": "2023-04-29T11:08:50.525268Z",
     "shell.execute_reply": "2023-04-29T11:08:50.523900Z"
    },
    "papermill": {
     "duration": 0.043416,
     "end_time": "2023-04-29T11:08:50.527802",
     "exception": false,
     "start_time": "2023-04-29T11:08:50.484386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adagrad(X , y , learning_rate = 0.01 , epsilon = 1e-7):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    losses = []\n",
    "    grad = [0] * 2\n",
    "\n",
    "    for _ in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        grad[1] = -2 * loss\n",
    "        \n",
    "        weights -= np.sqrt(-1 / (grad[1] + epsilon)) * grad[0] * learning_rate \n",
    "        biases -= np.sqrt(-1 / (grad[1] + epsilon)) * grad[0] * learning_rate\n",
    "\n",
    "        grad[1] = grad[0]\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc30b2ed",
   "metadata": {
    "papermill": {
     "duration": 0.03101,
     "end_time": "2023-04-29T11:08:50.589376",
     "exception": false,
     "start_time": "2023-04-29T11:08:50.558366",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4.1.5 | Clipnorm\n",
    "The `clipnorm` parameter in `RMSprop` is used to prevent the gradient from becoming too large during training, which can cause instability or divergence. When clipnorm is set, the gradient is clipped to a maximum norm value, effectively scaling the gradient if its norm exceeds the specified value. This helps to ensure that the updates to the weights are not too large and that the optimization process remains stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8f9f5e9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:50.652167Z",
     "iopub.status.busy": "2023-04-29T11:08:50.651484Z",
     "iopub.status.idle": "2023-04-29T11:08:50.661512Z",
     "shell.execute_reply": "2023-04-29T11:08:50.660211Z"
    },
    "papermill": {
     "duration": 0.044405,
     "end_time": "2023-04-29T11:08:50.664015",
     "exception": false,
     "start_time": "2023-04-29T11:08:50.619610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adagrad(X , y , learning_rate = 0.01 , epsilon = 1e-7 , clip_norm = None):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    losses = []\n",
    "    grad = [0] * 2\n",
    "\n",
    "    for _ in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        grad[1] = -2 * loss\n",
    "\n",
    "        if clip_norm != None:\n",
    "            \n",
    "            weights = np.clip(weights , weights , clip_norm)\n",
    "            biases = np.clip(biases , biases , clip_norm)\n",
    "        \n",
    "        weights -= np.sqrt(-1 / (grad[1] + epsilon)) * grad[0] * learning_rate \n",
    "        biases -= np.sqrt(-1 / (grad[1] + epsilon)) * grad[0] * learning_rate\n",
    "\n",
    "        grad[1] = grad[0]\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5570c683",
   "metadata": {
    "papermill": {
     "duration": 0.031781,
     "end_time": "2023-04-29T11:08:50.726413",
     "exception": false,
     "start_time": "2023-04-29T11:08:50.694632",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4.1.6 | Clip Value\n",
    "\n",
    "If the gradient is larger than the clipvalue threshold, it will be clipped to that value. If the gradient is smaller than the negative clipvalue threshold, it will be clipped to that negative value. This ensures that the gradient remains within a certain range, which can prevent numerical instability and improve convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a050c309",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:50.790214Z",
     "iopub.status.busy": "2023-04-29T11:08:50.789787Z",
     "iopub.status.idle": "2023-04-29T11:08:50.799833Z",
     "shell.execute_reply": "2023-04-29T11:08:50.798625Z"
    },
    "papermill": {
     "duration": 0.045204,
     "end_time": "2023-04-29T11:08:50.802204",
     "exception": false,
     "start_time": "2023-04-29T11:08:50.757000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adagrad(X , y , learning_rate = 0.01 , epsilon = 1e-7 , clip_norm = None , clip_value = None):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    losses = []\n",
    "    grad = np.zeros(2)\n",
    "\n",
    "    for _ in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        grad[1] = -2 * loss\n",
    "\n",
    "        if clip_norm != None:\n",
    "            \n",
    "            weights = np.clip(weights , weights , clip_norm)\n",
    "            biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "        if clip_value != None:\n",
    "\n",
    "            grad = np.clip(grad , grad , clip_value)\n",
    "        \n",
    "        weights -= np.sqrt(-1 / (grad[1] + epsilon)) * grad[0] * learning_rate \n",
    "        biases -= np.sqrt(-1 / (grad[1] + epsilon)) * grad[0] * learning_rate\n",
    "\n",
    "        grad[1] = grad[0]\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d751553",
   "metadata": {
    "papermill": {
     "duration": 0.031304,
     "end_time": "2023-04-29T11:08:50.863781",
     "exception": false,
     "start_time": "2023-04-29T11:08:50.832477",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4.1.7 EMA\n",
    "\n",
    "Wether to use the estimated momentum average or not. The major use of this function is when `ema_momentum` is given "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c0f47be1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:50.934458Z",
     "iopub.status.busy": "2023-04-29T11:08:50.934089Z",
     "iopub.status.idle": "2023-04-29T11:08:50.943593Z",
     "shell.execute_reply": "2023-04-29T11:08:50.942682Z"
    },
    "papermill": {
     "duration": 0.051091,
     "end_time": "2023-04-29T11:08:50.946240",
     "exception": false,
     "start_time": "2023-04-29T11:08:50.895149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adagrad(X , y , learning_rate = 0.01 , epsilon = 1e-7 , clip_norm = None , clip_value = None , ema = False):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    losses = []\n",
    "    grad = np.zeros(2)\n",
    "\n",
    "    for _ in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        grad[1] = -2 * loss\n",
    "\n",
    "        if ema: pass\n",
    "\n",
    "        if clip_norm != None:\n",
    "            \n",
    "            weights = np.clip(weights , weights , clip_norm)\n",
    "            biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "        if clip_value != None:\n",
    "\n",
    "            grad = np.clip(grad , grad , clip_value)\n",
    "        \n",
    "        weights -= np.sqrt(-1 / (grad[1] + epsilon)) * grad[0] * learning_rate \n",
    "        biases -= np.sqrt(-1 / (grad[1] + epsilon)) * grad[0] * learning_rate\n",
    "\n",
    "        grad[1] = grad[0]\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf42b7df",
   "metadata": {
    "papermill": {
     "duration": 0.02989,
     "end_time": "2023-04-29T11:08:51.006423",
     "exception": false,
     "start_time": "2023-04-29T11:08:50.976533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4.1.8 | EMA Momentum\n",
    "\n",
    "In RMSprop, the exponential moving average (EMA) of the gradient with momentum is used to improve convergence and prevent oscillations during training. The EMA of the gradient with momentum is updated at each iteration and incorporates information about the previous gradients and the current gradient to produce a more stable and consistent update direction. This helps to smooth out the noise in the gradient estimates and helps the optimizer to move more directly towards the minimum of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a95ebd58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:51.069842Z",
     "iopub.status.busy": "2023-04-29T11:08:51.068877Z",
     "iopub.status.idle": "2023-04-29T11:08:51.080356Z",
     "shell.execute_reply": "2023-04-29T11:08:51.079191Z"
    },
    "papermill": {
     "duration": 0.04602,
     "end_time": "2023-04-29T11:08:51.083079",
     "exception": false,
     "start_time": "2023-04-29T11:08:51.037059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adagrad(X , y , learning_rate = 0.01 , epsilon = 1e-7 , clip_norm = None , clip_value = None , ema = False):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    losses = []\n",
    "    grad = np.zeros(2)\n",
    "\n",
    "    for _ in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        grad[1] = -2 * loss\n",
    "\n",
    "        if ema: \n",
    "\n",
    "            grad = ema_momentum * grad + (1 - ema_momentum) * grad\n",
    "\n",
    "        if clip_norm != None:\n",
    "            \n",
    "            weights = np.clip(weights , weights , clip_norm)\n",
    "            biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "        if clip_value != None:\n",
    "\n",
    "            grad = np.clip(grad , grad , clip_value)\n",
    "        \n",
    "        weights -= np.sqrt(-1 / (grad[1] + epsilon)) * grad[0] * learning_rate \n",
    "        biases -= np.sqrt(-1 / (grad[1] + epsilon)) * grad[0] * learning_rate\n",
    "\n",
    "        grad[1] = grad[0]\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7637f1",
   "metadata": {
    "papermill": {
     "duration": 0.029855,
     "end_time": "2023-04-29T11:08:51.142934",
     "exception": false,
     "start_time": "2023-04-29T11:08:51.113079",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4.2 | Methods\n",
    "\n",
    "Now to add more functionalities, we will add different methods to our alogirthm, You can acces the list of methods we are going to use from the same link before we used for `Functionalities`. \n",
    "* `build` - Initialize optimizer variables. SGD optimizer has one variable momentums, only set if self.momentum is not 0.\n",
    "* * `weights` - Weights of the Optimizer\n",
    "* * `biases` - biases of the Optimizer\n",
    "* `compute_gradients` - Compute gradients of loss on trainable variables.\n",
    "* `minimize` - Minimize loss by updating `var_list`.\n",
    "* `update_step` - Update step given gradient and the associated model variable.\n",
    "\n",
    "**Note** - we will stop the function from being usable at some places\n",
    "\n",
    "# 4.2.1 | Build\n",
    "This method initializes all the values in the `rms_prop`, you can understand this as a constructor, but not perfectly as the one. \n",
    "\n",
    "We will give the user a functionality, that they can intialize there own variables. At the starting the `weights` and `baises` are `None`. If the user passes their own, the function first checks that they are or correct shape and if True assigns the values, else initialize random values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "20b7cc50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:51.206334Z",
     "iopub.status.busy": "2023-04-29T11:08:51.205309Z",
     "iopub.status.idle": "2023-04-29T11:08:51.218188Z",
     "shell.execute_reply": "2023-04-29T11:08:51.217001Z"
    },
    "papermill": {
     "duration": 0.04826,
     "end_time": "2023-04-29T11:08:51.221137",
     "exception": false,
     "start_time": "2023-04-29T11:08:51.172877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class adagrad:\n",
    "    def __init__(X , y , \n",
    "                 learning_rate = 0.01 , epsilon = 1e-7 , \n",
    "                 clip_norm = None , clip_value = None , \n",
    "                 ema = False , ema_momentum = 0):\n",
    "    \n",
    "        self.X = X\n",
    "        self.y = y \n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.clip_norm = clip_norm\n",
    "        self.clip_value = clip_value\n",
    "        self.ema = ema\n",
    "        self.ema_momentum = ema_momentum\n",
    "    \n",
    "    def build(self , weights = None , biases = None):\n",
    "    \n",
    "        if weights == None:\n",
    "            \n",
    "            self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if weights.shape[0] == self.X.shape[1]:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match , initalizing random weights\")\n",
    "                    \n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "        \n",
    "        if biases == None:\n",
    "            \n",
    "            self.biases = abs(np.random.randn(1))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if biases.shape[0] == 1:\n",
    "                \n",
    "                self.biases = biases\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match , initalizing random biases\")\n",
    "                    \n",
    "                self.biases = abs(np.random.randn(1))\n",
    "\n",
    "        self.losses = []\n",
    "        self.grad = np.zeros(2)\n",
    "\n",
    "    # for _ in range(100):\n",
    "\n",
    "    #     pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "    #     loss = np.sum((pred - y) ** 2)\n",
    "    #     losses.append(loss)\n",
    "        \n",
    "    #     grad[1] = -2 * loss\n",
    "\n",
    "    #     if ema: \n",
    "\n",
    "    #         grad = ema_momentum * grad + (1 - ema_momentum) * grad\n",
    "\n",
    "    #     if clip_norm != None:\n",
    "            \n",
    "    #         weights = np.clip(weights , weights , clip_norm)\n",
    "    #         biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "    #     if clip_value != None:\n",
    "\n",
    "    #         grad = np.clip(grad , grad , clip_value)\n",
    "        \n",
    "    #     weights -= np.sqrt(-1 / (grad[1] + epsilon)) * grad[0] * learning_rate \n",
    "    #     biases -= np.sqrt(-1 / (grad[1] + epsilon)) * grad[0] * learning_rate\n",
    "\n",
    "    #     grad[1] = grad[0]\n",
    "\n",
    "    # return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725d1733",
   "metadata": {
    "papermill": {
     "duration": 0.032151,
     "end_time": "2023-04-29T11:08:51.283863",
     "exception": false,
     "start_time": "2023-04-29T11:08:51.251712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4.2.2 | Compute Gradients\n",
    "\n",
    "Compuute the loss function and gradients for the losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f1587bf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:51.348211Z",
     "iopub.status.busy": "2023-04-29T11:08:51.347822Z",
     "iopub.status.idle": "2023-04-29T11:08:51.362772Z",
     "shell.execute_reply": "2023-04-29T11:08:51.361843Z"
    },
    "papermill": {
     "duration": 0.050221,
     "end_time": "2023-04-29T11:08:51.365448",
     "exception": false,
     "start_time": "2023-04-29T11:08:51.315227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class adagrad:\n",
    "    def __init__(X , y , \n",
    "                 learning_rate = 0.01 , epsilon = 1e-7 , \n",
    "                 clip_norm = None , clip_value = None , \n",
    "                 ema = False , ema_momentum = 0):\n",
    "    \n",
    "        self.X = X\n",
    "        self.y = y \n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.clip_norm = clip_norm\n",
    "        self.clip_value = clip_value\n",
    "        self.ema = ema\n",
    "        self.ema_momentum = ema_momentum\n",
    "    \n",
    "    def build(self , weights = None , biases = None):\n",
    "    \n",
    "        if weights == None:\n",
    "            \n",
    "            self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if weights.shape[0] == self.X.shape[1]:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match , initalizing random weights\")\n",
    "                    \n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "        \n",
    "        if biases == None:\n",
    "            \n",
    "            self.biases = abs(np.random.randn(1))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if biases.shape[0] == 1:\n",
    "                \n",
    "                self.biases = biases\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match , initalizing random biases\")\n",
    "                    \n",
    "                self.biases = abs(np.random.randn(1))\n",
    "\n",
    "        self.losses = []\n",
    "        self.grad = np.zeros(2)\n",
    "\n",
    "    def compute_gradient(self , weights , biases):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        \n",
    "        losses.append(loss)\n",
    "        \n",
    "        grad[1] = -2 * loss\n",
    "\n",
    "        yield grad\n",
    "\n",
    "#     if ema: \n",
    "\n",
    "#         grad = ema_momentum * grad + (1 - ema_momentum) * grad\n",
    "\n",
    "#     if clip_norm != None:\n",
    "        \n",
    "#         weights = np.clip(weights , weights , clip_norm)\n",
    "#         biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "#     if clip_value != None:\n",
    "\n",
    "#         grad = np.clip(grad , grad , clip_value)\n",
    "    \n",
    "#     weights -= np.sqrt(-1 / (grad[1] + epsilon)) * grad[0] * learning_rate \n",
    "#     biases -= np.sqrt(-1 / (grad[1] + epsilon)) * grad[0] * learning_rate\n",
    "\n",
    "#     grad[1] = grad[0]\n",
    "\n",
    "# return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4dbc59",
   "metadata": {
    "papermill": {
     "duration": 0.034454,
     "end_time": "2023-04-29T11:08:51.434899",
     "exception": false,
     "start_time": "2023-04-29T11:08:51.400445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4.2.3 | Minimize\n",
    "Minimze the loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a3da59d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:51.500796Z",
     "iopub.status.busy": "2023-04-29T11:08:51.499775Z",
     "iopub.status.idle": "2023-04-29T11:08:51.516051Z",
     "shell.execute_reply": "2023-04-29T11:08:51.514590Z"
    },
    "papermill": {
     "duration": 0.053553,
     "end_time": "2023-04-29T11:08:51.518768",
     "exception": false,
     "start_time": "2023-04-29T11:08:51.465215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class adagrad:\n",
    "    def __init__(X , y , \n",
    "                 learning_rate = 0.01 , epsilon = 1e-7 , \n",
    "                 clip_norm = None , clip_value = None , \n",
    "                 ema = False , ema_momentum = 0):\n",
    "    \n",
    "        self.X = X\n",
    "        self.y = y \n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.clip_norm = clip_norm\n",
    "        self.clip_value = clip_value\n",
    "        self.ema = ema\n",
    "        self.ema_momentum = ema_momentum\n",
    "    \n",
    "    def build(self , weights = None , biases = None):\n",
    "    \n",
    "        if weights == None:\n",
    "            \n",
    "            self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if weights.shape[0] == self.X.shape[1]:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match , initalizing random weights\")\n",
    "                    \n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "        \n",
    "        if biases == None:\n",
    "            \n",
    "            self.biases = abs(np.random.randn(1))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if biases.shape[0] == 1:\n",
    "                \n",
    "                self.biases = biases\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match , initalizing random biases\")\n",
    "                    \n",
    "                self.biases = abs(np.random.randn(1))\n",
    "\n",
    "        self.losses = []\n",
    "        self.grad = np.zeros(2)\n",
    "\n",
    "    def compute_gradient(self , weights , biases):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        \n",
    "        losses.append(loss)\n",
    "        \n",
    "        grad[1] = -2 * loss\n",
    "\n",
    "        yield grad\n",
    "\n",
    "#     if ema: \n",
    "\n",
    "#         grad = ema_momentum * grad + (1 - ema_momentum) * grad\n",
    "    def minimize(self):\n",
    "        \n",
    "        if clip_norm != None:\n",
    "            \n",
    "            weights = np.clip(weights , weights , clip_norm)\n",
    "            biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "        if clip_value != None:\n",
    "\n",
    "            grad = np.clip(grad , grad , clip_value)\n",
    "        \n",
    "        weights -= np.sqrt(-1 / (grad[1] + epsilon)) * grad[0] * learning_rate \n",
    "        biases -= np.sqrt(-1 / (grad[1] + epsilon)) * grad[0] * learning_rate\n",
    "\n",
    "        grad[1] = grad[0]\n",
    "\n",
    "        return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045e0b89",
   "metadata": {
    "papermill": {
     "duration": 0.030925,
     "end_time": "2023-04-29T11:08:51.580805",
     "exception": false,
     "start_time": "2023-04-29T11:08:51.549880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.2.4 | Update_Step\n",
    "Update the values accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fe35fa9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:51.645803Z",
     "iopub.status.busy": "2023-04-29T11:08:51.645163Z",
     "iopub.status.idle": "2023-04-29T11:08:51.660324Z",
     "shell.execute_reply": "2023-04-29T11:08:51.659160Z"
    },
    "papermill": {
     "duration": 0.050831,
     "end_time": "2023-04-29T11:08:51.662958",
     "exception": false,
     "start_time": "2023-04-29T11:08:51.612127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class adagrad:\n",
    "    def __init__(X , y , \n",
    "                 learning_rate = 0.01 , epsilon = 1e-7 , \n",
    "                 clip_norm = None , clip_value = None , \n",
    "                 ema = False , ema_momentum = 0):\n",
    "    \n",
    "        self.X = X\n",
    "        self.y = y \n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.clip_norm = clip_norm\n",
    "        self.clip_value = clip_value\n",
    "        self.ema = ema\n",
    "        self.ema_momentum = ema_momentum\n",
    "    \n",
    "    def build(self , weights = None , biases = None):\n",
    "    \n",
    "        if weights == None:\n",
    "            \n",
    "            self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if weights.shape[0] == self.X.shape[1]:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match , initalizing random weights\")\n",
    "                    \n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "        \n",
    "        if biases == None:\n",
    "            \n",
    "            self.biases = abs(np.random.randn(1))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if biases.shape[0] == 1:\n",
    "                \n",
    "                self.biases = biases\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match , initalizing random biases\")\n",
    "                    \n",
    "                self.biases = abs(np.random.randn(1))\n",
    "\n",
    "        self.losses = []\n",
    "        self.grad = np.zeros(2)\n",
    "\n",
    "    def compute_gradient(self , weights , biases):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        \n",
    "        losses.append(loss)\n",
    "        \n",
    "        grad[1] = -2 * loss\n",
    "\n",
    "        yield grad\n",
    "    def update_step(self):\n",
    "\n",
    "        if ema: \n",
    "\n",
    "            grad = ema_momentum * grad + (1 - ema_momentum) * grad\n",
    "    \n",
    "    def minimize(self):\n",
    "        \n",
    "        if clip_norm != None:\n",
    "            \n",
    "            weights = np.clip(weights , weights , clip_norm)\n",
    "            biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "        if clip_value != None:\n",
    "\n",
    "            grad = np.clip(grad , grad , clip_value)\n",
    "        \n",
    "        weights -= np.sqrt(-1 / (grad[1] + epsilon)) * grad[0] * learning_rate \n",
    "        biases -= np.sqrt(-1 / (grad[1] + epsilon)) * grad[0] * learning_rate\n",
    "\n",
    "        grad[1] = grad[0]\n",
    "\n",
    "        return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515568b9",
   "metadata": {
    "papermill": {
     "duration": 0.030733,
     "end_time": "2023-04-29T11:08:51.723798",
     "exception": false,
     "start_time": "2023-04-29T11:08:51.693065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4.2 | AdaGrad Final Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "82f8b8e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T11:08:51.786655Z",
     "iopub.status.busy": "2023-04-29T11:08:51.786271Z",
     "iopub.status.idle": "2023-04-29T11:08:51.803911Z",
     "shell.execute_reply": "2023-04-29T11:08:51.802781Z"
    },
    "papermill": {
     "duration": 0.05169,
     "end_time": "2023-04-29T11:08:51.806259",
     "exception": false,
     "start_time": "2023-04-29T11:08:51.754569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class adagrad:\n",
    "    def __init__(X , y , \n",
    "                 learning_rate = 0.01 , epsilon = 1e-7 , \n",
    "                 clip_norm = None , clip_value = None , \n",
    "                 ema = False , ema_momentum = 0):\n",
    "    \n",
    "        self.X = X\n",
    "        self.y = y \n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.clip_norm = clip_norm\n",
    "        self.clip_value = clip_value\n",
    "        self.ema = ema\n",
    "        self.ema_momentum = ema_momentum\n",
    "    \n",
    "    def build(self , weights = None , biases = None):\n",
    "    \n",
    "        if weights == None:\n",
    "            \n",
    "            self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if weights.shape[0] == self.X.shape[1]:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match , initalizing random weights\")\n",
    "                    \n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "        \n",
    "        if biases == None:\n",
    "            \n",
    "            self.biases = abs(np.random.randn(1))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if biases.shape[0] == 1:\n",
    "                \n",
    "                self.biases = biases\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match , initalizing random biases\")\n",
    "                    \n",
    "                self.biases = abs(np.random.randn(1))\n",
    "\n",
    "        self.losses = []\n",
    "        self.grad = np.zeros(2)\n",
    "\n",
    "    def compute_gradient(self , weights , biases):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        \n",
    "        losses.append(loss)\n",
    "        \n",
    "        grad[1] = -2 * loss\n",
    "\n",
    "        yield grad\n",
    "    def update_step(self):\n",
    "\n",
    "        if ema: \n",
    "\n",
    "            grad = ema_momentum * grad + (1 - ema_momentum) * grad\n",
    "    \n",
    "    def minimize(self):\n",
    "        \n",
    "        if clip_norm != None:\n",
    "            \n",
    "            weights = np.clip(weights , weights , clip_norm)\n",
    "            biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "        if clip_value != None:\n",
    "\n",
    "            grad = np.clip(grad , grad , clip_value)\n",
    "        \n",
    "        weights -= np.sqrt(-1 / (grad[1] + epsilon)) * grad[0] * learning_rate \n",
    "        biases -= np.sqrt(-1 / (grad[1] + epsilon)) * grad[0] * learning_rate\n",
    "\n",
    "        grad[1] = grad[0]\n",
    "\n",
    "        return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4db53a2",
   "metadata": {
    "papermill": {
     "duration": 0.030994,
     "end_time": "2023-04-29T11:08:51.867979",
     "exception": false,
     "start_time": "2023-04-29T11:08:51.836985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**THIS IS NOT THE FULL IMPLEMENTATION, IT STILL LACKS MANY FUNCTIONALITIES AND IS VULENRABLE TO MANY EDGE CASES, WE WILL IMPROVE THIS IN THE UPCOMING VERSIONS**\n",
    "\n",
    "**PLEASE COMMENT DOWN IF I DID ANY MISTAKES, OR IF CAN MAKE THIS MORE CONNECTED TO THE GROUND, OR SUGGESTIONS. YOUR ASSISTS ARE HIGHLY APPRECIABLE**\n",
    "\n",
    "**THATS IT FOR TODAY GUYS**\n",
    "\n",
    "**HOPE YOU UNDERSTOOD AND LIKED MY WORK**\n",
    "\n",
    "**DONT FORGET TO MAKE AN UPVOTE :)**\n",
    "\n",
    "**PEACE OUT !!!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 23.003018,
   "end_time": "2023-04-29T11:08:52.723546",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-29T11:08:29.720528",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
