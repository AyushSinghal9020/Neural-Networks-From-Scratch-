{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d71017",
   "metadata": {
    "papermill": {
     "duration": 0.023197,
     "end_time": "2023-04-29T08:16:03.281409",
     "exception": false,
     "start_time": "2023-04-29T08:16:03.258212",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Optimizers \n",
    "\n",
    "So what are `Optimizers`\n",
    "\n",
    "**What** - Optimizer is a technique to set the `weights` and `biases` of a particular model such that they produce minimum `Loss` Possible.\n",
    "\n",
    "The bestest way (in my opinion) to get started is to fist understand what the hell is this `Regression`\n",
    "\n",
    "**What** - `Regression` is just like the lost brother of `classification`. In `classification` we have `discrete` or `particular values`, that we want to `classify`, In `regression` we have `continuous values`, that we want to `predict`\n",
    "\n",
    "| Classification |Regression |\n",
    "| --- | --- |\n",
    "|  We have discrete values| We have continuous values |\n",
    "|Usually we know these values in depth | We usually don't know these values in depth|\n",
    "|These are comparatively less in number| These are comparatively more in number |\n",
    "\n",
    "# 1 | What other things we will learn here\n",
    "* Slope of Function\n",
    "* Baisc Diffrentiation\n",
    "* Intercept "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e246eb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:03.327582Z",
     "iopub.status.busy": "2023-04-29T08:16:03.326878Z",
     "iopub.status.idle": "2023-04-29T08:16:03.335835Z",
     "shell.execute_reply": "2023-04-29T08:16:03.334946Z"
    },
    "papermill": {
     "duration": 0.035582,
     "end_time": "2023-04-29T08:16:03.338269",
     "exception": false,
     "start_time": "2023-04-29T08:16:03.302687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17538594",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:03.382521Z",
     "iopub.status.busy": "2023-04-29T08:16:03.382072Z",
     "iopub.status.idle": "2023-04-29T08:16:04.456587Z",
     "shell.execute_reply": "2023-04-29T08:16:04.455442Z"
    },
    "papermill": {
     "duration": 1.099821,
     "end_time": "2023-04-29T08:16:04.459443",
     "exception": false,
     "start_time": "2023-04-29T08:16:03.359622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17483a32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:04.504134Z",
     "iopub.status.busy": "2023-04-29T08:16:04.503725Z",
     "iopub.status.idle": "2023-04-29T08:16:04.508609Z",
     "shell.execute_reply": "2023-04-29T08:16:04.507395Z"
    },
    "papermill": {
     "duration": 0.030422,
     "end_time": "2023-04-29T08:16:04.510835",
     "exception": false,
     "start_time": "2023-04-29T08:16:04.480413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b989f1",
   "metadata": {
    "papermill": {
     "duration": 0.025431,
     "end_time": "2023-04-29T08:16:04.557357",
     "exception": false,
     "start_time": "2023-04-29T08:16:04.531926",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Lets assume we have data like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcd23dd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:04.609834Z",
     "iopub.status.busy": "2023-04-29T08:16:04.608998Z",
     "iopub.status.idle": "2023-04-29T08:16:04.621521Z",
     "shell.execute_reply": "2023-04-29T08:16:04.620504Z"
    },
    "papermill": {
     "duration": 0.045296,
     "end_time": "2023-04-29T08:16:04.624816",
     "exception": false,
     "start_time": "2023-04-29T08:16:04.579520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.array([x for x in range(0 , 200 , 1)])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dda13d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:04.672917Z",
     "iopub.status.busy": "2023-04-29T08:16:04.672178Z",
     "iopub.status.idle": "2023-04-29T08:16:04.679085Z",
     "shell.execute_reply": "2023-04-29T08:16:04.678298Z"
    },
    "papermill": {
     "duration": 0.031315,
     "end_time": "2023-04-29T08:16:04.681126",
     "exception": false,
     "start_time": "2023-04-29T08:16:04.649811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   2,   4,   6,   8,  10,  12,  14,  16,  18,  20,  22,  24,\n",
       "        26,  28,  30,  32,  34,  36,  38,  40,  42,  44,  46,  48,  50,\n",
       "        52,  54,  56,  58,  60,  62,  64,  66,  68,  70,  72,  74,  76,\n",
       "        78,  80,  82,  84,  86,  88,  90,  92,  94,  96,  98, 100, 102,\n",
       "       104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128,\n",
       "       130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154,\n",
       "       156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180,\n",
       "       182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206,\n",
       "       208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232,\n",
       "       234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258,\n",
       "       260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284,\n",
       "       286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310,\n",
       "       312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336,\n",
       "       338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362,\n",
       "       364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388,\n",
       "       390, 392, 394, 396, 398])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = np.array([x for x in range(0 , 400 , 2)])\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b8dc67",
   "metadata": {
    "papermill": {
     "duration": 0.020983,
     "end_time": "2023-04-29T08:16:04.723163",
     "exception": false,
     "start_time": "2023-04-29T08:16:04.702180",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Lets assume there is a connection between the `target` , and `features`. By human instacne we know that every element in `target` is just a double of the corresponding element in `features`, or $target  = 2XFeatures$. \n",
    "\n",
    "Lets assume we change the target a little bit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0469f2fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:04.767895Z",
     "iopub.status.busy": "2023-04-29T08:16:04.767180Z",
     "iopub.status.idle": "2023-04-29T08:16:04.774499Z",
     "shell.execute_reply": "2023-04-29T08:16:04.773710Z"
    },
    "papermill": {
     "duration": 0.03228,
     "end_time": "2023-04-29T08:16:04.776721",
     "exception": false,
     "start_time": "2023-04-29T08:16:04.744441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   3,   5,   7,   9,  11,  13,  15,  17,  19,  21,  23,  25,\n",
       "        27,  29,  31,  33,  35,  37,  39,  41,  43,  45,  47,  49,  51,\n",
       "        53,  55,  57,  59,  61,  63,  65,  67,  69,  71,  73,  75,  77,\n",
       "        79,  81,  83,  85,  87,  89,  91,  93,  95,  97,  99, 101, 103,\n",
       "       105, 107, 109, 111, 113, 115, 117, 119, 121, 123, 125, 127, 129,\n",
       "       131, 133, 135, 137, 139, 141, 143, 145, 147, 149, 151, 153, 155,\n",
       "       157, 159, 161, 163, 165, 167, 169, 171, 173, 175, 177, 179, 181,\n",
       "       183, 185, 187, 189, 191, 193, 195, 197, 199, 201, 203, 205, 207,\n",
       "       209, 211, 213, 215, 217, 219, 221, 223, 225, 227, 229, 231, 233,\n",
       "       235, 237, 239, 241, 243, 245, 247, 249, 251, 253, 255, 257, 259,\n",
       "       261, 263, 265, 267, 269, 271, 273, 275, 277, 279, 281, 283, 285,\n",
       "       287, 289, 291, 293, 295, 297, 299, 301, 303, 305, 307, 309, 311,\n",
       "       313, 315, 317, 319, 321, 323, 325, 327, 329, 331, 333, 335, 337,\n",
       "       339, 341, 343, 345, 347, 349, 351, 353, 355, 357, 359, 361, 363,\n",
       "       365, 367, 369, 371, 373, 375, 377, 379, 381, 383, 385, 387, 389,\n",
       "       391, 393, 395, 397, 399])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = np.array([x + 1 for x in range(0 , 400 , 2)])\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611ab013",
   "metadata": {
    "papermill": {
     "duration": 0.020891,
     "end_time": "2023-04-29T08:16:04.819203",
     "exception": false,
     "start_time": "2023-04-29T08:16:04.798312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now what could be the trend here..., We can see the code above and with the help of that we can say. That `target` value is just the `double + 1` of the corresponding element in `features`. or $target = 2Xfeatures + 1$\n",
    "\n",
    "Till now the problem was really easy to solve, and thats why we used the brain only, But these are just examples. As we move closer to the real world. The examples/problems get difficulat and we find it harder to find proper trends in the two `arrays`. Thats we try to teach machine, how to find trend in the data. The formula we had before $target = 2Xfeature + 1$ is subjective to only one problem or a similar problem. But this formula can be generlized by the equation of `straight line`, which is $y = mx + b$\n",
    "\n",
    "So what does this line means ???\n",
    "\n",
    "Lets first try to plot the data we had on a scatter plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f348377e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:04.864751Z",
     "iopub.status.busy": "2023-04-29T08:16:04.864045Z",
     "iopub.status.idle": "2023-04-29T08:16:05.153090Z",
     "shell.execute_reply": "2023-04-29T08:16:05.152183Z"
    },
    "papermill": {
     "duration": 0.315213,
     "end_time": "2023-04-29T08:16:05.155551",
     "exception": false,
     "start_time": "2023-04-29T08:16:04.840338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x751598c22ed0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(features , target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a5d7fd",
   "metadata": {
    "papermill": {
     "duration": 0.021517,
     "end_time": "2023-04-29T08:16:05.200469",
     "exception": false,
     "start_time": "2023-04-29T08:16:05.178952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You can see we got a sequence of dots that resembles kind of straight line. \n",
    "\n",
    "Lets assume we have a line that tries to capture most of the points on this, like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f50a7f09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:05.246442Z",
     "iopub.status.busy": "2023-04-29T08:16:05.245701Z",
     "iopub.status.idle": "2023-04-29T08:16:05.497046Z",
     "shell.execute_reply": "2023-04-29T08:16:05.496200Z"
    },
    "papermill": {
     "duration": 0.276973,
     "end_time": "2023-04-29T08:16:05.499250",
     "exception": false,
     "start_time": "2023-04-29T08:16:05.222277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7515989f40d0>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(features , target)\n",
    "plt.plot([0 , 200] , [0 , 400] , \"yellow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9485fc8c",
   "metadata": {
    "papermill": {
     "duration": 0.022213,
     "end_time": "2023-04-29T08:16:05.544674",
     "exception": false,
     "start_time": "2023-04-29T08:16:05.522461",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Again by human intution we found the `best fit line`. But what if we want to generalize the things and kind of do not find the best fit line...?\n",
    "\n",
    "First of all lets get a little bit more deep into equation $y = mx + b$\n",
    "\n",
    "So what does these terms resembles in this eqution. \n",
    "* `m` is the slope of the line\n",
    "\n",
    "# 1.1 | Slope of A function\n",
    "\n",
    "Slope of a function shows how steep a function is, or the direction of a function at a given point on the curve.\n",
    "\n",
    "Lets assume we have this curve $y = 4x^2$ the slope of this curve will be $y = 8x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84f7b4b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:05.592762Z",
     "iopub.status.busy": "2023-04-29T08:16:05.591988Z",
     "iopub.status.idle": "2023-04-29T08:16:05.598158Z",
     "shell.execute_reply": "2023-04-29T08:16:05.597168Z"
    },
    "papermill": {
     "duration": 0.032815,
     "end_time": "2023-04-29T08:16:05.600299",
     "exception": false,
     "start_time": "2023-04-29T08:16:05.567484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"400\"\n",
       "            src=\"https://www.desmos.com/calculator/zluqu5vyuh\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x751598bad450>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.desmos.com/calculator/zluqu5vyuh\" , 400 , 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f73b2cc",
   "metadata": {
    "papermill": {
     "duration": 0.022425,
     "end_time": "2023-04-29T08:16:05.645395",
     "exception": false,
     "start_time": "2023-04-29T08:16:05.622970",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So how do we calculate the `slope` of a line???\n",
    "\n",
    "Lets assume we have a function $y = f(x)$,. To find the slope of a function, we simply diffrenctiate the function, thus, the slope of this line will be $y^` = f^`(x)$\n",
    "\n",
    "# 1.2 | Diffrentiation\n",
    "\n",
    "Diffrentaition can be explained as getting a small value of a function.\n",
    "\n",
    "lets assume we have a function `y = sin(x)`\n",
    "\n",
    "A small strip at that function will demonstrate taking a derivative of that function `sin(x)`\n",
    "\n",
    "Taking about the function we had taken before that is $y = 4x^2$\n",
    "\n",
    "Taking its derivative we will get $$y = 8x$$ ($x{n^`} = nx^{n-1}$)\n",
    "\n",
    "So the slope of $y = 4x^2$ can be represnted as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf80683e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:05.693213Z",
     "iopub.status.busy": "2023-04-29T08:16:05.692332Z",
     "iopub.status.idle": "2023-04-29T08:16:05.698890Z",
     "shell.execute_reply": "2023-04-29T08:16:05.697914Z"
    },
    "papermill": {
     "duration": 0.033421,
     "end_time": "2023-04-29T08:16:05.701353",
     "exception": false,
     "start_time": "2023-04-29T08:16:05.667932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"400\"\n",
       "            src=\"https://www.desmos.com/calculator/hrguwktg9q\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7515989b9810>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.desmos.com/calculator/hrguwktg9q\" , 400 , 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4707c26",
   "metadata": {
    "papermill": {
     "duration": 0.02254,
     "end_time": "2023-04-29T08:16:05.746859",
     "exception": false,
     "start_time": "2023-04-29T08:16:05.724319",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**If you want to know more aboud diffrentiation, here is [3Blue1Brown](https://www.youtube.com/@3blue1brown/featured) => [Essence Of Calculas](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)**\n",
    "\n",
    "So now you have a basic idea of `slope`\n",
    "\n",
    "# 1.3 | Intercept\n",
    "\n",
    "Now what `b` represents in the data. Usually it is called the `intercept`. Consider this graph of the equation $y = x$ or $y = 1x + b$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29e020f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:05.795351Z",
     "iopub.status.busy": "2023-04-29T08:16:05.794271Z",
     "iopub.status.idle": "2023-04-29T08:16:05.802113Z",
     "shell.execute_reply": "2023-04-29T08:16:05.800811Z"
    },
    "papermill": {
     "duration": 0.034558,
     "end_time": "2023-04-29T08:16:05.804617",
     "exception": false,
     "start_time": "2023-04-29T08:16:05.770059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"400\"\n",
       "            src=\"https://www.desmos.com/calculator/gai0veg5fh\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x751598c3b510>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.desmos.com/calculator/gai0veg5fh\" , 400 , 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a839078e",
   "metadata": {
    "papermill": {
     "duration": 0.02276,
     "end_time": "2023-04-29T08:16:05.850252",
     "exception": false,
     "start_time": "2023-04-29T08:16:05.827492",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This line passes the axis at $(0 , 0)$. These coordinates are called as the `intercepts` of this line. If we make `b` or `intercept` as $1$. The line will then pass from $(1 , -1)$. Basically the `intercept` moves a line in a plane. With that being said, Lets also undertand how the `slope` changes the line. If we make `m` as $2$. The line will rotate anti-clockwise. So as we increase the value of `m` or `slope`. The line moves anti-clockwise, And so the vice-versa, If we decrease the value of `m`, The slope will move in the clockwise direction. \n",
    "\n",
    "In short tweeking the values of `m` and `b` or `slope` and `intercept`. We can move the line in any direction and in any way we want `as long as it resembles a straight line`. We still cannot bend the line \n",
    "\n",
    "So now we have any data, we just need to difine the values of `slope` and `intercpet`. And we can get the best fit line. But still the question arises how do we generalize the values of these tuning parametes. \n",
    "\n",
    "In simple word we can say, How can we find a relation between the data we have and these tuning parameters. So that we only need to define that relationship and then we can easily predict the values.\n",
    "\n",
    "Lets think that the value assigned to the line is this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb1c39eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:05.897958Z",
     "iopub.status.busy": "2023-04-29T08:16:05.897533Z",
     "iopub.status.idle": "2023-04-29T08:16:06.143302Z",
     "shell.execute_reply": "2023-04-29T08:16:06.142049Z"
    },
    "papermill": {
     "duration": 0.27322,
     "end_time": "2023-04-29T08:16:06.146200",
     "exception": false,
     "start_time": "2023-04-29T08:16:05.872980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x751598987f90>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(features , target)\n",
    "plt.plot([0 , 300] , [0 , 400] , \"yellow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb863775",
   "metadata": {
    "papermill": {
     "duration": 0.023707,
     "end_time": "2023-04-29T08:16:06.193778",
     "exception": false,
     "start_time": "2023-04-29T08:16:06.170071",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If we test this line on the training data only, we will find that this line is not correct. It is predicting points incorrect, We know that the best fit line we drew first, will predict points wiht lowest incorrect ones. For example the line we just defined if asked the corresponding value of $200$, it will say $250$. But rather it was $400$. There was some `error`, some `loss`, or some `cost` with the `actual` and `predicted` values.\n",
    "\n",
    "For measuring this loss, what we can do is find the difference between the `actual value` and the `predicted value`. A best fit line will give the lowest value of this difference.\n",
    "\n",
    "The word difference here is very difficult to say, so we can give this term a new fancy name, which is `The Loss`.\n",
    "\n",
    "One can deifne loss as $$Loss = actual - predicted$$.\n",
    "\n",
    "We only took the example of one value. but there are a large group of values. that can show the same trait, For that we can change the formula to \n",
    "\n",
    "Lets denote $actual$ as $a$ and $predicted$ as $p$\n",
    "\n",
    "$$Loss = (a_1 - p_1) + (a_2 - p_2) + (a_3 - p_3) + ... + (a_n - p_n)$$\n",
    "\n",
    "or $$Loss = \\sum\\limits_{i = 1}^{n}a_i - p_i$$ or $$Loss = \\sum\\limits_{i = 1}^{n}(y_i - \\hat y_i)$$\n",
    "\n",
    "Whenever you see $\\hat y$, think of it as the `predicted value`\n",
    "\n",
    "Now lets assume we have data like this and a random line is drawn like this \n",
    "\n",
    "<img src = \"https://cdn-media-1.freecodecamp.org/images/MNskFmGPKuQfMLdmpkT-X7-8w2cJXulP3683\">\n",
    "\n",
    "If you look closely, a lot of error terms will tend to cancel out each other. We can also get into a state where the line is `not the best fit`, but still gives $0$ error. With the `Loss` we defined before, we are not chossing a `best fit line`. Rather we are chossing a line that is in the `middle` of those points. One way to counter this is to add a `modulus` function like this $$Loss = \\sum\\limits_{i = 1}^{n}|y - \\hat y|$$\n",
    "\n",
    "But what is a modulus function. The function is nothing but converts, any negative numbers to postive. For example \n",
    "$|-1| = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ca99933",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:06.243374Z",
     "iopub.status.busy": "2023-04-29T08:16:06.242960Z",
     "iopub.status.idle": "2023-04-29T08:16:06.249422Z",
     "shell.execute_reply": "2023-04-29T08:16:06.248522Z"
    },
    "papermill": {
     "duration": 0.034209,
     "end_time": "2023-04-29T08:16:06.251624",
     "exception": false,
     "start_time": "2023-04-29T08:16:06.217415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"400\"\n",
       "            src=\"https://www.desmos.com/calculator/kamxotjra2\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x75159894e350>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.desmos.com/calculator/kamxotjra2\" , 400 , 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5bd95b",
   "metadata": {
    "papermill": {
     "duration": 0.023811,
     "end_time": "2023-04-29T08:16:06.299303",
     "exception": false,
     "start_time": "2023-04-29T08:16:06.275492",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "But there is a problem with this function. A `modulus` is not diffrentiable. You might be thinking that why are we even seeing that part, like we we care for that. Why would you even diffrentiate a loss function. \n",
    "\n",
    "We actually diffrentiate loss function in further steps, thats why we will not use the modulus function. \n",
    "\n",
    "Another way of doing so is to, square the loss function like this $Loss = (y - \\hat y)^2$\n",
    "\n",
    "Its cool, its good and we can even diffrentiate this...\n",
    "\n",
    "Now we have a basic idea that we need to compute `m` and `b` for the lowest loss values. Now we should come to know how we can do this \n",
    "\n",
    "What if we somehow interelate the `losses` and `m and b`. \n",
    "\n",
    "Lets assume we intialize the parameters randomly, like this \n",
    "\n",
    "# 2 | SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "934745da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:06.348016Z",
     "iopub.status.busy": "2023-04-29T08:16:06.347546Z",
     "iopub.status.idle": "2023-04-29T08:16:06.355184Z",
     "shell.execute_reply": "2023-04-29T08:16:06.354033Z"
    },
    "papermill": {
     "duration": 0.034741,
     "end_time": "2023-04-29T08:16:06.357490",
     "exception": false,
     "start_time": "2023-04-29T08:16:06.322749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46310656])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.random.randn(1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd69a65e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:06.406335Z",
     "iopub.status.busy": "2023-04-29T08:16:06.405873Z",
     "iopub.status.idle": "2023-04-29T08:16:06.413555Z",
     "shell.execute_reply": "2023-04-29T08:16:06.412391Z"
    },
    "papermill": {
     "duration": 0.034893,
     "end_time": "2023-04-29T08:16:06.415860",
     "exception": false,
     "start_time": "2023-04-29T08:16:06.380967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92022388])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases = np.random.randn(1)\n",
    "biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec6d4c3",
   "metadata": {
    "papermill": {
     "duration": 0.023521,
     "end_time": "2023-04-29T08:16:06.462975",
     "exception": false,
     "start_time": "2023-04-29T08:16:06.439454",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So the predicitions will be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ecea6ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:06.511387Z",
     "iopub.status.busy": "2023-04-29T08:16:06.511019Z",
     "iopub.status.idle": "2023-04-29T08:16:06.518149Z",
     "shell.execute_reply": "2023-04-29T08:16:06.516996Z"
    },
    "papermill": {
     "duration": 0.034633,
     "end_time": "2023-04-29T08:16:06.520734",
     "exception": false,
     "start_time": "2023-04-29T08:16:06.486101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.81342074])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = weights * 30 + biases\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cef401",
   "metadata": {
    "papermill": {
     "duration": 0.023732,
     "end_time": "2023-04-29T08:16:06.568675",
     "exception": false,
     "start_time": "2023-04-29T08:16:06.544943",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "And its way far than what we had expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbc2f31e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:06.618397Z",
     "iopub.status.busy": "2023-04-29T08:16:06.617617Z",
     "iopub.status.idle": "2023-04-29T08:16:06.625199Z",
     "shell.execute_reply": "2023-04-29T08:16:06.624046Z"
    },
    "papermill": {
     "duration": 0.034985,
     "end_time": "2023-04-29T08:16:06.627439",
     "exception": false,
     "start_time": "2023-04-29T08:16:06.592454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-45.18657926])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = (pred - 60)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e891fd50",
   "metadata": {
    "papermill": {
     "duration": 0.02357,
     "end_time": "2023-04-29T08:16:06.674596",
     "exception": false,
     "start_time": "2023-04-29T08:16:06.651026",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Our main motive is to reduce this loss as much as possible,. \n",
    "\n",
    "What if we subtract a small subset of the derivative of this loss from the parameters like this. The derivative of the loss will show us the steepness of the curve, and thus doing so might get us to the valeus of minimum loss. So how do we find the derivative of this function $Loss = (y - \\hat y)^2$. What we know is $\\hat y = mx + b$. COmputing this value in we get $$Loss = (y - mx - b)^2$$, Now we can diffrentiate the function\n",
    "\n",
    "## Diffrentiating wrt `b`\n",
    "$$\\frac {dLoss}{db}= \\frac {d}{db}(y - mx - b)^2$$\n",
    "$$= 2(y - mx - b)(-1)$$\n",
    "\n",
    "## Diffrentiating wrt `m`\n",
    "$$\\frac {dLoss}{dm} = \\frac {d}{dm}(y - mx - b)^2$$\n",
    "$$= 2(y - mx - b)(-x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92fe9ba7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:06.724703Z",
     "iopub.status.busy": "2023-04-29T08:16:06.724256Z",
     "iopub.status.idle": "2023-04-29T08:16:06.729705Z",
     "shell.execute_reply": "2023-04-29T08:16:06.728470Z"
    },
    "papermill": {
     "duration": 0.033297,
     "end_time": "2023-04-29T08:16:06.732115",
     "exception": false,
     "start_time": "2023-04-29T08:16:06.698818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights -= (-2* (60 - weights*30 - biases)) * 0.001\n",
    "biases -= (2 * 30 * (60 - weights * 30 - biases)) * 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11578041",
   "metadata": {
    "papermill": {
     "duration": 0.023331,
     "end_time": "2023-04-29T08:16:06.779326",
     "exception": false,
     "start_time": "2023-04-29T08:16:06.755995",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "And if we then try to predict the values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e10b2892",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:06.830749Z",
     "iopub.status.busy": "2023-04-29T08:16:06.830348Z",
     "iopub.status.idle": "2023-04-29T08:16:06.839934Z",
     "shell.execute_reply": "2023-04-29T08:16:06.838519Z"
    },
    "papermill": {
     "duration": 0.037642,
     "end_time": "2023-04-29T08:16:06.842507",
     "exception": false,
     "start_time": "2023-04-29T08:16:06.804865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55347972]\n",
      "[-24.56500682]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([67.9606152])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(weights)\n",
    "print(biases)\n",
    "loss = (60 - (weights * 30 + biases))\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9c1308",
   "metadata": {
    "papermill": {
     "duration": 0.0239,
     "end_time": "2023-04-29T08:16:06.890180",
     "exception": false,
     "start_time": "2023-04-29T08:16:06.866280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Our losses have been decreased, so lets do it again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32bbf5e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:06.940148Z",
     "iopub.status.busy": "2023-04-29T08:16:06.939410Z",
     "iopub.status.idle": "2023-04-29T08:16:06.949576Z",
     "shell.execute_reply": "2023-04-29T08:16:06.948201Z"
    },
    "papermill": {
     "duration": 0.03806,
     "end_time": "2023-04-29T08:16:06.952122",
     "exception": false,
     "start_time": "2023-04-29T08:16:06.914062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91269202]\n",
      "[-23.20579451]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([25.82503378])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights -= -2 * loss * 0.01\n",
    "biases -= -2 * loss * 0.01\n",
    "print(weights)\n",
    "print(biases)\n",
    "loss = (60 - (weights * 30 + biases))\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7e159a",
   "metadata": {
    "papermill": {
     "duration": 0.024124,
     "end_time": "2023-04-29T08:16:07.000713",
     "exception": false,
     "start_time": "2023-04-29T08:16:06.976589",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So now we know if we do this iteratively, we will minimise the loss, and iteratively we will reach the optimal values of `weights` or `m` and `biases` or `m`\n",
    "\n",
    "Lets say we have runn this again and again for around 100 times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "152b1189",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:07.052518Z",
     "iopub.status.busy": "2023-04-29T08:16:07.051972Z",
     "iopub.status.idle": "2023-04-29T08:16:07.058515Z",
     "shell.execute_reply": "2023-04-29T08:16:07.057384Z"
    },
    "papermill": {
     "duration": 0.034356,
     "end_time": "2023-04-29T08:16:07.060577",
     "exception": false,
     "start_time": "2023-04-29T08:16:07.026221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    weights -= -2 * loss * 0.01\n",
    "    biases -= -2 * loss * 0.01\n",
    "    loss = (60 - (weights * 30 + biases))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe527b2",
   "metadata": {
    "papermill": {
     "duration": 0.024328,
     "end_time": "2023-04-29T08:16:07.109408",
     "exception": false,
     "start_time": "2023-04-29T08:16:07.085080",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Lets now see the weights and biases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d66eebd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:07.161057Z",
     "iopub.status.busy": "2023-04-29T08:16:07.160593Z",
     "iopub.status.idle": "2023-04-29T08:16:07.167289Z",
     "shell.execute_reply": "2023-04-29T08:16:07.166482Z"
    },
    "papermill": {
     "duration": 0.035202,
     "end_time": "2023-04-29T08:16:07.169303",
     "exception": false,
     "start_time": "2023-04-29T08:16:07.134101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.74575763]), array([-22.37272891]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights , biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafaa7f7",
   "metadata": {
    "papermill": {
     "duration": 0.024803,
     "end_time": "2023-04-29T08:16:07.218516",
     "exception": false,
     "start_time": "2023-04-29T08:16:07.193713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Though we have biases as high, but we have almost achived value of `weights`\n",
    "\n",
    "Lets do this all again, and now we will also try to plot a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdbe3dd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:07.269140Z",
     "iopub.status.busy": "2023-04-29T08:16:07.268749Z",
     "iopub.status.idle": "2023-04-29T08:16:07.276361Z",
     "shell.execute_reply": "2023-04-29T08:16:07.275105Z"
    },
    "papermill": {
     "duration": 0.036124,
     "end_time": "2023-04-29T08:16:07.278725",
     "exception": false,
     "start_time": "2023-04-29T08:16:07.242601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = abs(np.random.randn(1))\n",
    "biases = abs(np.random.randn(1))\n",
    "losses = []\n",
    "for _ in range(100):\n",
    "    weights -= -2 * loss * 0.01\n",
    "    biases -= -2 * loss * 0.01\n",
    "    loss = (60 - (weights * 30 + biases))\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5048493",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:07.329999Z",
     "iopub.status.busy": "2023-04-29T08:16:07.329589Z",
     "iopub.status.idle": "2023-04-29T08:16:07.564560Z",
     "shell.execute_reply": "2023-04-29T08:16:07.563416Z"
    },
    "papermill": {
     "duration": 0.263835,
     "end_time": "2023-04-29T08:16:07.567283",
     "exception": false,
     "start_time": "2023-04-29T08:16:07.303448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(np.array(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158790b8",
   "metadata": {
    "papermill": {
     "duration": 0.024412,
     "end_time": "2023-04-29T08:16:07.616562",
     "exception": false,
     "start_time": "2023-04-29T08:16:07.592150",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As we can see we have greatly decreased our losses "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c79fb4",
   "metadata": {
    "papermill": {
     "duration": 0.024383,
     "end_time": "2023-04-29T08:16:07.665533",
     "exception": false,
     "start_time": "2023-04-29T08:16:07.641150",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.1 | Functionalities\n",
    "We have made out our **SGD**, now we need to add some functionalities to it. We can get funcitonalites form **[Tensorflow](https://www.tensorflow.org/)=>[Keras](https://keras.io/about/)=>[Optimizers](https://keras.io/api/optimizers/)=>[Experimental](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental)=>[SGD](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental/SGD)**. The type of optimization tensorflow uses is a little bit different. It initializes and keeps the computation in the optimizer function and do the `fit` and `predict` on another function that uses this function. This is done so as to make a generalized optimizer for both `Linear` and `Sequential` models. But we will try to use the vanilla gradient descent and combine the initialization and the `fit` methods.\n",
    "\n",
    "* List Of columns\n",
    "* `learning_rate` - A Tensor, floating point value, or a schedule that is a **[tf.keras.optimizers.schedules.LearningRateSchedule](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/LearningRateSchedule)**, or a callable that takes no arguments and returns the actual value to use. The learning rate. Defaults to $0.001$.\n",
    "* `momentum` - float hyperparameter >= 0 that accelerates gradient descent in the relevant direction and dampens oscillations. Defaults to 0, $i.e.$, vanilla gradient descent.\n",
    "* `weight_decay : Float, defaults to None` - If set, weight decay is applied.\n",
    "* `nesterov : boolean` - Whether to apply Nesterov momentum. Defaults to False.\n",
    "* `weight_decay : Float, defaults to None` - If set, weight decay is applied.\n",
    "* `clipnorm : Float` - If set, the gradient of each weight is individually clipped so that its norm is no higher than this value.\n",
    "* `clipvalue : Float` - If set, the gradient of each weight is clipped to be no higher than this value.\n",
    "* `use_ema : Boolean, defaults to False` - If True, exponential moving average (EMA) is applied. EMA consists of computing an exponential moving average of the weights of the model (as the weight values change after each training batch), and periodically overwriting the weights with their moving average.\n",
    "* `ema_momentum : Float, defaults to 0.99` - Only used if use_ema=True. This is the momentum to use when computing the EMA of the model's weights: `new_average = ema_momentum * old_average + (1 - ema_momentum) * current_variable_value`.\n",
    "\n",
    "# 2.1.1 | List Of Columns \n",
    "\n",
    "This function will only work if there are only $2$ columns, one `feature` and the other one `target`. What if the user gives out a list of columns. For this we nee dto take two different arguemnts form the user and work on them differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7ed939f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:07.716732Z",
     "iopub.status.busy": "2023-04-29T08:16:07.716298Z",
     "iopub.status.idle": "2023-04-29T08:16:07.724282Z",
     "shell.execute_reply": "2023-04-29T08:16:07.722999Z"
    },
    "papermill": {
     "duration": 0.036417,
     "end_time": "2023-04-29T08:16:07.726622",
     "exception": false,
     "start_time": "2023-04-29T08:16:07.690205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SGD(X , y):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        weights -= -2 * loss * 0.01\n",
    "        biases -= -2 * loss * 0.01\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a504e92a",
   "metadata": {
    "papermill": {
     "duration": 0.024079,
     "end_time": "2023-04-29T08:16:07.775400",
     "exception": false,
     "start_time": "2023-04-29T08:16:07.751321",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.1.2 | Learning Rate\n",
    "\n",
    "You remeber we were taking a small part of the loss, not the whole loss. The parameter that defines how much loss we are taking is called the **Learning Rate** \n",
    "\n",
    "You might be thinking that it is not that important, but it is really important concept, a higher learning rate has a high chance that you will never converge to the model, a low very low learning rate means that you will take a very long time to converge for the model \n",
    "\n",
    "Here is a very good image that explains the importance of the learning rate \n",
    "\n",
    "<img src = \"https://www.researchgate.net/profile/Hajar-Feizi/publication/341609757/figure/fig2/AS:894745802977280@1590335431623/Changes-in-the-loss-function-vs-the-epoch-by-the-learning-rate-40.png\">\n",
    "\n",
    "It will be really easy for us to apply this functinality, we just need to change some varaibales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d2ed795",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:07.827278Z",
     "iopub.status.busy": "2023-04-29T08:16:07.826430Z",
     "iopub.status.idle": "2023-04-29T08:16:07.833545Z",
     "shell.execute_reply": "2023-04-29T08:16:07.832724Z"
    },
    "papermill": {
     "duration": 0.035375,
     "end_time": "2023-04-29T08:16:07.835752",
     "exception": false,
     "start_time": "2023-04-29T08:16:07.800377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SGD(X , y , learning_rate = 0.01):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        weights -= -2 * loss * learning_rate\n",
    "        biases -= -2 * loss * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a06963",
   "metadata": {
    "papermill": {
     "duration": 0.024049,
     "end_time": "2023-04-29T08:16:07.884878",
     "exception": false,
     "start_time": "2023-04-29T08:16:07.860829",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.1.3 | Momentum \n",
    "\n",
    "The probelem with gradient descent is this \n",
    "\n",
    "<img src = \"https://winder.ai/blog/2017/img/gradient_descent.svg\">\n",
    "\n",
    "Notice that before we were taking larger steps and as we reached the minimum value, are step size shortened. It is kind of a blessing as well as sometimes a curse for us. The problem is while getting very near to the minimum value, the step size gets so small. that it merely becomes $0$. not $0$ (that means we have reahed the minimum value). To cunter this we introduce momentum to the formula \n",
    "\n",
    "Lets assume you are going to a place that you dont know. Like you dont know where it exists. So what you do is you ask people in the way that where is the place, and they show you the directions. Lets assume the directions are only right $(->)$ and left $(<-)$. So you ask $1^{st}$ person and he says to go to the right $(->)$, then you ask the $2^{nd}$ person and he also asks you to go right $(->)$, so you gain a confidence that you are going right. so you increase your speed. you might skip the $3^{rd}$ person and directly ask the $4^{th}$. \n",
    "\n",
    "Lets assume the same situation from start. So you ask the $1^{st}$ person and he says to go to the right $(->)$ and then you ask the $2^{nd}$ person and he says to go left. But your inner instinct says that you are in the correct direction, but due to the influence of the $2^{nd}$ person, you will go slowly.\n",
    "\n",
    "So you increase and decrease you speed on the basis of the previously gained knowledge. This is the same concept `momentum`, tries to implement. \n",
    "\n",
    "One way of doing so is to add the commulative sum of all the gradients we achived previously like this \n",
    "\n",
    "$$w_{n+1} = w_n - \\frac {dLoss}{dw} \\alpha + (\\sum\\limits_{i = 1}^{n}w_i)$$\n",
    "\n",
    "$$b_{n+1} = b_n - \\frac {dLoss}{db} \\alpha + (\\sum\\limits_{i = 1}^{n}b_i)$$\n",
    "\n",
    "But there are majorly $2$ probelems with this formula $:-$\n",
    "* Rather than fastening the gradients a little bit, it will fasten them exponentially.\n",
    "* This formula values every gradient equal, \n",
    "\n",
    "To rectify this probelm we take the weighted average sum of all the gradients. or we actually multiply the sum with some constant. we change the formula a little bit \n",
    "\n",
    "$$w_{n+1} = w_n - (\\beta w_m + \\alpha(1 - \\beta) \\frac {dLoss}{dw})$$\n",
    "\n",
    "$$b_{n+1} = b_n - (\\beta b_m + \\alpha(1 - \\beta) \\frac {dLoss}{db})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95fb02b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:07.936571Z",
     "iopub.status.busy": "2023-04-29T08:16:07.935398Z",
     "iopub.status.idle": "2023-04-29T08:16:07.943800Z",
     "shell.execute_reply": "2023-04-29T08:16:07.942830Z"
    },
    "papermill": {
     "duration": 0.037171,
     "end_time": "2023-04-29T08:16:07.946447",
     "exception": false,
     "start_time": "2023-04-29T08:16:07.909276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SGD(X , y , learning_rate = 0.01 , momentum = 0):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        m_weights = momentum * m_weights + (1 - momentum) * (-2 * loss)\n",
    "        m_biases = momentum * m_biases + (1 - momentum) * (-2 * loss)\n",
    "        \n",
    "        weights -= m_weights[epochs + 1] * 0.01\n",
    "        biases -= m_biases[epochs + 1] * 0.01\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc087c59",
   "metadata": {
    "papermill": {
     "duration": 0.024241,
     "end_time": "2023-04-29T08:16:07.995366",
     "exception": false,
     "start_time": "2023-04-29T08:16:07.971125",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.1.4 | Nestrov\n",
    "While SGD with momentum can be effective at overcoming oscillations in the cost function, NAG has been shown to converge faster and more reliably in many cases, and thats why we will give the functionality of this function to \n",
    "$$w^n = w^{n-1} - \\beta w_v^{n-1} + (1-\\beta)(w^{n-1} - \\beta w_v^{n-1})$$\n",
    "$$b^n = b^{n-1} - \\beta b_v^{n-1} + (1-\\beta)(b^{n-1} - \\beta b_v^{n-1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52f82c64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:08.047584Z",
     "iopub.status.busy": "2023-04-29T08:16:08.046807Z",
     "iopub.status.idle": "2023-04-29T08:16:08.055786Z",
     "shell.execute_reply": "2023-04-29T08:16:08.054883Z"
    },
    "papermill": {
     "duration": 0.03777,
     "end_time": "2023-04-29T08:16:08.058210",
     "exception": false,
     "start_time": "2023-04-29T08:16:08.020440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SGD(X , y , learning_rate = 0.01 , momentum = 0 , nestrov = False):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "    \n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "    \n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if nestrov :\n",
    "\n",
    "            m_weights = (momentum * m_weights + ((1 - momentum) * (weights - momentum * m_weights))) * -2 * loss \n",
    "            m_biases = (momentum * m_biases + ((1 - momentum) * (weights - momentum * m_biases))) * -2 * loss\n",
    "\n",
    "        else :\n",
    "\n",
    "            m_weights = momentum * m_weights + (1 - momentum) * -2 * loss\n",
    "            m_biases = momentum * m_biases + (1 - momentum) * -2 * loss\n",
    "\n",
    "        weights -= m_weights * learning_rate\n",
    "        biases -= m_biases * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6fbefd",
   "metadata": {
    "papermill": {
     "duration": 0.024722,
     "end_time": "2023-04-29T08:16:08.107538",
     "exception": false,
     "start_time": "2023-04-29T08:16:08.082816",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.1.5 | Weight Decay \n",
    "Weight decay is a powerful regularization technique that can help to prevent overfitting and improve the generalization performance of machine learning models trained with SGD optimization.\n",
    "$$u = u + \\gamma u$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6e4304c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:08.159392Z",
     "iopub.status.busy": "2023-04-29T08:16:08.158323Z",
     "iopub.status.idle": "2023-04-29T08:16:08.167780Z",
     "shell.execute_reply": "2023-04-29T08:16:08.166864Z"
    },
    "papermill": {
     "duration": 0.037857,
     "end_time": "2023-04-29T08:16:08.170300",
     "exception": false,
     "start_time": "2023-04-29T08:16:08.132443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SGD(X , y , learning_rate = 0.01 , momentum = 0 , nestrov = False , weight_decay = None):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "    \n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "    \n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if nestrov :\n",
    "\n",
    "            m_weights = (momentum * m_weights + (1 - momentum) * (weights - momentum * m_weights)) * -2 * loss \n",
    "            m_biases = (momentum * m_biases + (1 - momentum) * (weights - momentum * m_biases)) * -2 * loss\n",
    "\n",
    "        else :\n",
    "\n",
    "            m_weights = momentum * m_weights + (1 - momentum) * -2 * loss\n",
    "            m_biases = momentum * m_biases + (1 - momentum) * -2 * loss\n",
    "\n",
    "        weights -= (m_weights + weight_decay * m_weights) * learning_rate\n",
    "        biases -= (m_biases + weight_decay * m_biases) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2991cf",
   "metadata": {
    "papermill": {
     "duration": 0.024346,
     "end_time": "2023-04-29T08:16:08.219734",
     "exception": false,
     "start_time": "2023-04-29T08:16:08.195388",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.1.6 | Clip Norm\n",
    "\n",
    "Sometimes there is a chance that the weights go skyrocketting, which is generally considered a bad idea, parameters in the range of $(-1, 1)$ are considered to be good. Thats why sometimes we use clip-norm , that generates a upper baseline for the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d27fa695",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:08.271622Z",
     "iopub.status.busy": "2023-04-29T08:16:08.270458Z",
     "iopub.status.idle": "2023-04-29T08:16:08.281361Z",
     "shell.execute_reply": "2023-04-29T08:16:08.280498Z"
    },
    "papermill": {
     "duration": 0.03924,
     "end_time": "2023-04-29T08:16:08.283695",
     "exception": false,
     "start_time": "2023-04-29T08:16:08.244455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SGD(X , y , learning_rate = 0.01 , momentum = 0 , nestrov = False , weight_decay = None , clip_norm = None):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "    \n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "    \n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if nestrov :\n",
    "\n",
    "            m_weights = (momentum * m_weights + (1 - momentum) * (weights - momentum * m_weights)) * -2 * loss \n",
    "            m_biases = (momentum * m_biases + (1 - momentum) * (weights - momentum * m_biases)) * -2 * loss\n",
    "\n",
    "        else :\n",
    "\n",
    "            m_weights = momentum * m_weights + (1 - momentum) * -2 * loss\n",
    "            m_biases = momentum * m_biases + (1 - momentum) * -2 * loss\n",
    "\n",
    "        if clip_norm != None:\n",
    "            \n",
    "            weights = np.clip(weights , weights , clip_norm)\n",
    "            biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "        weights -= (m_weights + weight_decay * m_weights) * learning_rate\n",
    "        biases -= (m_biases + weight_decay * m_biases) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a765c72f",
   "metadata": {
    "papermill": {
     "duration": 0.026373,
     "end_time": "2023-04-29T08:16:08.335054",
     "exception": false,
     "start_time": "2023-04-29T08:16:08.308681",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.1.7 | Clip Values\n",
    "\n",
    "Sometimes the same can go with the `gradients` and thus we also clip them. Higher gradients means that learning rate is very high and need to be lowered. This gives us $2$ tuners to control the gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bbf6db6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:08.386930Z",
     "iopub.status.busy": "2023-04-29T08:16:08.386516Z",
     "iopub.status.idle": "2023-04-29T08:16:08.397525Z",
     "shell.execute_reply": "2023-04-29T08:16:08.396257Z"
    },
    "papermill": {
     "duration": 0.039812,
     "end_time": "2023-04-29T08:16:08.399903",
     "exception": false,
     "start_time": "2023-04-29T08:16:08.360091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SGD(X , y , learning_rate = 0.01 , momentum = 0 , nestrov = False , weight_decay = None , clip_norm = None):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "    \n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "    \n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if nestrov :\n",
    "\n",
    "            m_weights = (momentum * m_weights + (1 - momentum) * (weights - momentum * m_weights)) * -2 * loss \n",
    "            m_biases = (momentum * m_biases + (1 - momentum) * (weights - momentum * m_biases)) * -2 * loss\n",
    "\n",
    "        else :\n",
    "\n",
    "            m_weights = momentum * m_weights + (1 - momentum) * -2 * loss\n",
    "            m_biases = momentum * m_biases + (1 - momentum) * -2 * loss\n",
    "\n",
    "        if clip_norm != None:\n",
    "            \n",
    "            weights = np.clip(weights , weights , clip_norm)\n",
    "            biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "        if clip_value != None:\n",
    "            \n",
    "            m_weights = np.clip(m_weights , m_weights , clip_value)\n",
    "            m_biases = np.clip(m_biases , m_biases , clip_value)\n",
    "\n",
    "        weights -= (m_weights + weight_decay * m_weights) * learning_rate\n",
    "        biases -= (m_biases + weight_decay * m_biases) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9577ba67",
   "metadata": {
    "papermill": {
     "duration": 0.024839,
     "end_time": "2023-04-29T08:16:08.449518",
     "exception": false,
     "start_time": "2023-04-29T08:16:08.424679",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.1.8 | Use EMA\n",
    "\n",
    "Wether to use the estimated momentum average or not. The major use of this function is when `ema_momentum` is given "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "830f7196",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:08.501488Z",
     "iopub.status.busy": "2023-04-29T08:16:08.500564Z",
     "iopub.status.idle": "2023-04-29T08:16:08.511865Z",
     "shell.execute_reply": "2023-04-29T08:16:08.510909Z"
    },
    "papermill": {
     "duration": 0.040098,
     "end_time": "2023-04-29T08:16:08.514390",
     "exception": false,
     "start_time": "2023-04-29T08:16:08.474292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SGD(X , y , learning_rate = 0.01 , momentum = 0 , nestrov = False , weight_decay = None , clip_norm = None , ema = False):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "    \n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "    \n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if nestrov :\n",
    "\n",
    "            m_weights = (momentum * m_weights + (1 - momentum) * (weights - momentum * m_weights)) * -2 * loss \n",
    "            m_biases = (momentum * m_biases + (1 - momentum) * (weights - momentum * m_biases)) * -2 * loss\n",
    "\n",
    "        else :\n",
    "\n",
    "            m_weights = momentum * m_weights + (1 - momentum) * -2 * loss\n",
    "            m_biases = momentum * m_biases + (1 - momentum) * -2 * loss\n",
    "\n",
    "        if ema:pass\n",
    "\n",
    "        if clip_norm != None:\n",
    "            \n",
    "            weights = np.clip(weights , weights , clip_norm)\n",
    "            biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "        if clip_value != None:\n",
    "            \n",
    "            m_weights = np.clip(m_weights , m_weights , clip_value)\n",
    "            m_biases = np.clip(m_biases , m_biases , clip_value)\n",
    "\n",
    "        weights -= (m_weights + weight_decay * m_weights) * learning_rate\n",
    "        biases -= (m_biases + weight_decay * m_biases) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30975a2d",
   "metadata": {
    "papermill": {
     "duration": 0.025021,
     "end_time": "2023-04-29T08:16:08.565239",
     "exception": false,
     "start_time": "2023-04-29T08:16:08.540218",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.1.9 | EMA Momentum \n",
    "\n",
    "In RMSprop, the exponential moving average (EMA) of the gradient with momentum is used to improve convergence and prevent oscillations during training. The EMA of the gradient with momentum is updated at each iteration and incorporates information about the previous gradients and the current gradient to produce a more stable and consistent update direction. This helps to smooth out the noise in the gradient estimates and helps the optimizer to move more directly towards the minimum of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3aa26f84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:08.617807Z",
     "iopub.status.busy": "2023-04-29T08:16:08.617044Z",
     "iopub.status.idle": "2023-04-29T08:16:08.629250Z",
     "shell.execute_reply": "2023-04-29T08:16:08.628003Z"
    },
    "papermill": {
     "duration": 0.04082,
     "end_time": "2023-04-29T08:16:08.631634",
     "exception": false,
     "start_time": "2023-04-29T08:16:08.590814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SGD(X , y , learning_rate = 0.01 , momentum = 0 , nestrov = False , weight_decay = None , clip_norm = None , ema = False , ema_momentum = 0.99):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "    \n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "    \n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum((pred - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if nestrov :\n",
    "\n",
    "            m_weights = (momentum * m_weights + (1 - momentum) * (weights - momentum * m_weights)) * -2 * loss \n",
    "            m_biases = (momentum * m_biases + (1 - momentum) * (weights - momentum * m_biases)) * -2 * loss\n",
    "\n",
    "        else :\n",
    "\n",
    "            m_weights = momentum * m_weights + (1 - momentum) * -2 * loss\n",
    "            m_biases = momentum * m_biases + (1 - momentum) * -2 * loss\n",
    "\n",
    "        if ema:\n",
    "            m_weights = ema_momentum * m_weights + (1 - ema_momentum) * -2 * loss\n",
    "            m_biases = ema_momentum * m_biases + (1 - ema_momentum) * -2 * loss\n",
    "\n",
    "        if clip_norm != None:\n",
    "            \n",
    "            weights = np.clip(weights , weights , clip_norm)\n",
    "            biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "        if clip_value != None:\n",
    "            \n",
    "            m_weights = np.clip(m_weights , m_weights , clip_value)\n",
    "            m_biases = np.clip(m_biases , m_biases , clip_value)\n",
    "\n",
    "        weights -= (m_weights + weight_decay * m_weights) * learning_rate\n",
    "        biases -= (m_biases + weight_decay * m_biases) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366dd9c1",
   "metadata": {
    "papermill": {
     "duration": 0.024597,
     "end_time": "2023-04-29T08:16:08.681483",
     "exception": false,
     "start_time": "2023-04-29T08:16:08.656886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.2 | Methods\n",
    "\n",
    "Now to add more functionalities, we will add different methods to our alogirthm, You can acces the list of methods we are going to use from the same link before we used for `Functionalities`. \n",
    "* `build` - Initialize optimizer variables. SGD optimizer has one variable momentums, only set if self.momentum is not 0.\n",
    "* * `weights` - Weights of the Optimizer\n",
    "* * `biases` - biases of the Optimizer\n",
    "* `compute_gradients` - Compute gradients of loss on trainable variables.\n",
    "* `minimize` - Minimize loss by updating `var_list`.\n",
    "* `update_step` - Update step given gradient and the associated model variable.\n",
    "\n",
    "**Note** - we will stop the function from being usable at some places\n",
    "\n",
    "# 2.2.1 | Build\n",
    "This method initializes all the values in the `SGD`, you can understand this as a constructor, but not perfectly as the one. \n",
    "\n",
    "We will give the user a functionality, that they can intialize there own variables. At the starting the `weights` and `baises` are `None`. If the user passes their own, the function first checks that they are or correct shape and if True assigns the values, else initialize random values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7e881c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:08.733776Z",
     "iopub.status.busy": "2023-04-29T08:16:08.732983Z",
     "iopub.status.idle": "2023-04-29T08:16:08.752278Z",
     "shell.execute_reply": "2023-04-29T08:16:08.750915Z"
    },
    "papermill": {
     "duration": 0.048521,
     "end_time": "2023-04-29T08:16:08.755104",
     "exception": false,
     "start_time": "2023-04-29T08:16:08.706583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \n",
    "    def __init__(self , \n",
    "                 X , y , \n",
    "                 learning_rate = 0.01 , momentum = 0 , \n",
    "                 nestrov = False , weight_decay = 0 , \n",
    "                 clip_norm = None , clip_value = None , \n",
    "                 use_ema = False , ema_momentum = 0):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.nestrov = nestrov\n",
    "        self.weight_decay = weight_decay\n",
    "        self.clip_norm = clip_norm\n",
    "        self.clip_value = clip_value\n",
    "        self.use_ema = use_ema\n",
    "        self.ema_momentum = ema_momentum\n",
    "    \n",
    "    def build(self , weights = None , biases = None):\n",
    "        \n",
    "        if weights == None:\n",
    "            \n",
    "            self.weights = abs(np.random.randn(self.X.shape[0]))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if weights.shape[0] == self.X.shape[1]:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "                \n",
    "                raise UserWarning(\"Values do not match with the `X` , intializing random Values\")\n",
    "                \n",
    "        \n",
    "        if biases == None:\n",
    "            \n",
    "            self.biases = abs(np.random.randn(1))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if biases.shape[0] == 1:\n",
    "                \n",
    "                self.biases = biases\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                self.biases = abs(np.random.randn(1))\n",
    "                \n",
    "                raise UserWarning(\"Values do not match with the `X` , intializing random Values\")\n",
    "    \n",
    "        self.m_weights = 0\n",
    "        self.m_biases = 0\n",
    "    \n",
    "        self.losses = []\n",
    "    \n",
    "        for _ in range(100):\n",
    "        \n",
    "            pred = np.sum((weights * self.X).T) + biases\n",
    "            \n",
    "            loss = np.sum((pred - self.y) ** 2)\n",
    "            self.losses.append(loss)\n",
    "            \n",
    "            if self.nestrov :\n",
    "\n",
    "                self.m_weights = (self.momentum * self.m_weights + (1 - self.momentum) * (self.weights - self.momentum * self.m_weights)) * -2 * loss \n",
    "                self.m_biases = (self.momentum * self.m_biases + (1 - self.momentum) * (self.biases - self.momentum * self.m_biases)) * -2 * loss\n",
    "\n",
    "            else :\n",
    "\n",
    "                self.m_weights = self.momentum * self.m_weights + (1 - self.momentum) * -2 * loss\n",
    "                self.m_biases = self.momentum * self.m_biases + (1 - self.momentum) * -2 * loss\n",
    "            \n",
    "            if self.use_ema:\n",
    "                \n",
    "                self.m_weights = self.ema_momentum * self.m_weights + (1 - self.ema_momentum) * -2 * loss\n",
    "                self.m_biases = self.ema_momentum * self.m_biases + (1 - self.ema_momentum) * -2 * loss\n",
    "                \n",
    "            if self.clip_norm != None:\n",
    "                \n",
    "                weights = np.clip(weights , weights , self.clip_norm)\n",
    "                biases = np.clip(biases , biases , self.clip_norm)\n",
    "            \n",
    "            if self.clip_value != None:\n",
    "                \n",
    "                self.m_weights = np.clip(self.m_weights , self.m_weights , self.clip_value)\n",
    "                self.m_biases = np.clip(self.m_biases , self.m_biases , self.clip_value)\n",
    "            \n",
    "            weights -= (self.m_weights + self.weight_decay * self.m_weights) * self.learning_rate\n",
    "            biases -= (self.m_biases + self.weight_decay * self.m_biases) * self.learning_rate\n",
    "\n",
    "        return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f772b75",
   "metadata": {
    "papermill": {
     "duration": 0.024394,
     "end_time": "2023-04-29T08:16:08.804940",
     "exception": false,
     "start_time": "2023-04-29T08:16:08.780546",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.2.2 | Compute Gradients\n",
    "\n",
    "Compuute the loss function and gradients for the losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f393a447",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:08.856560Z",
     "iopub.status.busy": "2023-04-29T08:16:08.855789Z",
     "iopub.status.idle": "2023-04-29T08:16:08.868973Z",
     "shell.execute_reply": "2023-04-29T08:16:08.867954Z"
    },
    "papermill": {
     "duration": 0.04167,
     "end_time": "2023-04-29T08:16:08.871510",
     "exception": false,
     "start_time": "2023-04-29T08:16:08.829840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \n",
    "    def __init__(self , \n",
    "                 X , y , \n",
    "                 learning_rate = 0.01 , momentum = 0 , \n",
    "                 nestrov = False , weight_decay = 0 , \n",
    "                 clip_norm = None , clip_value = None , \n",
    "                 use_ema = False , ema_momentum = 0):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.nestrov = nestrov\n",
    "        self.weight_decay = weight_decay\n",
    "        self.clip_norm = clip_norm\n",
    "        self.clip_value = clip_value\n",
    "        self.use_ema = use_ema\n",
    "        self.ema_momentum = ema_momentum\n",
    "    \n",
    "    def build(self , weights = None , biases = None):\n",
    "        \n",
    "        if weights == None:\n",
    "            \n",
    "            self.weights = abs(np.random.randn(self.X.shape[0]))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if weights.shape[0] == self.X.shape[1]:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "                \n",
    "                raise UserWarning(\"Values do not match with the `X` , intializing random Values\")\n",
    "                \n",
    "        \n",
    "        if biases == None:\n",
    "            \n",
    "            self.biases = abs(np.random.randn(1))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if biases.shape[0] == 1:\n",
    "                \n",
    "                self.biases = biases\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                self.biases = abs(np.random.randn(1))\n",
    "                \n",
    "                raise UserWarning(\"Values do not match with the `X` , intializing random Values\")\n",
    "    \n",
    "        self.m_weights = 0\n",
    "        self.m_biases = 0\n",
    "    \n",
    "        self.losses = []\n",
    "\n",
    "    def comput_gradients(self , weights , biases):\n",
    "        \n",
    "            pred = np.sum((weights * self.X).T) + biases\n",
    "            \n",
    "            loss = np.sum((pred - self.y) ** 2)\n",
    "            \n",
    "            yield loss\n",
    "            \n",
    "            self.losses.append(loss)\n",
    "            \n",
    "        #     if self.nestrov :\n",
    "\n",
    "        #         self.m_weights = (self.momentum * self.m_weights + (1 - self.momentum) * (self.weights - self.momentum * self.m_weights)) * -2 * loss \n",
    "        #         self.m_biases = (self.momentum * self.m_biases + (1 - self.momentum) * (self.biases - self.momentum * self.m_biases)) * -2 * loss\n",
    "\n",
    "        #     else :\n",
    "\n",
    "        #         self.m_weights = self.momentum * self.m_weights + (1 - self.momentum) * -2 * loss\n",
    "        #         self.m_biases = self.momentum * self.m_biases + (1 - self.momentum) * -2 * loss\n",
    "            \n",
    "        #     if self.use_ema:\n",
    "                \n",
    "        #         self.m_weights = self.ema_momentum * self.momentum + (1 - self.ema_momentum) * -2 * loss\n",
    "        #         self.m_biases = self.ema_momentum * self.momentum + (1 - self.ema_momentum) * -2 * loss\n",
    "                \n",
    "        #     if self.clip_norm != None:\n",
    "                \n",
    "        #         weights = np.clip(weights , weights , self.clip_norm)\n",
    "        #         biases = np.clip(biases , biases , self.clip_norm)\n",
    "            \n",
    "        #     if self.clip_value != None:\n",
    "                \n",
    "        #         self.m_weights = np.clip(self.m_weights , self.m_weights , self.clip_value)\n",
    "        #         self.m_biases = np.clip(self.m_biases , self.m_biases , self.clip_value)\n",
    "            \n",
    "        #     weights -= (self.m_weights + self.weight_decay * self.m_weights) * self.learning_rate\n",
    "        #     biases -= (self.m_biases + self.weight_decay * self.m_biases) * self.learning_rate\n",
    "\n",
    "        # return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359586ad",
   "metadata": {
    "papermill": {
     "duration": 0.024573,
     "end_time": "2023-04-29T08:16:08.920927",
     "exception": false,
     "start_time": "2023-04-29T08:16:08.896354",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.2.3 | Minimize \n",
    "Minimze the loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9668b0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:08.973369Z",
     "iopub.status.busy": "2023-04-29T08:16:08.972934Z",
     "iopub.status.idle": "2023-04-29T08:16:08.989036Z",
     "shell.execute_reply": "2023-04-29T08:16:08.988105Z"
    },
    "papermill": {
     "duration": 0.045705,
     "end_time": "2023-04-29T08:16:08.991319",
     "exception": false,
     "start_time": "2023-04-29T08:16:08.945614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \n",
    "    def __init__(self , \n",
    "                 X , y , \n",
    "                 learning_rate = 0.01 , momentum = 0 , \n",
    "                 nestrov = False , weight_decay = 0 , \n",
    "                 clip_norm = None , clip_value = None , \n",
    "                 use_ema = False , ema_momentum = 0):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.nestrov = nestrov\n",
    "        self.weight_decay = weight_decay\n",
    "        self.clip_norm = clip_norm\n",
    "        self.clip_value = clip_value\n",
    "        self.use_ema = use_ema\n",
    "        self.ema_momentum = ema_momentum\n",
    "    \n",
    "    def build(self , weights = None , biases = None):\n",
    "        \n",
    "        if weights == None:\n",
    "            \n",
    "            self.weights = abs(np.random.randn(self.X.shape[0]))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if weights.shape[0] == self.X.shape[1]:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "                \n",
    "                raise UserWarning(\"Values do not match with the `X` , intializing random Values\")\n",
    "                \n",
    "        \n",
    "        if biases == None:\n",
    "            \n",
    "            self.biases = abs(np.random.randn(1))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if biases.shape[0] == 1:\n",
    "                \n",
    "                self.biases = biases\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                self.biases = abs(np.random.randn(1))\n",
    "                \n",
    "                raise UserWarning(\"Values do not match with the `X` , intializing random Values\")\n",
    "    \n",
    "        self.m_weights = 0\n",
    "        self.m_biases = 0\n",
    "    \n",
    "        self.losses = []\n",
    "\n",
    "    def comput_gradients(self , weights , biases):\n",
    "        \n",
    "            pred = np.sum((weights * self.X).T) + biases\n",
    "            \n",
    "            loss = np.sum((pred - self.y) ** 2)\n",
    "            \n",
    "            yield loss\n",
    "            \n",
    "            self.losses.append(loss)\n",
    "            \n",
    "        #     if self.nestrov :\n",
    "\n",
    "        #         self.m_weights = (self.momentum * self.m_weights + (1 - self.momentum) * (self.weights - self.momentum * self.m_weights)) * -2 * loss \n",
    "        #         self.m_biases = (self.momentum * self.m_biases + (1 - self.momentum) * (self.biases - self.momentum * self.m_biases)) * -2 * loss\n",
    "\n",
    "        #     else :\n",
    "\n",
    "        #         self.m_weights = self.momentum * self.m_weights + (1 - self.momentum) * -2 * loss\n",
    "        #         self.m_biases = self.momentum * self.m_biases + (1 - self.momentum) * -2 * loss\n",
    "            \n",
    "        #     if self.use_ema:\n",
    "                \n",
    "        #         self.m_weights = self.ema_momentum * self.momentum + (1 - self.ema_momentum) * -2 * loss\n",
    "        #         self.m_biases = self.ema_momentum * self.momentum + (1 - self.ema_momentum) * -2 * loss\n",
    "    def minimize(self):                \n",
    "        if self.clip_norm != None:\n",
    "            \n",
    "            weights = np.clip(weights , weights , self.clip_norm)\n",
    "            biases = np.clip(biases , biases , self.clip_norm)\n",
    "        \n",
    "        if self.clip_value != None:\n",
    "            \n",
    "            self.m_weights = np.clip(self.m_weights , self.m_weights , self.clip_value)\n",
    "            self.m_biases = np.clip(self.m_biases , self.m_biases , self.clip_value)\n",
    "        \n",
    "        weights -= (self.m_weights + self.weight_decay * self.m_weights) * self.learning_rate\n",
    "        biases -= (self.m_biases + self.weight_decay * self.m_biases) * self.learning_rate\n",
    "\n",
    "        # return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dda6fc9",
   "metadata": {
    "papermill": {
     "duration": 0.024848,
     "end_time": "2023-04-29T08:16:09.040960",
     "exception": false,
     "start_time": "2023-04-29T08:16:09.016112",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.2.4 | Update_Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d05ce8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:09.092889Z",
     "iopub.status.busy": "2023-04-29T08:16:09.092471Z",
     "iopub.status.idle": "2023-04-29T08:16:09.111673Z",
     "shell.execute_reply": "2023-04-29T08:16:09.110731Z"
    },
    "papermill": {
     "duration": 0.048462,
     "end_time": "2023-04-29T08:16:09.114100",
     "exception": false,
     "start_time": "2023-04-29T08:16:09.065638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \n",
    "    def __init__(self , \n",
    "                 X , y , \n",
    "                 learning_rate = 0.01 , momentum = 0 , \n",
    "                 nestrov = False , weight_decay = 0 , \n",
    "                 clip_norm = None , clip_value = None , \n",
    "                 use_ema = False , ema_momentum = 0):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.nestrov = nestrov\n",
    "        self.weight_decay = weight_decay\n",
    "        self.clip_norm = clip_norm\n",
    "        self.clip_value = clip_value\n",
    "        self.use_ema = use_ema\n",
    "        self.ema_momentum = ema_momentum\n",
    "    \n",
    "    def build(self , weights = None , biases = None):\n",
    "        \n",
    "        if weights == None:\n",
    "            \n",
    "            self.weights = abs(np.random.randn(self.X.shape[0]))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if weights.shape[0] == self.X.shape[1]:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "                \n",
    "                raise UserWarning(\"Values do not match with the `X` , intializing random Values\")\n",
    "                \n",
    "        \n",
    "        if biases == None:\n",
    "            \n",
    "            self.biases = abs(np.random.randn(1))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if biases.shape[0] == 1:\n",
    "                \n",
    "                self.biases = biases\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                self.biases = abs(np.random.randn(1))\n",
    "                \n",
    "                raise UserWarning(\"Values do not match with the `X` , intializing random Values\")\n",
    "    \n",
    "        self.m_weights = 0\n",
    "        self.m_biases = 0\n",
    "    \n",
    "        self.losses = []\n",
    "\n",
    "    def comput_gradients(self , weights , biases):\n",
    "        \n",
    "            pred = np.sum((weights * self.X).T) + biases\n",
    "            \n",
    "            loss = np.sum((pred - self.y) ** 2)\n",
    "            \n",
    "            yield loss\n",
    "            \n",
    "            self.losses.append(loss)\n",
    "            \n",
    "    def update_step(self , loss):\n",
    "\n",
    "        if self.nestrov :\n",
    "\n",
    "            self.m_weights = (self.momentum * self.m_weights + (1 - self.momentum) * (self.weights - self.momentum * self.m_weights)) * -2 * loss \n",
    "            self.m_biases = (self.momentum * self.m_biases + (1 - self.momentum) * (self.biases - self.momentum * self.m_biases)) * -2 * loss\n",
    "\n",
    "        else :\n",
    "\n",
    "            self.m_weights = self.momentum * self.m_weights + (1 - self.momentum) * -2 * loss\n",
    "            self.m_biases = self.momentum * self.m_biases + (1 - self.momentum) * -2 * loss\n",
    "\n",
    "        if self.use_ema:\n",
    "\n",
    "            self.m_weights = self.ema_momentum * self.m_weights + (1 - self.ema_momentum) * -2 * loss\n",
    "            self.m_biases = self.ema_momentum * self.m_biases + (1 - self.ema_momentum) * -2 * loss\n",
    "    def minimize(self):                \n",
    "        if self.clip_norm != None:\n",
    "            \n",
    "            weights = np.clip(weights , weights , self.clip_norm)\n",
    "            biases = np.clip(biases , biases , self.clip_norm)\n",
    "        \n",
    "        if self.clip_value != None:\n",
    "            \n",
    "            self.m_weights = np.clip(self.m_weights , self.m_weights , self.clip_value)\n",
    "            self.m_biases = np.clip(self.m_biases , self.m_biases , self.clip_value)\n",
    "        \n",
    "        weights -= (self.m_weights + self.weight_decay * self.m_weights) * self.learning_rate\n",
    "        biases -= (self.m_biases + self.weight_decay * self.m_biases) * self.learning_rate\n",
    "\n",
    "        # return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f068ffe6",
   "metadata": {
    "papermill": {
     "duration": 0.024616,
     "end_time": "2023-04-29T08:16:09.163448",
     "exception": false,
     "start_time": "2023-04-29T08:16:09.138832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.2 | SGD Final Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b7dd171",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:09.215151Z",
     "iopub.status.busy": "2023-04-29T08:16:09.214403Z",
     "iopub.status.idle": "2023-04-29T08:16:09.233873Z",
     "shell.execute_reply": "2023-04-29T08:16:09.232655Z"
    },
    "papermill": {
     "duration": 0.048557,
     "end_time": "2023-04-29T08:16:09.236710",
     "exception": false,
     "start_time": "2023-04-29T08:16:09.188153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \n",
    "    def __init__(self , \n",
    "                 X , y , \n",
    "                 learning_rate = 0.01 , momentum = 0 , \n",
    "                 nestrov = False , weight_decay = 0 , \n",
    "                 clip_norm = None , clip_value = None , \n",
    "                 use_ema = False , ema_momentum = 0):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.nestrov = nestrov\n",
    "        self.weight_decay = weight_decay\n",
    "        self.clip_norm = clip_norm\n",
    "        self.clip_value = clip_value\n",
    "        self.use_ema = use_ema\n",
    "        self.ema_momentum = ema_momentum\n",
    "    \n",
    "    def build(self , weights = None , biases = None):\n",
    "        \n",
    "        if weights == None:\n",
    "            \n",
    "            self.weights = abs(np.random.randn(self.X.shape[0]))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if weights.shape[0] == self.X.shape[1]:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "                \n",
    "                raise UserWarning(\"Values do not match with the `X` , intializing random Values\")\n",
    "                \n",
    "        \n",
    "        if biases == None:\n",
    "            \n",
    "            self.biases = abs(np.random.randn(1))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if biases.shape[0] == 1:\n",
    "                \n",
    "                self.biases = biases\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                self.biases = abs(np.random.randn(1))\n",
    "                \n",
    "                raise UserWarning(\"Values do not match with the `X` , intializing random Values\")\n",
    "    \n",
    "        self.m_weights = 0\n",
    "        self.m_biases = 0\n",
    "    \n",
    "        self.losses = []\n",
    "\n",
    "    def comput_gradients(self , weights , biases):\n",
    "        \n",
    "            pred = np.sum((weights * self.X).T) + biases\n",
    "            \n",
    "            loss = np.sum((pred - self.y) ** 2)\n",
    "            \n",
    "            yield loss\n",
    "            \n",
    "            self.losses.append(loss)\n",
    "    def update_step(self , loss):    \n",
    "        \n",
    "        if self.nestrov :\n",
    "\n",
    "            self.m_weights = (self.momentum * self.m_weights + (1 - self.momentum) * (self.weights - self.momentum * self.m_weights)) * -2 * loss \n",
    "            self.m_biases = (self.momentum * self.m_biases + (1 - self.momentum) * (self.biases - self.momentum * self.m_biases)) * -2 * loss\n",
    "\n",
    "        else :\n",
    "\n",
    "            self.m_weights = self.momentum * self.m_weights + (1 - self.momentum) * -2 * loss\n",
    "            self.m_biases = self.momentum * self.m_biases + (1 - self.momentum) * -2 * loss\n",
    "        \n",
    "        if self.use_ema:\n",
    "            \n",
    "            self.m_weights = self.ema_momentum * self.m_weights + (1 - self.ema_momentum) * -2 * loss\n",
    "            self.m_biases = self.ema_momentum * self.m_weights + (1 - self.ema_momentum) * -2 * loss\n",
    "    \n",
    "    def minimize(self):                \n",
    "        if self.clip_norm != None:\n",
    "            \n",
    "            weights = np.clip(weights , weights , self.clip_norm)\n",
    "            biases = np.clip(biases , biases , self.clip_norm)\n",
    "        \n",
    "        if self.clip_value != None:\n",
    "            \n",
    "            self.m_weights = np.clip(self.m_weights , self.m_weights , self.clip_value)\n",
    "            self.m_biases = np.clip(self.m_biases , self.m_biases , self.clip_value)\n",
    "        \n",
    "        weights -= (self.m_weights + self.weight_decay * self.m_weights) * self.learning_rate\n",
    "        biases -= (self.m_biases + self.weight_decay * self.m_biases) * self.learning_rate\n",
    "\n",
    "        return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef932062",
   "metadata": {
    "papermill": {
     "duration": 0.024338,
     "end_time": "2023-04-29T08:16:09.285837",
     "exception": false,
     "start_time": "2023-04-29T08:16:09.261499",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3 | RMS Prop\n",
    "So what is this `RMSProp` and why the hell do we need this thing \n",
    "\n",
    "So actually there were some problems with `SGD`. I just searched on ChatGPT and found that \n",
    "* SGD was applying a fixed learning rate in every situation, and RMSProp adapts the learning rate for the situation\n",
    "* SGD is not really good with sparse inputs, and RMSProp works good with sparse inputs too\n",
    "We know a little bit about `SGD` before the basic vanilla formula for SGD is \n",
    "$$p_{n} = p_{n-1} - \\frac {dLoss}{dp}\\alpha$$\n",
    "The formula for `RMSProp` is simple as hell\n",
    "****\n",
    "$$v_t = \\beta v_{t-1} + (1 - \\beta)\\frac{dLoss}{dp}$$\n",
    "$$u = \\frac {n}{\\sqrt{v_t + e}}\\frac{dLoss}{dp}$$\n",
    "$$p_n = p_{n-1} - u$$\n",
    "****\n",
    "We know everything in this fomrula except of this one guy $e$, what is this $e$ doing here. Lets assume at some point the weight of one feature has became purely $0$. One way to think about this is as that feature doesnt contribute litrally anything to target values, then as we are dividing by `weight`  which is $0$, we will get error, as division by $0$ is not possible, thats why we add a terms $e$ in the weights. the $e$ is so small that when the weights have some value, it doesnt really make any sense, and if the values are $0$. it prevents from $0$ division\n",
    "Remeber this code...?\n",
    "\n",
    "We can modify this a little bit and we can find the code for `RMSProp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "67fdf69e",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:09.337317Z",
     "iopub.status.busy": "2023-04-29T08:16:09.336886Z",
     "iopub.status.idle": "2023-04-29T08:16:09.346084Z",
     "shell.execute_reply": "2023-04-29T08:16:09.345058Z"
    },
    "papermill": {
     "duration": 0.037647,
     "end_time": "2023-04-29T08:16:09.348214",
     "exception": false,
     "start_time": "2023-04-29T08:16:09.310567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SGD(X , y , learning_rate = 0.01 , momentum = 0):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    m_weights = [0] * (100 + 1)\n",
    "    m_biases = [0] * (100 + 1)\n",
    "\n",
    "    predic = []\n",
    "    losses = []\n",
    "\n",
    "    for epochs in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        m_weights[epochs + 1] = momentum * m_weights[epochs] + (1 - momentum) * (-2 * loss)\n",
    "        m_biases[epochs + 1] = momentum * m_biases[epochs] + (1 - momentum) * (-2 * loss)\n",
    "        \n",
    "        weights -= m_weights[epochs + 1] * 0.01\n",
    "        biases -= m_biases[epochs + 1] * 0.01\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74756c9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:09.399737Z",
     "iopub.status.busy": "2023-04-29T08:16:09.399301Z",
     "iopub.status.idle": "2023-04-29T08:16:09.407808Z",
     "shell.execute_reply": "2023-04-29T08:16:09.406658Z"
    },
    "papermill": {
     "duration": 0.036751,
     "end_time": "2023-04-29T08:16:09.409934",
     "exception": false,
     "start_time": "2023-04-29T08:16:09.373183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rms_prop(X , y):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for epochs in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        l_weights = 0 * l_weights + (1 - 0) * (-2 * loss)\n",
    "        l_biases = 0 * l_biases + (1 - 0) * (-2 * loss)\n",
    "        \n",
    "        weights -= 1/np.sqrt(l_weights + 1e-6) * 0.01\n",
    "        biases -= 1/np.sqrt(l_biases + 1e-6) * 0.01\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2ea7d8",
   "metadata": {
    "papermill": {
     "duration": 0.024114,
     "end_time": "2023-04-29T08:16:09.459127",
     "exception": false,
     "start_time": "2023-04-29T08:16:09.435013",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "And now we have our code for the `RMSProp` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc60b57c",
   "metadata": {
    "papermill": {
     "duration": 0.02379,
     "end_time": "2023-04-29T08:16:09.507152",
     "exception": false,
     "start_time": "2023-04-29T08:16:09.483362",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.1 | Functionalities\n",
    "Now lets add some functionalieties to our code as we did before to make it more usable\n",
    "* `learning_rate` - Initial value for the learning rate: either a floating point value, or a **[tf](https://www.tensorflow.org/)=>[keras](https://keras.io/)=>[optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)=>[schedules](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules)=>[LearningRateSchedule](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/LearningRateSchedule)** instance. Defaults to $0.001$.\n",
    "* `rho` - float, defaults to $0.9$. Discounting factor for the old gradients.\n",
    "* `momentum` - float, defaults to $0.0$. If not $0.0$., the optimizer tracks the momentum value, with a decay rate equals to $1$ - momentum.\n",
    "* `epsilon` - A small constant for numerical stability. This epsilon is \"epsilon hat\" in the Kingma and Ba paper (in the formula just before Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to $1e-7$.\n",
    "* `clipnorm` - Float. If set, the gradient of each weight is individually clipped so that its norm is no higher than this value.\n",
    "* `clipvalue` - Float. If set, the gradient of each weight is clipped to be no higher than this value.\n",
    "# 3.1.1 | Learning Rate\n",
    "\n",
    "It is the learning rate with how the model learns, we had a discussion about this when we were creating the `SGD`. Just scroll a little up and you will find that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce8bf365",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:09.557441Z",
     "iopub.status.busy": "2023-04-29T08:16:09.556991Z",
     "iopub.status.idle": "2023-04-29T08:16:09.564973Z",
     "shell.execute_reply": "2023-04-29T08:16:09.564091Z"
    },
    "papermill": {
     "duration": 0.035679,
     "end_time": "2023-04-29T08:16:09.566993",
     "exception": false,
     "start_time": "2023-04-29T08:16:09.531314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rms_prop(X , y , learning_rate = 0.01):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "    predic = []\n",
    "\n",
    "    for epochs in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        l_weights = 0 * l_weights + (1 - 0) * (-2 * loss)\n",
    "        l_biases = 0 * l_biases + (1 - 0) * (-2 * loss)\n",
    "        \n",
    "        weights -= 1/np.sqrt(l_weights + 1e-6) * learning_rate\n",
    "        biases -= 1/np.sqrt(l_biases + 1e-6) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f88623",
   "metadata": {
    "papermill": {
     "duration": 0.02419,
     "end_time": "2023-04-29T08:16:09.616025",
     "exception": false,
     "start_time": "2023-04-29T08:16:09.591835",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.1.2 | Rho\n",
    "This is the $\\beta$ we saw in the formula, we had made our algortihtm such that the $\\beta$ was $0$, now we will replace that $0$ with a vraible, that the user can change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "27ae1cff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:09.666177Z",
     "iopub.status.busy": "2023-04-29T08:16:09.665744Z",
     "iopub.status.idle": "2023-04-29T08:16:09.673638Z",
     "shell.execute_reply": "2023-04-29T08:16:09.672787Z"
    },
    "papermill": {
     "duration": 0.035682,
     "end_time": "2023-04-29T08:16:09.675757",
     "exception": false,
     "start_time": "2023-04-29T08:16:09.640075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rms_prop(X , y , learning_rate = 0.01 , rho = 0.9):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "\n",
    "    predic = []\n",
    "\n",
    "    for epochs in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        l_weights = rho * l_weights + (1 - rho) * (-2 * loss)\n",
    "        l_biases = rho * l_biases + (1 - rho) * (-2 * loss)\n",
    "        \n",
    "        weights -= 1/np.sqrt(l_weights + 1e-6) * learning_rate\n",
    "        biases -= 1/np.sqrt(l_biases + 1e-6) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3171c6f6",
   "metadata": {
    "papermill": {
     "duration": 0.024127,
     "end_time": "2023-04-29T08:16:09.724615",
     "exception": false,
     "start_time": "2023-04-29T08:16:09.700488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.1.3 | Momentum\n",
    "\n",
    "Momentum allows the optimizer to build up velocity in directions with consistent gradients, leading to faster convergence. It can also help to reduce oscillations in the optimization process by smoothing out variations in the gradient updates. The combination of RMSprop and momentum can lead to more efficient and effective training of deep neural networks.\n",
    "\n",
    "$$m = \\gamma m + (1 - \\gamma)\\frac {dLoss}{dp}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a9c8bd6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:09.777176Z",
     "iopub.status.busy": "2023-04-29T08:16:09.776717Z",
     "iopub.status.idle": "2023-04-29T08:16:09.786779Z",
     "shell.execute_reply": "2023-04-29T08:16:09.785474Z"
    },
    "papermill": {
     "duration": 0.039253,
     "end_time": "2023-04-29T08:16:09.789118",
     "exception": false,
     "start_time": "2023-04-29T08:16:09.749865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rms_prop(X , y , learning_rate = 0.01 , rho = 0.9 , momentum = 0):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "\n",
    "    u_weights = 0\n",
    "    u_biases = 0\n",
    "\n",
    "    predic = []\n",
    "\n",
    "    for epochs in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        u_weights = rho * l_weights + (1 - rho) * (-2 * loss)\n",
    "        u_biases = rho * l_biases + (1 - rho) * (-2 * loss)\n",
    "\n",
    "        m_weights = momentum * m_weights + (1 - momentum) * loss\n",
    "        m_biases = momentum * m_biases + (1 - momentum) * loss\n",
    "        \n",
    "        weights -= (1/np.sqrt(u_weights + 1e-6) * 1 / np.sqrt(m_weights + 1e-6)) * learning_rate\n",
    "        biases -= (1/np.sqrt(u_biases + 1e-6) * 1 / np.sqrt(m_boases + 1e-6)) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a475cd",
   "metadata": {
    "papermill": {
     "duration": 0.023948,
     "end_time": "2023-04-29T08:16:09.837986",
     "exception": false,
     "start_time": "2023-04-29T08:16:09.814038",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.1.4 | Epsilon\n",
    "\n",
    "In RMSprop, epsilon is a small constant used to prevent division by zero and improve numerical stability during the calculation of the adaptive learning rate. It is added to the denominator of the weight update equation to ensure that the divisor is always positive. Epsilon is typically set to a small value, such as 1e-8, and has a minimal impact on the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2c0ca8eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:09.888057Z",
     "iopub.status.busy": "2023-04-29T08:16:09.887507Z",
     "iopub.status.idle": "2023-04-29T08:16:09.897835Z",
     "shell.execute_reply": "2023-04-29T08:16:09.896467Z"
    },
    "papermill": {
     "duration": 0.038019,
     "end_time": "2023-04-29T08:16:09.900161",
     "exception": false,
     "start_time": "2023-04-29T08:16:09.862142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rms_prop(X , y , learning_rate = 0.01 , rho = 0.9 , momentum = 0 , epsilon = 1e-7):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "\n",
    "    u_weights = 0\n",
    "    u_biases = 0\n",
    "\n",
    "    predic = []\n",
    "\n",
    "    for epochs in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        u_weights = rho * l_weights + (1 - rho) * (-2 * loss)\n",
    "        u_biases = rho * l_biases + (1 - rho) * (-2 * loss)\n",
    "\n",
    "        m_weights = momentum * m_weights + (1 - momentum) * loss\n",
    "        m_biases = momentum * m_biases + (1 - momentum) * loss\n",
    "        \n",
    "        weights -= (1/np.sqrt(u_weights + epsilon) * 1 / np.sqrt(m_weights + epsilon)) * learning_rate\n",
    "        biases -= (1/np.sqrt(u_biases + epsilon) * 1 / np.sqrt(m_boases + epsilon)) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c15a4a",
   "metadata": {
    "papermill": {
     "duration": 0.024228,
     "end_time": "2023-04-29T08:16:09.949004",
     "exception": false,
     "start_time": "2023-04-29T08:16:09.924776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.1.5 | Clipnorm\n",
    "The `clipnorm` parameter in `RMSprop` is used to prevent the gradient from becoming too large during training, which can cause instability or divergence. When clipnorm is set, the gradient is clipped to a maximum norm value, effectively scaling the gradient if its norm exceeds the specified value. This helps to ensure that the updates to the weights are not too large and that the optimization process remains stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "28a11179",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:10.001022Z",
     "iopub.status.busy": "2023-04-29T08:16:10.000566Z",
     "iopub.status.idle": "2023-04-29T08:16:10.011448Z",
     "shell.execute_reply": "2023-04-29T08:16:10.010612Z"
    },
    "papermill": {
     "duration": 0.039338,
     "end_time": "2023-04-29T08:16:10.013663",
     "exception": false,
     "start_time": "2023-04-29T08:16:09.974325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rms_prop(X , y , learning_rate = 0.01 , rho = 0.9 , momentum = 0 , epsilon = 1e-7 , clip_norm = None):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "\n",
    "    u_weights = 0\n",
    "    u_biases = 0\n",
    "\n",
    "    predic = []\n",
    "\n",
    "    for epochs in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        u_weights = rho * l_weights + (1 - rho) * (-2 * loss)\n",
    "        u_biases = rho * l_biases + (1 - rho) * (-2 * loss)\n",
    "\n",
    "        m_weights = momentum * m_weights + (1 - momentum) * loss\n",
    "        m_biases = momentum * m_biases + (1 - momentum) * loss\n",
    "\n",
    "        if clip_norm != None:\n",
    "            \n",
    "            weights = np.clip(weights , weights , clip_norm)\n",
    "            biases = np.clip(biases , biases , clip_norm)\n",
    "            \n",
    "        weights -= (1/np.sqrt(u_weights + epsilon) * 1 / np.sqrt(m_weights + epsilon)) * learning_rate\n",
    "        biases -= (1/np.sqrt(u_biases + epsilon) * 1 / np.sqrt(m_boases + epsilon)) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0986d5dc",
   "metadata": {
    "papermill": {
     "duration": 0.024193,
     "end_time": "2023-04-29T08:16:10.061812",
     "exception": false,
     "start_time": "2023-04-29T08:16:10.037619",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.1.6 | Clip Value\n",
    "\n",
    "If the gradient is larger than the clipvalue threshold, it will be clipped to that value. If the gradient is smaller than the negative clipvalue threshold, it will be clipped to that negative value. This ensures that the gradient remains within a certain range, which can prevent numerical instability and improve convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba7611fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:10.112375Z",
     "iopub.status.busy": "2023-04-29T08:16:10.111956Z",
     "iopub.status.idle": "2023-04-29T08:16:10.123391Z",
     "shell.execute_reply": "2023-04-29T08:16:10.122541Z"
    },
    "papermill": {
     "duration": 0.039396,
     "end_time": "2023-04-29T08:16:10.125437",
     "exception": false,
     "start_time": "2023-04-29T08:16:10.086041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rms_prop(X , y , learning_rate = 0.01 , rho = 0.9 , momentum = 0 , epsilon = 1e-7 , clip_norm = None , clip_value = None):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "\n",
    "    u_weights = 0\n",
    "    u_biases = 0\n",
    "\n",
    "    predic = []\n",
    "\n",
    "    for epochs in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        u_weights = rho * l_weights + (1 - rho) * (-2 * loss)\n",
    "        u_biases = rho * l_biases + (1 - rho) * (-2 * loss)\n",
    "\n",
    "        m_weights = momentum * m_weights + (1 - momentum) * loss\n",
    "        m_biases = momentum * m_biases + (1 - momentum) * loss\n",
    "\n",
    "        if clip_norm != None:\n",
    "\n",
    "            weights = np.clip(weights , weights , clip_norm)\n",
    "            biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "        if clip_value != None:\n",
    "            \n",
    "            weights = np.clip(m_weigts , m_weights , clip_value)\n",
    "            biases = np.clip(m_biases , m_biases , clip_value)\n",
    "            \n",
    "        weights -= (1/np.sqrt(u_weights + epsilon) * 1 / np.sqrt(m_weights + epsilon)) * learning_rate\n",
    "        biases -= (1/np.sqrt(u_biases + epsilon) * 1 / np.sqrt(m_boases + epsilon)) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c254360a",
   "metadata": {
    "papermill": {
     "duration": 0.024424,
     "end_time": "2023-04-29T08:16:10.174099",
     "exception": false,
     "start_time": "2023-04-29T08:16:10.149675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.1.7 EMA\n",
    "\n",
    "Wether to use the estimated momentum average or not. The major use of this function is when `ema_momentum` is given "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e49b8bac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:10.224455Z",
     "iopub.status.busy": "2023-04-29T08:16:10.224050Z",
     "iopub.status.idle": "2023-04-29T08:16:10.235904Z",
     "shell.execute_reply": "2023-04-29T08:16:10.234701Z"
    },
    "papermill": {
     "duration": 0.039729,
     "end_time": "2023-04-29T08:16:10.238260",
     "exception": false,
     "start_time": "2023-04-29T08:16:10.198531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rms_prop(X , y , learning_rate = 0.01 , rho = 0.9 , momentum = 0 , epsilon = 1e-7 , clip_norm = None , clip_value = None , ema = False):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "\n",
    "    u_weights = 0\n",
    "    u_biases = 0\n",
    "\n",
    "    predic = []\n",
    "\n",
    "    for epochs in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        u_weights = rho * l_weights + (1 - rho) * (-2 * loss)\n",
    "        u_biases = rho * l_biases + (1 - rho) * (-2 * loss)\n",
    "\n",
    "        m_weights = momentum * m_weights + (1 - momentum) * loss\n",
    "        m_biases = momentum * m_biases + (1 - momentum) * loss\n",
    "        \n",
    "        if ema:pass\n",
    "\n",
    "        if clip_norm != None:\n",
    "\n",
    "            weights = np.clip(weights , weights , clip_norm)\n",
    "            biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "        if clip_value != None:\n",
    "            \n",
    "            weights = np.clip(m_weigts , m_weights , clip_value)\n",
    "            biases = np.clip(m_biases , m_biases , clip_value)\n",
    "            \n",
    "        weights -= (1/np.sqrt(u_weights + epsilon) * 1 / np.sqrt(m_weights + epsilon)) * learning_rate\n",
    "        biases -= (1/np.sqrt(u_biases + epsilon) * 1 / np.sqrt(m_boases + epsilon)) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c97b18",
   "metadata": {
    "papermill": {
     "duration": 0.024017,
     "end_time": "2023-04-29T08:16:10.286825",
     "exception": false,
     "start_time": "2023-04-29T08:16:10.262808",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.1.8 | EMA Momentum\n",
    "\n",
    "In RMSprop, the exponential moving average (EMA) of the gradient with momentum is used to improve convergence and prevent oscillations during training. The EMA of the gradient with momentum is updated at each iteration and incorporates information about the previous gradients and the current gradient to produce a more stable and consistent update direction. This helps to smooth out the noise in the gradient estimates and helps the optimizer to move more directly towards the minimum of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "41d6ecd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:10.337138Z",
     "iopub.status.busy": "2023-04-29T08:16:10.336700Z",
     "iopub.status.idle": "2023-04-29T08:16:10.348978Z",
     "shell.execute_reply": "2023-04-29T08:16:10.347913Z"
    },
    "papermill": {
     "duration": 0.040369,
     "end_time": "2023-04-29T08:16:10.351329",
     "exception": false,
     "start_time": "2023-04-29T08:16:10.310960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rms_prop(X , y , learning_rate = 0.01 , rho = 0.9 , momentum = 0 , epsilon = 1e-7 , clip_norm = None , clip_value = None , ema = False , ema_momentum = 0):\n",
    "    \n",
    "    weights = abs(np.random.randn(X.shape[1]))\n",
    "    biases = abs(np.random.randn(1))\n",
    "\n",
    "    m_weights = 0\n",
    "    m_biases = 0\n",
    "\n",
    "    u_weights = 0\n",
    "    u_biases = 0\n",
    "\n",
    "    predic = []\n",
    "\n",
    "    for epochs in range(100):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        u_weights = rho * l_weights + (1 - rho) * (-2 * loss)\n",
    "        u_biases = rho * l_biases + (1 - rho) * (-2 * loss)\n",
    "\n",
    "        m_weights = momentum * m_weights + (1 - momentum) * loss\n",
    "        m_biases = momentum * m_biases + (1 - momentum) * loss\n",
    "        \n",
    "        if ema:\n",
    "            \n",
    "            m_weights = ema_momentum * m_weights + (1 - ema_momentum) * -2 * loss\n",
    "            m_biases = ema_momentum * m_biases + (1 - ema_momentum) * -2 * loss\n",
    "\n",
    "        if clip_norm != None:\n",
    "\n",
    "            weights = np.clip(weights , weights , clip_norm)\n",
    "            biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "        if clip_value != None:\n",
    "            \n",
    "            weights = np.clip(m_weigts , m_weights , clip_value)\n",
    "            biases = np.clip(m_biases , m_biases , clip_value)\n",
    "            \n",
    "        weights -= (1/np.sqrt(u_weights + epsilon) * 1 / np.sqrt(m_weights + epsilon)) * learning_rate\n",
    "        biases -= (1/np.sqrt(u_biases + epsilon) * 1 / np.sqrt(m_boases + epsilon)) * learning_rate\n",
    "\n",
    "    return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d70c5ae",
   "metadata": {
    "papermill": {
     "duration": 0.024036,
     "end_time": "2023-04-29T08:16:10.400165",
     "exception": false,
     "start_time": "2023-04-29T08:16:10.376129",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.2 | Methods\n",
    "\n",
    "Now to add more functionalities, we will add different methods to our alogirthm, You can acces the list of methods we are going to use from the same link before we used for `Functionalities`. \n",
    "* `build` - Initialize optimizer variables. SGD optimizer has one variable momentums, only set if self.momentum is not 0.\n",
    "* * `weights` - Weights of the Optimizer\n",
    "* * `biases` - biases of the Optimizer\n",
    "* `compute_gradients` - Compute gradients of loss on trainable variables.\n",
    "* `minimize` - Minimize loss by updating `var_list`.\n",
    "* `update_step` - Update step given gradient and the associated model variable.\n",
    "\n",
    "**Note** - we will stop the function from being usable at some places\n",
    "\n",
    "# 3.2.1 | Build\n",
    "This method initializes all the values in the `rms_prop`, you can understand this as a constructor, but not perfectly as the one. \n",
    "\n",
    "We will give the user a functionality, that they can intialize there own variables. At the starting the `weights` and `baises` are `None`. If the user passes their own, the function first checks that they are or correct shape and if True assigns the values, else initialize random values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f089a417",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:10.451551Z",
     "iopub.status.busy": "2023-04-29T08:16:10.450787Z",
     "iopub.status.idle": "2023-04-29T08:16:10.462698Z",
     "shell.execute_reply": "2023-04-29T08:16:10.461878Z"
    },
    "papermill": {
     "duration": 0.040071,
     "end_time": "2023-04-29T08:16:10.464774",
     "exception": false,
     "start_time": "2023-04-29T08:16:10.424703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class rms_prop:\n",
    "    def __init__(X , y , \n",
    "                 learning_rate = 0.01 , rho = 0.9 , \n",
    "                 momentum = 0 , epsilon = 1e-7 , \n",
    "                 clip_norm = None , clip_value = None , \n",
    "                 ema = False , ema_momentum = 0):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.learning_rate = learning_rate\n",
    "        self.rho = rho\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.clip_norm = clip-norm\n",
    "        self.ema = ema\n",
    "        self.ema_momentum = ema_momentum\n",
    "\n",
    "    def build(self , weights = None , biases = None):\n",
    "        \n",
    "        if weights == None:\n",
    "            \n",
    "            self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if weights.shape[1] == self.X.shape[1]:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match. Using random variables\")\n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "\n",
    "        if biases == None:\n",
    "            \n",
    "            self.biases = abs(np.random.randn(1))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if baises.shape[1] == 1:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match. Using random variables\")\n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "\n",
    "        self.m_weights = 0\n",
    "        self.m_biases = 0\n",
    "\n",
    "        self.u_weights = 0\n",
    "        self.u_biases = 0\n",
    "\n",
    "        self.predic = []\n",
    "\n",
    "    # for epochs in range(100):\n",
    "\n",
    "    #     pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "    #     loss = np.sum(pred - y)\n",
    "    #     losses.append(loss)\n",
    "\n",
    "    #     u_weights = rho * l_weights + (1 - rho) * (-2 * loss)\n",
    "    #     u_biases = rho * l_biases + (1 - rho) * (-2 * loss)\n",
    "\n",
    "    #     m_weights = momentum * m_weights + (1 - momentum) * loss\n",
    "    #     m_biases = momentum * m_biases + (1 - momentum) * loss\n",
    "        \n",
    "    #     if ema:\n",
    "            \n",
    "    #         m_weights = ema_momentum * m_weights + (1 - ema_momentum) * -2 * loss\n",
    "    #         m_biases = ema_momentum * m_biases + (1 - ema_momentum) * -2 * loss\n",
    "\n",
    "    #     if clip_norm != None:\n",
    "\n",
    "    #         weights = np.clip(weights , weights , clip_norm)\n",
    "    #         biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "    #     if clip_value != None:\n",
    "            \n",
    "    #         weights = np.clip(m_weigts , m_weights , clip_value)\n",
    "    #         biases = np.clip(m_biases , m_biases , clip_value)\n",
    "            \n",
    "    #     weights -= (1/np.sqrt(u_weights + epsilon) * 1 / np.sqrt(m_weights + epsilon)) * learning_rate\n",
    "    #     biases -= (1/np.sqrt(u_biases + epsilon) * 1 / np.sqrt(m_boases + epsilon)) * learning_rate\n",
    "\n",
    "    # return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61d4632",
   "metadata": {
    "papermill": {
     "duration": 0.023781,
     "end_time": "2023-04-29T08:16:10.513563",
     "exception": false,
     "start_time": "2023-04-29T08:16:10.489782",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.2.2 | Compute Gradients\n",
    "\n",
    "Compuute the loss function and gradients for the losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "227d0145",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:10.563858Z",
     "iopub.status.busy": "2023-04-29T08:16:10.563123Z",
     "iopub.status.idle": "2023-04-29T08:16:10.575800Z",
     "shell.execute_reply": "2023-04-29T08:16:10.574949Z"
    },
    "papermill": {
     "duration": 0.040432,
     "end_time": "2023-04-29T08:16:10.578002",
     "exception": false,
     "start_time": "2023-04-29T08:16:10.537570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class rms_prop:\n",
    "    def __init__(X , y , \n",
    "                 learning_rate = 0.01 , rho = 0.9 , \n",
    "                 momentum = 0 , epsilon = 1e-7 , \n",
    "                 clip_norm = None , clip_value = None , \n",
    "                 ema = False , ema_momentum = 0):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.learning_rate = learning_rate\n",
    "        self.rho = rho\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.clip_norm = clip-norm\n",
    "        self.ema = ema\n",
    "        self.ema_momentum = ema_momentum\n",
    "\n",
    "    def build(self , weights = None , biases = None):\n",
    "        \n",
    "        if weights == None:\n",
    "            \n",
    "            self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if weights.shape[1] == self.X.shape[1]:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match. Using random variables\")\n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "\n",
    "        if biases == None:\n",
    "            \n",
    "            self.biases = abs(np.random.randn(1))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if baises.shape[1] == 1:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match. Using random variables\")\n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "\n",
    "        self.m_weights = 0\n",
    "        self.m_biases = 0\n",
    "\n",
    "        self.u_weights = 0\n",
    "        self.u_biases = 0\n",
    "\n",
    "        self.predic = []\n",
    "\n",
    "    def compute_gradients(self , weights , biases):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        \n",
    "        yield loss\n",
    "        \n",
    "        losses.append(loss)\n",
    "\n",
    "    #     u_weights = rho * l_weights + (1 - rho) * (-2 * loss)\n",
    "    #     u_biases = rho * l_biases + (1 - rho) * (-2 * loss)\n",
    "\n",
    "    #     m_weights = momentum * m_weights + (1 - momentum) * loss\n",
    "    #     m_biases = momentum * m_biases + (1 - momentum) * loss\n",
    "        \n",
    "    #     if ema:\n",
    "            \n",
    "    #         m_weights = ema_momentum * m_weights + (1 - ema_momentum) * -2 * loss\n",
    "    #         m_biases = ema_momentum * m_biases + (1 - ema_momentum) * -2 * loss\n",
    "\n",
    "    #     if clip_norm != None:\n",
    "\n",
    "    #         weights = np.clip(weights , weights , clip_norm)\n",
    "    #         biases = np.clip(biases , biases , clip_norm)\n",
    "\n",
    "    #     if clip_value != None:\n",
    "            \n",
    "    #         weights = np.clip(m_weigts , m_weights , clip_value)\n",
    "    #         biases = np.clip(m_biases , m_biases , clip_value)\n",
    "            \n",
    "    #     weights -= (1/np.sqrt(u_weights + epsilon) * 1 / np.sqrt(m_weights + epsilon)) * learning_rate\n",
    "    #     biases -= (1/np.sqrt(u_biases + epsilon) * 1 / np.sqrt(m_boases + epsilon)) * learning_rate\n",
    "\n",
    "    # return weights , biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efd875d",
   "metadata": {
    "papermill": {
     "duration": 0.023904,
     "end_time": "2023-04-29T08:16:10.626059",
     "exception": false,
     "start_time": "2023-04-29T08:16:10.602155",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.2.3 | Minimize\n",
    "Minimze the loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87921e10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:10.677016Z",
     "iopub.status.busy": "2023-04-29T08:16:10.676295Z",
     "iopub.status.idle": "2023-04-29T08:16:10.692575Z",
     "shell.execute_reply": "2023-04-29T08:16:10.691681Z"
    },
    "papermill": {
     "duration": 0.044974,
     "end_time": "2023-04-29T08:16:10.695187",
     "exception": false,
     "start_time": "2023-04-29T08:16:10.650213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class rms_prop:\n",
    "    def __init__(X , y , \n",
    "                 learning_rate = 0.01 , rho = 0.9 , \n",
    "                 momentum = 0 , epsilon = 1e-7 , \n",
    "                 clip_norm = None , clip_value = None , \n",
    "                 ema = False , ema_momentum = 0):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.learning_rate = learning_rate\n",
    "        self.rho = rho\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.clip_norm = clip-norm\n",
    "        self.ema = ema\n",
    "        self.ema_momentum = ema_momentum\n",
    "\n",
    "    def build(self , weights = None , biases = None):\n",
    "        \n",
    "        if weights == None:\n",
    "            \n",
    "            self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if weights.shape[1] == self.X.shape[1]:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match. Using random variables\")\n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "\n",
    "        if biases == None:\n",
    "            \n",
    "            self.biases = abs(np.random.randn(1))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if baises.shape[1] == 1:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match. Using random variables\")\n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "\n",
    "        self.m_weights = 0\n",
    "        self.m_biases = 0\n",
    "\n",
    "        self.u_weights = 0\n",
    "        self.u_biases = 0\n",
    "\n",
    "        self.predic = []\n",
    "\n",
    "    def compute_gradients(self , weights , biases):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        \n",
    "        yield loss\n",
    "        \n",
    "        losses.append(loss)\n",
    "\n",
    "    #     u_weights = rho * l_weights + (1 - rho) * (-2 * loss)\n",
    "    #     u_biases = rho * l_biases + (1 - rho) * (-2 * loss)\n",
    "\n",
    "    #     m_weights = momentum * m_weights + (1 - momentum) * loss\n",
    "    #     m_biases = momentum * m_biases + (1 - momentum) * loss\n",
    "        \n",
    "    #     if ema:\n",
    "            \n",
    "    #         m_weights = ema_momentum * m_weights + (1 - ema_momentum) * -2 * loss\n",
    "    #         m_biases = ema_momentum * m_biases + (1 - ema_momentum) * -2 * loss\n",
    "    def minimize(self):\n",
    "        \n",
    "        if self.clip_norm != None:\n",
    "\n",
    "            self.weights = np.clip(self.weights , self.weights , self.clip_norm)\n",
    "            self.biases = np.clip(self.biases , self.biases , self.clip_norm)\n",
    "\n",
    "        if self.clip_value != None:\n",
    "            \n",
    "            self.weights = np.clip(self.m_weights , self.m_weights , self.clip_value)\n",
    "            self.biases = np.clip(self.m_biases , self.m_biases , self.clip_value)\n",
    "            \n",
    "        self.weights -= (1/np.sqrt(self.u_weights + self.epsilon) * 1 / np.sqrt(self.m_weights + self.epsilon)) * learning_rate\n",
    "        self.biases -= (1/np.sqrt(self.u_biases + self.epsilon) * 1 / np.sqrt(self.m_biases + self.epsilon)) * learning_rate\n",
    "\n",
    "        return self.weights , self.biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820e3dd2",
   "metadata": {
    "papermill": {
     "duration": 0.024355,
     "end_time": "2023-04-29T08:16:10.743823",
     "exception": false,
     "start_time": "2023-04-29T08:16:10.719468",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.2.4 | Update_Step\n",
    "Update the values accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fcf41cef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:10.794793Z",
     "iopub.status.busy": "2023-04-29T08:16:10.794019Z",
     "iopub.status.idle": "2023-04-29T08:16:10.813511Z",
     "shell.execute_reply": "2023-04-29T08:16:10.812184Z"
    },
    "papermill": {
     "duration": 0.048555,
     "end_time": "2023-04-29T08:16:10.816621",
     "exception": false,
     "start_time": "2023-04-29T08:16:10.768066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class rms_prop:\n",
    "    def __init__(X , y , \n",
    "                 learning_rate = 0.01 , rho = 0.9 , \n",
    "                 momentum = 0 , epsilon = 1e-7 , \n",
    "                 clip_norm = None , clip_value = None , \n",
    "                 ema = False , ema_momentum = 0):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.learning_rate = learning_rate\n",
    "        self.rho = rho\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.clip_norm = clip-norm\n",
    "        self.ema = ema\n",
    "        self.ema_momentum = ema_momentum\n",
    "\n",
    "    def build(self , weights = None , biases = None):\n",
    "        \n",
    "        if weights == None:\n",
    "            \n",
    "            self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if weights.shape[1] == self.X.shape[1]:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match. Using random variables\")\n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "\n",
    "        if biases == None:\n",
    "            \n",
    "            self.biases = abs(np.random.randn(1))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if baises.shape[1] == 1:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match. Using random variables\")\n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "\n",
    "        self.m_weights = 0\n",
    "        self.m_biases = 0\n",
    "\n",
    "        self.u_weights = 0\n",
    "        self.u_biases = 0\n",
    "\n",
    "        self.predic = []\n",
    "\n",
    "    def compute_gradients(self , weights , biases):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        \n",
    "        yield loss\n",
    "        \n",
    "        losses.append(loss)\n",
    "    def update_step(self , loss):\n",
    "        \n",
    "        self.u_weights = self.rho * self.u_weights + (1 - self.rho) * (-2 * loss)\n",
    "        self.u_biases = self.rho * self.u_biases + (1 - self.rho) * (-2 * loss)\n",
    "\n",
    "        self.m_weights = self.momentum * self.m_weights + (1 - self.momentum) * loss\n",
    "        self.m_biases = self.momentum * self.m_biases + (1 - self.momentum) * loss\n",
    "        \n",
    "        if self.ema:\n",
    "            \n",
    "            self.m_weights = self.ema_momentum * self.m_weights + (1 - self.ema_momentum) * -2 * loss\n",
    "            self.m_biases = self.ema_momentum * self.m_biases + (1 - self.ema_momentum) * -2 * loss\n",
    "    \n",
    "    def minimize(self):\n",
    "        \n",
    "        if self.clip_norm != None:\n",
    "\n",
    "            self.weights = np.clip(self.weights , self.weights , self.clip_norm)\n",
    "            self.biases = np.clip(self.biases , self.biases , self.clip_norm)\n",
    "\n",
    "        if self.clip_value != None:\n",
    "            \n",
    "            self.weights = np.clip(m_weigts , self.m_weights , self.clip_value)\n",
    "            self.biases = np.clip(self.m_biases , self.m_biases , self.clip_value)\n",
    "            \n",
    "        self.weights -= (1/np.sqrt(self.u_weights + self.epsilon) * 1 / np.sqrt(self.m_weights + self.epsilon)) * self.learning_rate\n",
    "        self.biases -= (1/np.sqrt(self.u_biases + self.epsilon) * 1 / np.sqrt(self.m_biases + self.epsilon)) * self.learning_rate\n",
    "\n",
    "        return self.weights , self.biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2cb715",
   "metadata": {
    "papermill": {
     "duration": 0.025639,
     "end_time": "2023-04-29T08:16:10.867179",
     "exception": false,
     "start_time": "2023-04-29T08:16:10.841540",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.2 | RMSProp Final Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b75a568e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T08:16:10.919784Z",
     "iopub.status.busy": "2023-04-29T08:16:10.918969Z",
     "iopub.status.idle": "2023-04-29T08:16:10.938605Z",
     "shell.execute_reply": "2023-04-29T08:16:10.937213Z"
    },
    "papermill": {
     "duration": 0.048954,
     "end_time": "2023-04-29T08:16:10.941406",
     "exception": false,
     "start_time": "2023-04-29T08:16:10.892452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class rms_prop:\n",
    "    def __init__(X , y , \n",
    "                 learning_rate = 0.01 , rho = 0.9 , \n",
    "                 momentum = 0 , epsilon = 1e-7 , \n",
    "                 clip_norm = None , clip_value = None , \n",
    "                 ema = False , ema_momentum = 0):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.learning_rate = learning_rate\n",
    "        self.rho = rho\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.clip_norm = clip-norm\n",
    "        self.ema = ema\n",
    "        self.ema_momentum = ema_momentum\n",
    "\n",
    "    def build(self , weights = None , biases = None):\n",
    "        \n",
    "        if weights == None:\n",
    "            \n",
    "            self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if weights.shape[1] == self.X.shape[1]:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match. Using random variables\")\n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "\n",
    "        if biases == None:\n",
    "            \n",
    "            self.biases = abs(np.random.randn(1))\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            if baises.shape[1] == 1:\n",
    "                \n",
    "                self.weights = weights\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                raise UserWarning(\"Shapes do not match. Using random variables\")\n",
    "                self.weights = abs(np.random.randn(self.X.shape[1]))\n",
    "\n",
    "        self.m_weights = 0\n",
    "        self.m_biases = 0\n",
    "\n",
    "        self.u_weights = 0\n",
    "        self.u_biases = 0\n",
    "\n",
    "        self.predic = []\n",
    "\n",
    "    def compute_gradients(self , weights , biases):\n",
    "\n",
    "        pred = np.sum((weights * X).T) + biases\n",
    "        \n",
    "        loss = np.sum(pred - y)\n",
    "        \n",
    "        yield loss\n",
    "        \n",
    "        losses.append(loss)\n",
    "    def update_step(self , loss):\n",
    "        \n",
    "        self.u_weights = self.rho * self.u_weights + (1 - self.rho) * (-2 * loss)\n",
    "        self.u_biases = self.rho * self.u_biases + (1 - self.rho) * (-2 * loss)\n",
    "\n",
    "        self.m_weights = self.momentum * self.m_weights + (1 - self.momentum) * loss\n",
    "        self.m_biases = self.momentum * self.m_biases + (1 - self.momentum) * loss\n",
    "        \n",
    "        if self.ema:\n",
    "            \n",
    "            self.m_weights = self.ema_momentum * self.m_weights + (1 - self.ema_momentum) * -2 * loss\n",
    "            self.m_biases = self.ema_momentum * self.m_biases + (1 - self.ema_momentum) * -2 * loss\n",
    "    \n",
    "    def minimize(self):\n",
    "        \n",
    "        if self.clip_norm != None:\n",
    "\n",
    "            self.weights = np.clip(self.weights , self.weights , self.clip_norm)\n",
    "            self.biases = np.clip(self.biases , self.biases , self.clip_norm)\n",
    "\n",
    "        if self.clip_value != None:\n",
    "            \n",
    "            self.weights = np.clip(m_weigts , self.m_weights , self.clip_value)\n",
    "            self.biases = np.clip(self.m_biases , self.m_biases , self.clip_value)\n",
    "            \n",
    "        self.weights -= (1/np.sqrt(self.u_weights + self.epsilon) * 1 / np.sqrt(self.m_weights + self.epsilon)) * self.learning_rate\n",
    "        self.biases -= (1/np.sqrt(self.u_biases + self.epsilon) * 1 / np.sqrt(self.m_biases + self.epsilon)) * self.learning_rate\n",
    "\n",
    "        return self.weights , self.biases , losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77783297",
   "metadata": {
    "papermill": {
     "duration": 0.024143,
     "end_time": "2023-04-29T08:16:10.990350",
     "exception": false,
     "start_time": "2023-04-29T08:16:10.966207",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**THIS IS NOT THE FULL IMPLEMENTATION, IT STILL LACKS MANY FUNCTIONALITIES AND IS VULENRABLE TO MANY EDGE CASES, WE WILL IMPROVE THIS IN THE UPCOMING VERSIONS**\n",
    "\n",
    "**PLEASE COMMENT DOWN IF I DID ANY MISTAKES, OR IF CAN MAKE THIS MORE CONNECTED TO THE GROUND, OR SUGGESTIONS. YOUR ASSISTS ARE HIGHLY APPRECIABLE**\n",
    "\n",
    "**THATS IT FOR TODAY GUYS**\n",
    "\n",
    "**HOPE YOU UNDERSTOOD AND LIKED MY WORK**\n",
    "\n",
    "**DONT FORGET TO MAKE AN UPVOTE :)**\n",
    "\n",
    "**PEACE OUT !!!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18.330603,
   "end_time": "2023-04-29T08:16:11.737464",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-29T08:15:53.406861",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
