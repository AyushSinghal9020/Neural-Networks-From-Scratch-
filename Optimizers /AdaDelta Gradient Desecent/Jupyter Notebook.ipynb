{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AdaDelta Gradient Descent \n",
        "\n",
        "AdaDelta is an optimization algorithm that is designed to overcome some of the limitations of traditional gradient descent methods, such as the need for manual tuning of learning rates and difficulties in optimizing in high-dimensional spaces.\n",
        "\n",
        "In AdaDelta, the learning rate is adaptively adjusted during training based on the historical information of the gradients, which allows for more efficient convergence and better handling of sparse gradients.\n",
        "\n",
        "The algorithm also includes two decaying averages, one for the squared gradients and one for the squared parameter updates, which help to normalize the learning rate and prevent it from becoming too large or too small.\n",
        "\n",
        "AdaDelta has been shown to work well in a variety of settings, including deep neural networks and natural language processing tasks. It is often used as an alternative to other optimization algorithms such as Adam or RMSProp, and can be particularly effective when working with large datasets or when dealing with sparse gradients.\n",
        "\n",
        "$$learning_-rate_{new} = \\frac {learning_-rate_{old}}{\\sqrt {k + e}}$$\n",
        "$$k_{new} = rk_{old} + (1-r)(gradient_{old}^2)$$"
      ],
      "metadata": {
        "id": "e3LHV3l1UDPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np "
      ],
      "metadata": {
        "id": "Eb3bTUGyVscx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our motive it to create this function somehow $$learning_-rate_{new} = \\frac {learning_-rate_{old}}{\\sqrt {k + e}}$$\n",
        "But what are these values \n",
        "* `k` is a parameter that takes the account of the gradients, We first initialize it and then it is calculated acorrding to the formula \n",
        "$$k_{new} = rk_{old} + (1-r)(gradient_{old}^2)$$\n",
        "* `e` is a parameter that prevents divison by zero\n",
        "\n",
        "So now we just need to run a for loop, that stores the grdient values"
      ],
      "metadata": {
        "id": "i970zXatWIRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets consider the most basic dataset, that has one column/feature and one target, so the eqaution of a line that tries to predict the values will be $line = \\beta_1x_1 + \\beta_0$ \n",
        " there will be one value of, as we can see there are two hyperparameters, and so we need to predict two random values"
      ],
      "metadata": {
        "id": "6hickja0XmKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.randn(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSc_ZQbQX8ek",
        "outputId": "9a987490-f07f-47f3-b2d5-486b41365aeb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.8133194,  0.4189067])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These values are not consistent, and we know that for finding one graddient, we intialize some values randomly, it is a good practice to set the values, between $(-1 , 1)$"
      ],
      "metadata": {
        "id": "rXac3sz3YAW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way we can do this is to divide these values by 10 at the time of intialization"
      ],
      "metadata": {
        "id": "XxXpgubzYJEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.randn(2) * 0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzDY-FdqYEQE",
        "outputId": "3f80221d-96d4-4a88-9a8b-3bf19f6dc3ce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.00346916, -0.07717803])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = np.random.randn(2) * 0.1"
      ],
      "metadata": {
        "id": "oTn6RfB5YNlc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets caculate the gradient of these values, gradient is just the derivatives of these functions, tese values are actually in the form of \n",
        "$x^2$ and there derivates will be \n",
        ", $2x$ lets do these"
      ],
      "metadata": {
        "id": "GJuBWNdJYQHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradient = [params[0] * 2 , params[1] * 2]"
      ],
      "metadata": {
        "id": "NG4WJOaxYUNW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have created our adagrad, lets put it in function for better usage"
      ],
      "metadata": {
        "id": "K6kuB-YLYW87"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KPhLd-zUOhze"
      },
      "outputs": [],
      "source": [
        "def rms_prop(columns , rho = 0.999 , lr = 0.01 , epochs = 100 , epsilon = 1e-7 , r = 0.95 , k = 0.001):\n",
        "    \n",
        "    params = np.random.ran(len(columns)) * 0.1\n",
        "    \n",
        "    gradient = [params[:len(columns) - 2] * 2 , params[-1] * 2]\n",
        "    \n",
        "    step_size = np.random.rand(len(columns)) * 0.1\n",
        "\n",
        "    gradient_record = []\n",
        "    k_record = []\n",
        "\n",
        "    for i in range(len(gradient)):\n",
        "        \n",
        "        gradient[i] = (gradient[i]) + ((gradient[i] ** 2) * lr) \n",
        "        \n",
        "        k = r*k + (1- r)*(gradient[i]**2)\n",
        "\n",
        "        lr = lr/np.sqrt(k + epsilon)\n",
        "\n",
        "        for i in range(epochs) : \n",
        "    \n",
        "            step_size = step_size * lr\n",
        "\n",
        "    return step_size"
      ]
    }
  ]
}