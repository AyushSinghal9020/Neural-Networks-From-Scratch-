{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "RMS Porp\n",
        "RMSprop (Root Mean Square Propagation) is a popular optimization algorithm used in deep learning to update the parameters of a neural network. It was introduced by Geoffrey Hinton in 2012 as an improvement over the standard stochastic gradient descent (SGD) optimizer.\n",
        "\n",
        "The main idea behind RMSprop is to adjust the learning rate for each weight based on the average of the squared gradients for that weight. The algorithm keeps track of an exponential moving average of the squared gradients, which is then used to normalize the learning rate for each weight. This normalization prevents the learning rate from becoming too small or too large, which can slow down the training process or cause it to diverge.\n",
        "\n",
        "RMSprop is particularly effective in dealing with sparse gradients, which are common in deep neural networks. It has become a popular choice for optimizing neural networks due to its ability to converge faster and produce better results than other optimization algorithms, especially in deep architectures.\n",
        "\n",
        "Overall, RMSprop is a powerful optimization algorithm that can help accelerate the training of deep neural networks and improve their performance.\n",
        "\n",
        "$$E[g^2]_t = \\beta E[g^2]_{t-1} + (1 - \\beta)\\frac {dC}{dw}$$\n",
        "$$w_t = w_{t - 1} - \\frac {lr}{\\sqrt {E[g^2]_t + epsilon}} \\frac {dC}{dw}$$"
      ],
      "metadata": {
        "id": "EL-63L-xE84Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "4OfZUuXAHEi2"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMS Prop at the end is just a optimizing algorithm, that works on the underlying concept of Gradient Descent. As we know gradient descent wants weights and biases for a staring point, lets assume we have two paramters, these two parameters will have $2$ weights and $2$ biases"
      ],
      "metadata": {
        "id": "fhcnvntQFJfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What to remember while weights intialization\n",
        "* From experiments it is found, when weights are intialsed as $0$, they do not change at all \n",
        "* From experiments it is found that weight intialization of random numbers but wiht huge difference is a bad habit \n",
        "* From experimaent it is found that weight intialization of random numbers but with very little difference is a bad habit \n"
      ],
      "metadata": {
        "id": "87KQvR2yFu7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = np.random.rand(2) * 0.1\n",
        "baises = np.random.rand(2) * 0.1"
      ],
      "metadata": {
        "id": "TdUKOhaJGmbg"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKjk4ykKG-nF",
        "outputId": "f1dde99d-0809-410a-ede8-10c5035d7e1c"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.03937234, 0.09868363])"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baises"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LGt6dweG_71",
        "outputId": "f9df06c4-0014-4e2f-d30f-0573604b0044"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.08005411, 0.07454717])"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets store both of these values into another array `params`"
      ],
      "metadata": {
        "id": "Szm07NYjHIMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = np.array([weights , baises])"
      ],
      "metadata": {
        "id": "ppI_090mHOP1"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFfN4hnGHoMO",
        "outputId": "1787dfb3-a0d9-426c-f9ec-9183215f6d71"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.03937234, 0.09868363],\n",
              "       [0.08005411, 0.07454717]])"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we know gradient is nothing but the derivative of the slopes, the slopes are in the form $x^2$, so there derivative will be $2x$ as $$\\frac {d(x^n)}{dx} = nx^{n-1} ===> \\frac {d(x^2)}{dx} = 2x^{2-1} ==> 2x$$"
      ],
      "metadata": {
        "id": "t6ywnVH6HwVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradient = np.array([[params[0] * 2] , [params[1] * 2]])"
      ],
      "metadata": {
        "id": "bdYud2q4IV6T"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradient "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3xW-IplIXtU",
        "outputId": "d57f0ee1-4916-4e15-de71-b117d8c7f956"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.07874469, 0.19736726]],\n",
              "\n",
              "       [[0.16010821, 0.14909435]]])"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets have a look at the formula $$E[g^2]_t = \\beta E[g^2]_{t-1} + (1 - \\beta)\\frac {dC}{dw}$$\n",
        "\n",
        "What is this $\\frac {dC}{dw}$ here ?, It is the weighted moving average\n",
        "\n",
        "Lets first apply this in the formula, for instance lets think that $\\beta$ is not present in the equation \n"
      ],
      "metadata": {
        "id": "JQpWXZxNIgsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "updated_gradient = np.empty(shape = (2,2))\n",
        "\n",
        "for i in range(2):\n",
        "    upda_gradient = np.empty(shape = (2,))\n",
        "    for j in range(2):\n",
        "        up_gradient = gradient[i][0][j] ** 2 + gradient[i][0][j]\n",
        "        upda_gradient = np.hstack([upda_gradient , up_gradient])\n",
        "        upda_gradient = np.delete(upda_gradient , 0  , 0)\n",
        "    updated_gradient = np.vstack([updated_gradient , upda_gradient])\n",
        "    updated_gradient = np.delete(updated_gradient , 0 , 0)"
      ],
      "metadata": {
        "id": "HdihszKTNFtX"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_gradient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1R0TsCcRnED",
        "outputId": "513a5a45-1376-4681-9703-5fad9407fb33"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.08494541, 0.23632109],\n",
              "       [0.18574285, 0.17132347]])"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets now add $\\beta$ to this, but what is beta, it is a hyperparameter constant, which controls the gradient, its value is $(0.9)$"
      ],
      "metadata": {
        "id": "toad9f_TRvEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "beta = 0.9"
      ],
      "metadata": {
        "id": "kK2NVkdjSSTs"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_gradient = beta * gradient[0][0] ** 2 +(1 - beta) * gradient[0][0] "
      ],
      "metadata": {
        "id": "lH9V8GviSMQ7"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_gradient = np.empty(shape = (2,2))\n",
        "\n",
        "for i in range(2):\n",
        "    upda_gradient = np.empty(shape = (2,))\n",
        "    for j in range(2):\n",
        "        up_gradient = beta * gradient[i][0][j] ** 2 + (1 - beta) * gradient[i][0][j]\n",
        "        upda_gradient = np.hstack([upda_gradient , up_gradient])\n",
        "        upda_gradient = np.delete(upda_gradient , 0  , 0)\n",
        "    updated_gradient = np.vstack([updated_gradient , upda_gradient])\n",
        "    updated_gradient = np.delete(updated_gradient , 0 , 0)"
      ],
      "metadata": {
        "id": "Fo2iCuyCSZIE"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_gradient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1ipnYd9SekT",
        "outputId": "853af99e-bca8-4f8f-af6b-79574e7f2102"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.01345512, 0.05479518],\n",
              "       [0.039082  , 0.03491565]])"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to understand the basic concept behind weighted, I found a great explanation [here](https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a), or the expanation below \n",
        "* First, we look at the signs of the last two gradients for the weight.\n",
        "* * If they have the same sign, that means, we’re going in the right direction, and should accelerate it by a small fraction, meaning we should increase the step size multiplicatively(e.g by a factor of 1.2). \n",
        "* * If they’re different, that means we did too large of a step and jumped over a local minima, thus we should decrease the step size multiplicatively(e.g. by a factor of 0.5).\n",
        "* Then, we limit the step size between some two values. These values really depend on your application and dataset, good values that can be for default are 50 and a millionth, which is a good start.\n",
        "* Now we can apply the weight update "
      ],
      "metadata": {
        "id": "W2T2r06DSonT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "updated_gradient = np.empty(shape = (2,2))\n",
        "gradient_rec = []\n",
        "for k in range(0 ,10):\n",
        "    if k == 0 or k == 1:\n",
        "        gradient = gradient\n",
        "    else :\n",
        "        if np.all(np.array(gradient_rec[k-2]) > np.array(gradient_rec[k-1])):\n",
        "            gradient = gradient_rec[k-2]\n",
        "        else :\n",
        "            gradient = gradient_rec[k-1]\n",
        "    for i in range(2):\n",
        "        upda_gradient = np.empty(shape = (2,))\n",
        "        for j in range(2):\n",
        "            up_gradient = beta * gradient[i][j] ** 2 + (1 - beta) * gradient[i][j]\n",
        "            upda_gradient = np.hstack([upda_gradient , up_gradient])\n",
        "            upda_gradient = np.delete(upda_gradient , 0  , 0)\n",
        "        updated_gradient = np.vstack([updated_gradient , upda_gradient])\n",
        "        updated_gradient = np.delete(updated_gradient , 0 , 0)\n",
        "    gradient_rec.append(updated_gradient)"
      ],
      "metadata": {
        "id": "Yx5GERGoU4Gd"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to apply another formula $$w_t = w_{t - 1} - \\frac {lr}{\\sqrt {E[g^2]_t + epsilon}} \\frac {dC}{dw}$$"
      ],
      "metadata": {
        "id": "IJgKaD7yuEYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rms_prop(columns , lr = 0.001 , beta = 0.9 , epochs = 100 , epsilon = 1e-7): \n",
        "    updated_gradient = np.empty(shape = (2,2))\n",
        "    gradient_rec = []\n",
        "    gradient_sum = np.zeros(shape = (len(columns) , 2))\n",
        "    for epochs in range(epochs):\n",
        "        if epochs == 0 or epochs == 1:\n",
        "            gradient = gradient\n",
        "        else :\n",
        "            if np.all(np.array(gradient_rec[epochs-2]) > np.array(gradient_rec[epochs-1])):\n",
        "                gradient = gradient_rec[epochs-2]\n",
        "            else :\n",
        "                gradient = gradient_rec[epochs-1]\n",
        "        for parameters in range(2):\n",
        "            upda_gradient = np.empty(shape = (len(columns),))\n",
        "            for values in range(2):\n",
        "                up_gradient = beta * gradient[parameters][values] ** 2 + (1 - beta) * gradient[parameters][values]\n",
        "                upda_gradient = np.hstack([upda_gradient , up_gradient])\n",
        "                upda_gradient = np.delete(upda_gradient , 0  , 0)\n",
        "            updated_gradient = np.vstack([updated_gradient , upda_gradient])\n",
        "            updated_gradient = np.delete(updated_gradient , 0 , 0)\n",
        "        gradient_rec.append(updated_gradient)\n",
        "\n",
        "        gradient_sum += np.array(gradient)\n",
        "\n",
        "        params = params - np.dot((lr/ np.sqrt(gradient + epsilon)) , gradient_sum)\n",
        "\n",
        "    return params "
      ],
      "metadata": {
        "id": "6MVU1X42uKog"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "c482gmdQFlAj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ugi54YTvFls0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
