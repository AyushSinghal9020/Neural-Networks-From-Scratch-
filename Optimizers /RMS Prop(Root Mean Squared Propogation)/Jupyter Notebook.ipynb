{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RMS Porp \n",
        "\n",
        "RMSprop (Root Mean Square Propagation) is a popular optimization algorithm used in deep learning to update the parameters of a neural network. It was introduced by Geoffrey Hinton in 2012 as an improvement over the standard stochastic gradient descent (SGD) optimizer.\n",
        "\n",
        "The main idea behind RMSprop is to adjust the learning rate for each weight based on the average of the squared gradients for that weight. The algorithm keeps track of an exponential moving average of the squared gradients, which is then used to normalize the learning rate for each weight. This normalization prevents the learning rate from becoming too small or too large, which can slow down the training process or cause it to diverge.\n",
        "\n",
        "RMSprop is particularly effective in dealing with sparse gradients, which are common in deep neural networks. It has become a popular choice for optimizing neural networks due to its ability to converge faster and produce better results than other optimization algorithms, especially in deep architectures.\n",
        "\n",
        "Overall, RMSprop is a powerful optimization algorithm that can help accelerate the training of deep neural networks and improve their performance.\n",
        "$$calculated_-step_-size = \\frac {step_-size}{1e-8 + \\sqrt {s}}$$\n",
        "$$s_{t+1} = ((s_t r_{ho}) + (f^1(x^2_t (1 - r_{ho})))$$\n"
      ],
      "metadata": {
        "id": "keCxU2f35m75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np "
      ],
      "metadata": {
        "id": "rvxl6swi7mU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our motive is to create this function somehow $$calculated_-step_-size = \\frac {step_-size}{1e-8 + \\sqrt {s}}$$ But what are these values, We will slowly create these values and then implememnt them in this function, But first lets create a skeleton for this function "
      ],
      "metadata": {
        "id": "bZwNqsMYSS22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "calculated_step_size = step_sizes / (1e-8 + np.sqrt(s))"
      ],
      "metadata": {
        "id": "YSvLRzO4SOFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets initialize these values as random for test"
      ],
      "metadata": {
        "id": "k9H8TMvGS8-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step_sizes = 1\n",
        "s = 1\n",
        "calculated_step_size = step_sizes / (1e-8 + np.sqrt(s))"
      ],
      "metadata": {
        "id": "62lokWnDTCDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculated_step_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rmc3mnJOTGpk",
        "outputId": "62c9fecf-2042-4b9f-841c-10b8ed649006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999999900000002"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will try to build the values, first comes the `s`, so what is `s`. `s` is basically a value that stores the `sum of all the sqaured partial derivative gradients` we would have taken if the normal gradient worked on a particualr set of data \n",
        "\n",
        "So now we know we need to find the `sum of all the sqaured partial derivative gradients`, for this we first need to find `all squared partial derivative gradients`, or we can just run a for loop and append all the gradients in a list, for this we first need to find a way to get the `partial derivative of one gradient`, and for this we need to find a way how to get `gradient descent `"
      ],
      "metadata": {
        "id": "3bZbEOqgTqWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets consider the most basic dataset, that has one column/feature and one target, so the eqaution of a line that tries to predict the values will be $line = \\beta_{1}x_1 + \\beta_0$ there will be one value of, as we can see there are two hyperparameters, and so we need to predict two random values"
      ],
      "metadata": {
        "id": "hsjdi5FuVGqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.randn(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdT-xGhQWPJs",
        "outputId": "2c7b5678-d52c-41ac-e492-017d2e15900f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.24475101, -0.2307477 ])"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These values are not consistent, and we know that for finding one graddient, we intialize some values randomly, it is a good practice to set the values, between $(-1 , 1)$ "
      ],
      "metadata": {
        "id": "hXkz3dALWRK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way we can do this is to divide these values by $10$ at the time of intialization"
      ],
      "metadata": {
        "id": "8BcGV6BcWb7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.randn(2) * 0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFeS9KzGWmTZ",
        "outputId": "46d76a35-f42d-4397-c90c-7377eb8778c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.02019788,  0.15916401])"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And this is perfect\n",
        "\n",
        "These can also be said to be weights and biases for the initial training process. "
      ],
      "metadata": {
        "id": "S4mm6UhuYmVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = np.random.randn(2) * 0.1"
      ],
      "metadata": {
        "id": "-c44kxVAajcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0TjiIFlanaO",
        "outputId": "8212552f-8cb0-45ca-9414-bd8c151b580e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.07061944,  0.10155017])"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets caculate the gradient of these values, gradient is just the derivatives of these functions, tese values are actually in the form of $x^2$ and there derivates will be $2x$, lets do these "
      ],
      "metadata": {
        "id": "PC1hnU1YbLmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradient = [params[0] * 2 , params[1] * 2]"
      ],
      "metadata": {
        "id": "Dp1v1lEubLWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradient "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rbkz35tWcW4V",
        "outputId": "7bd23f51-760b-417a-d1f0-fec5b2e21f7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.14123887758702727, 0.20310033563751598]"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to add these values into a list and add them all, but wait, we only have $1$ of these values, Lets first think that we only have $1$ of these values, also we only need to add the weights not the biases. So at this point we dont need to do anything of adding. So now we have our `s`, now we need to compute the `step_size`. \n",
        "\n",
        "Step size is the random value from where we start, its the initializing value"
      ],
      "metadata": {
        "id": "8W6QzFYlc08r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step_size = np.random.rand(2) * 0.1"
      ],
      "metadata": {
        "id": "TydaaOeBplB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If8cIaftpoaV",
        "outputId": "29f4fb4c-3ad5-49be-cc1d-3a837ae01f4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.04709335, 0.08997532])"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now apply the fromula"
      ],
      "metadata": {
        "id": "dn_Y4ITkpkAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step_size = step_size / (1e-8 + gradient)"
      ],
      "metadata": {
        "id": "WNhZYaQicxHs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "93c68e60-9004-4024-de46-b64041402645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-125-380c59701e54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1e-8\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'float' and 'list'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This error is because, $1e-8$ is an int and gradient is a list, but we need to do that, so lets just make this an array "
      ],
      "metadata": {
        "id": "uYI73ylCqJeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step_size = step_size / (1e-8 + np.array(gradient))"
      ],
      "metadata": {
        "id": "LzBdYJL2qUvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYRflom8qYiE",
        "outputId": "c30b2996-de5f-4a3b-f2da-c81f599f3532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.33343056,  0.4430092 ])"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our RMS Prop has been created , lets put it in a function for better usage "
      ],
      "metadata": {
        "id": "mAtexx9htgIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rms_prop():\n",
        "    params = np.random.ran(2) * 0.1\n",
        "    gradient = [params[0] * 2 , params[1] * 2]\n",
        "    step_size = np.random.rand(2) * 0.1\n",
        "\n",
        "    return step_size / (1e-8 + np.array(gradient))\n"
      ],
      "metadata": {
        "id": "4PlceKvEtmsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But this function lacks some abilites, such as \n",
        "* What if we run this more than once ?\n",
        "* What if user has a list of columns instead of one ?"
      ],
      "metadata": {
        "id": "Ql3QQ7Bgt8-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What if we run this more than once"
      ],
      "metadata": {
        "id": "C_WkIVwXuS0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we run this more than once, we need to add another hyperparameter, `rho` for that, this is actualy a condition, which we had skipped "
      ],
      "metadata": {
        "id": "53fSH_B0uZGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rms_prop(rho = 0.999 , lr = 0.01 , epochs = 100):\n",
        "    params = np.random.ran(2) * 0.1\n",
        "    gradient = [params[0] * 2 , params[1] * 2]\n",
        "    step_size = np.random.rand(2) * 0.1\n",
        "    for i in range(len(gradient)):\n",
        "        gradient[i] = (gradient[i] * rho) + ((gradeint[i] ** 2) * lr)\n",
        "        for i in range(epochs) : \n",
        "            step_size = step_size / (1e-8 + np.array(gradient))"
      ],
      "metadata": {
        "id": "wVyBuRtfun4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Whta if user has a list of columns instead of one "
      ],
      "metadata": {
        "id": "iRW7b5Y4v1Ww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will implement one more formula $$s_{t+1} = ((s_t r_{ho}) + (f^1(x^2_t (1 - r_{ho})))$$"
      ],
      "metadata": {
        "id": "M9ygOqhS7THQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rms_prop(columns , rho = 0.999 , lr = 0.01 , epochs = 100):\n",
        "    params = np.random.ran(len(columns)) * 0.1\n",
        "    gradient = [params[:len(columns) - 2] * 2 , params[-1] * 2]\n",
        "    step_size = np.random.rand(len(columns)) * 0.1\n",
        "    for i in range(len(gradient)):\n",
        "        gradient[i] = (gradient[i] * rho) + ((gradient[i] ** 2) * lr)\n",
        "        for i in range(epochs) : \n",
        "            step_size = step_size / (1e-8 + np.array(gradient[i]))"
      ],
      "metadata": {
        "id": "BmhJ-iKHv6Cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now our function is created, lets beutify it a little bit "
      ],
      "metadata": {
        "id": "uWtEiWOWwcLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rms_prop(columns , rho = 0.999 , lr = 0.01 , epochs = 100):\n",
        "    \n",
        "    params = np.random.ran(len(columns)) * 0.1\n",
        "    \n",
        "    gradient = [params[:len(columns) - 2] * 2 , params[-1] * 2]\n",
        "    \n",
        "    step_size = np.random.rand(len(columns)) * 0.1\n",
        "    \n",
        "    for i in range(len(gradient)):\n",
        "    \n",
        "        gradient[i] = (gradient[i] * rho) + ((gradient[i] ** 2) * lr)\n",
        "    \n",
        "        for i in range(epochs) : \n",
        "    \n",
        "            step_size = step_size / (1e-8 + np.array(gradient[i]))\n",
        "\n",
        "    return step_size"
      ],
      "metadata": {
        "id": "VNvpWHm4wb_A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}