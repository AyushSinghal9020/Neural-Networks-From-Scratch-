{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Adaptive Gradient Boost\n",
        "\n",
        "Adagrad (Adaptive Gradient Descent) is an optimization algorithm used in machine learning for gradient-based optimization of a cost function. Adagrad adapts the learning rate of each parameter based on the historical gradient information, effectively giving larger updates to parameters that are less frequently updated and smaller updates to parameters that are frequently updated.\n",
        "\n",
        "The Adagrad algorithm maintains a separate learning rate for each parameter of the model, which is inversely proportional to the square root of the sum of the squared gradients of that parameter over time. This means that parameters with large gradients will have a smaller learning rate, while parameters with small gradients will have a larger learning rate.\n",
        "\n",
        "Adagrad is particularly useful for problems where the gradient for some parameters may be much larger than for others, as it allows for a more effective learning rate to be applied to each parameter. However, it may suffer from a diminishing learning rate problem in later stages of training, where the learning rate becomes too small to make further progress. This can be mitigated by using variants of Adagrad such as Adadelta and RMSprop."
      ],
      "metadata": {
        "id": "h8MNbLB7ppDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np "
      ],
      "metadata": {
        "id": "sbffEwj3pxaC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our motive it to create this function somehow\n",
        "$new_-gradient = \\frac {-(lr)}{\\sqrt {gradients} + epsilon}(old_-gradient)$"
      ],
      "metadata": {
        "id": "sVBL1cwzp8a7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But what is this epsilon here, Epsilon is a hyperparameter, that prevents division by zero "
      ],
      "metadata": {
        "id": "h_unoHQoqeGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now we just need to run a for loop, that stores the grdient values"
      ],
      "metadata": {
        "id": "HSAcNFyH2rYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets consider the most basic dataset, that has one column/feature and one target, so the eqaution of a line that tries to predict the values will be $line = \\beta_{1}x_1 + \\beta_0$ there will be one value of, as we can see there are two hyperparameters, and so we need to predict two random values"
      ],
      "metadata": {
        "id": "mLaQC0hC20Hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.randn(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XNKef2u2pGO",
        "outputId": "e9791acf-6147-4151-8fcb-b7452d2a98ce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.31984117, 1.45958699])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These values are not consistent, and we know that for finding one graddient, we intialize some values randomly, it is a good practice to set the values, between $(-1 , 1)$ "
      ],
      "metadata": {
        "id": "lsv_x4e_26fC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.randn(2) * 0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnan0ZLy2_p6",
        "outputId": "565a5d02-2142-4d19-c8c2-cd8890afe19b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.06010894, -0.10773568])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way we can do this is to divide these values by  10  at the time of intialization"
      ],
      "metadata": {
        "id": "_trOKK9G2-AS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = np.random.randn(2) * 0.1"
      ],
      "metadata": {
        "id": "yLtnwxfB3HyZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets caculate the gradient of these values, gradient is just the derivatives of these functions, tese values are actually in the form of $x^2$ and there derivates will be $2x$, lets do these "
      ],
      "metadata": {
        "id": "RjdisWxb3L4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradient = [params[0] * 2 , params[1] * 2]"
      ],
      "metadata": {
        "id": "kUlIlJif3OIJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradient "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixmaCnX03Qih",
        "outputId": "d4cc4457-8f2e-4c9b-8829-5a9db2ba53da"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3880253044056885, 0.23862905750904934]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have created our adagrad, lets put it in function for better usage"
      ],
      "metadata": {
        "id": "UsJAdFez3ZMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adagrad():\n",
        "    \n",
        "    params = np.random.ran(2) * 0.1\n",
        "    gradient = [params[0] * 2 , params[1] * 2]\n",
        "\n",
        "    return gradient"
      ],
      "metadata": {
        "id": "sRf-67kd3jqE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But this function lacks some abilites, such as \n",
        "* What if we run this more than once ?\n",
        "* What if user has a list of columns instead of one ?"
      ],
      "metadata": {
        "id": "Xx_VVhEV3tTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets solve these at once "
      ],
      "metadata": {
        "id": "Rzg_pf4c30z7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4IYileRm4Y9"
      },
      "outputs": [],
      "source": [
        "def rms_prop(columns , rho = 0.999 , lr = 0.01 , epochs = 100 , epsilon = 1e-7):\n",
        "    \n",
        "    params = np.random.ran(len(columns)) * 0.1\n",
        "    \n",
        "    gradient = [params[:len(columns) - 2] * 2 , params[-1] * 2]\n",
        "    \n",
        "    step_size = np.random.rand(len(columns)) * 0.1\n",
        "\n",
        "    gradient_sum = 0\n",
        "    \n",
        "    for i in range(len(gradient)):\n",
        "        \n",
        "        gradient[i] = (gradient[i]) + ((gradient[i] ** 2) * lr)\n",
        "        \n",
        "        lr = lr/np.sqrt(gradient_sum + epsilon)\n",
        "        \n",
        "        gradient_sum +=gradient[i]\n",
        "    \n",
        "        for i in range(epochs) : \n",
        "    \n",
        "            step_size = step_size * lr\n",
        "\n",
        "    return step_size"
      ]
    }
  ]
}