{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Categorical Cross Entorpy\n",
        "\n",
        "<img src = \"https://androidkt.com/wp-content/uploads/2021/05/Selection_098.png\">\n",
        "\n",
        "Categorical cross-entropy is a loss function commonly used in machine learning for multi-class classification problems. It measures the dissimilarity between the predicted probability distribution and the true probability distribution of the target variable.\n",
        "\n",
        "In other words, it calculates the difference between the predicted probabilities of each class and the true probabilities. The predicted probabilities are obtained using a softmax function, which converts the output of a neural network into a probability distribution. The true probabilities are represented as one-hot encoded vectors.\n",
        "\n",
        "The categorical cross-entropy loss is defined as the negative log-likelihood of the true class, given the predicted probability distribution. The goal of the learning algorithm is to minimize the categorical cross-entropy loss, which means that it tries to make the predicted probabilities as close as possible to the true probabilities.\n",
        "\n",
        "Categorical cross-entropy is a popular loss function because it is differentiable and relatively easy to optimize. It is widely used in deep learning models for image classification, natural language processing, and other applications that involve multi-class classification problems.\n",
        "\n",
        "$$CCE = \\sum\\sum y(log(y))$$"
      ],
      "metadata": {
        "id": "IcBj3z43s3it"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rtAp7o25siwP"
      },
      "outputs": [],
      "source": [
        "import numpy as np "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets assume we have these values"
      ],
      "metadata": {
        "id": "EEeq1rsPtMGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = 1\n",
        "actual = 0"
      ],
      "metadata": {
        "id": "7NcCAiYJtK2f"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The error, also can be said as the difference between these values will be"
      ],
      "metadata": {
        "id": "8CqZVhNPtQg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted - actual"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VdGfJustPR9",
        "outputId": "03b57424-56c7-46a2-9279-6750344b0211"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But this is not powerfull, assume a situation, the model says there is $51$% chance of some label, the penalty for not being the correct label to the model should be less for this when comapred to a situation when the model says there is $99$% chance for some label. Cross entropy tries to balance this thing. It calaulates error for the probablity of some label to be the predicted label "
      ],
      "metadata": {
        "id": "Yj0WdkB3tURw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also a few ml models spit out direct labels, most of the algos spit out the probablity which is further preprocessed to make a label"
      ],
      "metadata": {
        "id": "SDOEqJHfuLkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets assume we have the probablity of the label like this, for any label"
      ],
      "metadata": {
        "id": "qQuoMAVXucr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = 89.9"
      ],
      "metadata": {
        "id": "qkpq018ftSGD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The error will be "
      ],
      "metadata": {
        "id": "N4o225Xmu9en"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted * np.log1p(predicted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FVfKdSHulTz",
        "outputId": "58bca104-ac38-45de-87c0-59f6208873a5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "405.4274241063906"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now this our error "
      ],
      "metadata": {
        "id": "2TgHbf6CvJD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets assume we have a list of values "
      ],
      "metadata": {
        "id": "jiU6-BCKvlql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in list_of_values:\n",
        "    error = 0\n",
        "    error += predicted * np.log1p(predicted)"
      ],
      "metadata": {
        "id": "ZH0rixdkvoil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now if we need to do for a list of clsses "
      ],
      "metadata": {
        "id": "Sz8f1aeWvOpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in classes :\n",
        "    for _ in list_of_values:\n",
        "        error = 0\n",
        "        error += predicted * np.log1p(predicted)"
      ],
      "metadata": {
        "id": "bpyAiwQMvHta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we just need to put all of this into a funciton "
      ],
      "metadata": {
        "id": "ocN1gBGev94H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CCE(predicted , actual):\n",
        "    for _ in classes :\n",
        "        \n",
        "        for _ in list_of_values:\n",
        "        \n",
        "            error = 0\n",
        "            error += predicted * np.log1p(predicted)\n",
        "\n",
        "    return error"
      ],
      "metadata": {
        "id": "dngZxvz8wA1C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}